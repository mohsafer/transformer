


2024-12-10 14:50:08
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: False
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-10 14:56:14
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: False
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-10 15:04:02
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: False
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-10 15:07:05
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: False
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-10 15:15:50
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: False
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-10 15:19:22
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.3041s/iter; left time: 2493.6995s
	speed: 0.1990s/iter; left time: 1611.9870s
	speed: 0.1999s/iter; left time: 1598.9083s
	speed: 0.2006s/iter; left time: 1584.5023s
	speed: 0.2011s/iter; left time: 1568.6167s
	speed: 0.2011s/iter; left time: 1548.4682s
	speed: 0.2015s/iter; left time: 1531.5540s
	speed: 0.2014s/iter; left time: 1510.0034s
	speed: 0.2017s/iter; left time: 1492.4273s
	speed: 0.2018s/iter; left time: 1472.7735s
	speed: 0.2018s/iter; left time: 1452.4411s
	speed: 0.2017s/iter; left time: 1432.2077s
	speed: 0.2019s/iter; left time: 1412.8755s
	speed: 0.2021s/iter; left time: 1394.1769s
	speed: 0.2018s/iter; left time: 1372.1136s
	speed: 0.2016s/iter; left time: 1350.7536s
	speed: 0.2021s/iter; left time: 1333.8330s
	speed: 0.2016s/iter; left time: 1309.9156s
	speed: 0.2017s/iter; left time: 1290.7184s
	speed: 0.2019s/iter; left time: 1271.9362s
	speed: 0.2016s/iter; left time: 1249.9794s
	speed: 0.2016s/iter; left time: 1229.6113s
	speed: 0.2020s/iter; left time: 1211.8154s
	speed: 0.2018s/iter; left time: 1190.2170s
	speed: 0.2016s/iter; left time: 1169.1852s
	speed: 0.2016s/iter; left time: 1148.8253s
	speed: 0.2016s/iter; left time: 1129.0011s
Epoch: 1, Cost time: 791.857s 
	speed: 2.5815s/iter; left time: 14025.3045s
	speed: 0.2015s/iter; left time: 1074.8080s
	speed: 0.2014s/iter; left time: 1054.0259s
	speed: 0.2013s/iter; left time: 1033.2518s
	speed: 0.2016s/iter; left time: 1014.5822s
	speed: 0.2012s/iter; left time: 992.3319s
	speed: 0.2013s/iter; left time: 972.9351s
	speed: 0.2015s/iter; left time: 953.5414s
	speed: 0.2013s/iter; left time: 932.6480s
	speed: 0.2015s/iter; left time: 913.1867s
	speed: 0.2013s/iter; left time: 892.3090s
	speed: 0.2013s/iter; left time: 872.2478s
	speed: 0.2011s/iter; left time: 851.3936s
	speed: 0.2014s/iter; left time: 832.4565s
	speed: 0.2016s/iter; left time: 812.9707s
	speed: 0.2012s/iter; left time: 791.4140s
	speed: 0.2010s/iter; left time: 770.2653s
	speed: 0.2014s/iter; left time: 751.7875s
	speed: 0.2013s/iter; left time: 731.1944s
	speed: 0.2012s/iter; left time: 710.8647s
	speed: 0.2012s/iter; left time: 690.8663s
	speed: 0.2014s/iter; left time: 671.2088s
	speed: 0.2015s/iter; left time: 651.4936s
	speed: 0.2012s/iter; left time: 630.3756s
	speed: 0.2015s/iter; left time: 611.2337s
	speed: 0.2015s/iter; left time: 590.9467s
	speed: 0.2015s/iter; left time: 570.7087s
Epoch: 2, Cost time: 781.851s 
	speed: 2.5842s/iter; left time: 6892.1079s
	speed: 0.2012s/iter; left time: 516.5673s
	speed: 0.2011s/iter; left time: 496.2156s
	speed: 0.2012s/iter; left time: 476.1699s
	speed: 0.2011s/iter; left time: 455.9295s
	speed: 0.2013s/iter; left time: 436.1124s
	speed: 0.2011s/iter; left time: 415.7753s
	speed: 0.2012s/iter; left time: 395.6894s
	speed: 0.2011s/iter; left time: 375.4861s
	speed: 0.2013s/iter; left time: 355.6639s
	speed: 0.2011s/iter; left time: 335.1743s
	speed: 0.2014s/iter; left time: 315.5586s
	speed: 0.2013s/iter; left time: 295.2707s
	speed: 0.2013s/iter; left time: 275.2382s
	speed: 0.2014s/iter; left time: 255.1836s
	speed: 0.2012s/iter; left time: 234.7934s
	speed: 0.2009s/iter; left time: 214.3749s
	speed: 0.2012s/iter; left time: 194.5958s
	speed: 0.2010s/iter; left time: 174.3030s
	speed: 0.2013s/iter; left time: 154.4149s
	speed: 0.2012s/iter; left time: 134.2085s
	speed: 0.2011s/iter; left time: 114.0379s
	speed: 0.2013s/iter; left time: 94.0088s
	speed: 0.2011s/iter; left time: 73.8122s
	speed: 0.2014s/iter; left time: 53.7679s
	speed: 0.2013s/iter; left time: 33.6113s
	speed: 0.2013s/iter; left time: 13.4878s
Epoch: 3, Cost time: 781.654s 
Threshold : 0.9999186992645264
pa_accuracy           : 0.9885
pa_precision          : 0.8621
pa_recall             : 0.8631
pa_f_score            : 0.8626
MCC_score             : nan
Affiliation precision : 0.5093
Affiliation recall    : 0.8955
R_AUC_ROC             : 0.7207
R_AUC_PR              : 0.6745
VUS_ROC               : 0.6996
VUS_PR                : 0.6544
Accuracy : 0.9885, Precision : 0.8621, Recall : 0.8631, F-score : 0.8626 



2024-12-11 01:59:19
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4129s/iter; left time: 1672.4114s
	speed: 0.3944s/iter; left time: 1557.9457s
	speed: 0.3961s/iter; left time: 1524.9438s
	speed: 0.3968s/iter; left time: 1488.1257s
	speed: 0.3971s/iter; left time: 1449.2767s
	speed: 0.3973s/iter; left time: 1410.4300s
	speed: 0.3976s/iter; left time: 1371.7883s
	speed: 0.3978s/iter; left time: 1332.5394s
	speed: 0.3978s/iter; left time: 1292.9414s



2024-12-11 03:50:33
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.2278s/iter; left time: 1237.6725s
	speed: 0.1986s/iter; left time: 1059.3343s
	speed: 0.1997s/iter; left time: 1045.1895s
	speed: 0.2002s/iter; left time: 1027.3832s
	speed: 0.2007s/iter; left time: 1010.2506s
	speed: 0.2010s/iter; left time: 991.6916s
	speed: 0.2012s/iter; left time: 972.4152s
	speed: 0.2013s/iter; left time: 952.8407s
	speed: 0.2014s/iter; left time: 933.1043s
	speed: 0.2018s/iter; left time: 914.7100s
	speed: 0.2014s/iter; left time: 892.9801s
	speed: 0.2016s/iter; left time: 873.4745s
	speed: 0.2016s/iter; left time: 853.3434s
	speed: 0.2016s/iter; left time: 833.1231s
	speed: 0.2017s/iter; left time: 813.3942s
	speed: 0.2018s/iter; left time: 793.6040s
	speed: 0.2021s/iter; left time: 774.5820s
	speed: 0.2015s/iter; left time: 752.1802s
	speed: 0.2013s/iter; left time: 731.3516s
	speed: 0.2017s/iter; left time: 712.6209s
	speed: 0.2015s/iter; left time: 691.6729s
	speed: 0.2017s/iter; left time: 672.3184s
	speed: 0.2015s/iter; left time: 651.4930s
	speed: 0.2017s/iter; left time: 631.9273s
	speed: 0.2016s/iter; left time: 611.4786s
	speed: 0.2015s/iter; left time: 591.1072s
	speed: 0.2020s/iter; left time: 572.2826s
Epoch: 1, Cost time: 783.561s 
	speed: 2.5792s/iter; left time: 6878.7249s
	speed: 0.2015s/iter; left time: 517.2628s
	speed: 0.2014s/iter; left time: 496.9511s
	speed: 0.2016s/iter; left time: 477.1625s
	speed: 0.2016s/iter; left time: 457.0433s
	speed: 0.2014s/iter; left time: 436.4878s
	speed: 0.2015s/iter; left time: 416.5353s
	speed: 0.2017s/iter; left time: 396.7890s
	speed: 0.2017s/iter; left time: 376.4843s
	speed: 0.2016s/iter; left time: 356.2142s
	speed: 0.2015s/iter; left time: 335.8175s
	speed: 0.2018s/iter; left time: 316.2449s
	speed: 0.2018s/iter; left time: 295.9812s
	speed: 0.2016s/iter; left time: 275.5590s
	speed: 0.2019s/iter; left time: 255.8143s
	speed: 0.2015s/iter; left time: 235.1480s
	speed: 0.2017s/iter; left time: 215.2663s
	speed: 0.2016s/iter; left time: 194.9859s
	speed: 0.2017s/iter; left time: 174.8729s
	speed: 0.2014s/iter; left time: 154.5076s
	speed: 0.2017s/iter; left time: 134.5022s
	speed: 0.2017s/iter; left time: 114.3683s
	speed: 0.2015s/iter; left time: 94.1167s
	speed: 0.2016s/iter; left time: 73.9737s
	speed: 0.2018s/iter; left time: 53.8726s
	speed: 0.2015s/iter; left time: 33.6463s
	speed: 0.2017s/iter; left time: 13.5136s
Epoch: 2, Cost time: 782.118s 
Threshold : 0.9967628717422485
pa_accuracy           : 0.9881
pa_precision          : 0.8615
pa_recall             : 0.8546
pa_f_score            : 0.8580
MCC_score             : nan
Affiliation precision : 0.5088
Affiliation recall    : 0.8924
R_AUC_ROC             : 0.7124
R_AUC_PR              : 0.6664
VUS_ROC               : 0.6893
VUS_PR                : 0.6444
Accuracy : 0.9881, Precision : 0.8615, Recall : 0.8546, F-score : 0.8580 



2024-12-11 04:58:12
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.2278s/iter; left time: 1868.1265s
	speed: 0.1980s/iter; left time: 1603.9277s
	speed: 0.1987s/iter; left time: 1589.5531s
	speed: 0.1993s/iter; left time: 1574.5693s
	speed: 0.1997s/iter; left time: 1557.7423s
	speed: 0.2002s/iter; left time: 1541.0865s
	speed: 0.2006s/iter; left time: 1524.6290s
	speed: 0.2005s/iter; left time: 1503.3571s
	speed: 0.2003s/iter; left time: 1482.2651s
	speed: 0.2004s/iter; left time: 1462.8320s
	speed: 0.2008s/iter; left time: 1445.4652s
	speed: 0.2009s/iter; left time: 1426.1724s
	speed: 0.2008s/iter; left time: 1405.4373s
	speed: 0.2008s/iter; left time: 1385.6503s
	speed: 0.2010s/iter; left time: 1366.7921s
	speed: 0.2009s/iter; left time: 1346.0839s
	speed: 0.2007s/iter; left time: 1324.2764s
	speed: 0.2007s/iter; left time: 1304.5649s
	speed: 0.2008s/iter; left time: 1284.7073s
	speed: 0.2007s/iter; left time: 1264.2313s
	speed: 0.2009s/iter; left time: 1245.0766s
	speed: 0.2009s/iter; left time: 1225.4418s
	speed: 0.2008s/iter; left time: 1204.4918s
	speed: 0.2008s/iter; left time: 1184.5651s
	speed: 0.2008s/iter; left time: 1164.2265s
	speed: 0.2007s/iter; left time: 1143.6713s
	speed: 0.2010s/iter; left time: 1125.1681s
Epoch: 1, Cost time: 781.593s 
	speed: 2.5817s/iter; left time: 14026.5377s
	speed: 0.2007s/iter; left time: 1070.5504s
	speed: 0.2004s/iter; left time: 1048.6566s
	speed: 0.2005s/iter; left time: 1029.0620s
	speed: 0.2008s/iter; left time: 1010.7688s
	speed: 0.2006s/iter; left time: 989.5906s
	speed: 0.2004s/iter; left time: 968.3977s
	speed: 0.2004s/iter; left time: 948.7002s
	speed: 0.2009s/iter; left time: 930.8565s
	speed: 0.2003s/iter; left time: 908.1697s
	speed: 0.2005s/iter; left time: 888.9588s
	speed: 0.2006s/iter; left time: 869.3517s
	speed: 0.2006s/iter; left time: 848.9609s
	speed: 0.2003s/iter; left time: 827.8278s
	speed: 0.2006s/iter; left time: 808.8374s
	speed: 0.2004s/iter; left time: 788.2915s
	speed: 0.2005s/iter; left time: 768.4208s
	speed: 0.2004s/iter; left time: 748.0475s
	speed: 0.2006s/iter; left time: 728.9281s
	speed: 0.2003s/iter; left time: 707.8275s
	speed: 0.2003s/iter; left time: 687.5514s
	speed: 0.2008s/iter; left time: 669.3284s
	speed: 0.2004s/iter; left time: 647.8428s
	speed: 0.2003s/iter; left time: 627.4188s
	speed: 0.2004s/iter; left time: 607.6874s
	speed: 0.2003s/iter; left time: 587.4832s
	speed: 0.2002s/iter; left time: 567.2408s
Epoch: 2, Cost time: 779.291s 
	speed: 2.5808s/iter; left time: 6883.0095s
	speed: 0.2002s/iter; left time: 513.8635s
	speed: 0.2004s/iter; left time: 494.4398s
	speed: 0.2003s/iter; left time: 474.1544s
	speed: 0.2005s/iter; left time: 454.4477s
	speed: 0.2002s/iter; left time: 433.8271s
	speed: 0.2003s/iter; left time: 414.0504s
	speed: 0.2004s/iter; left time: 394.1267s
	speed: 0.2001s/iter; left time: 373.5740s
	speed: 0.2005s/iter; left time: 354.2535s
	speed: 0.2003s/iter; left time: 333.8949s
	speed: 0.2001s/iter; left time: 313.5529s
	speed: 0.2004s/iter; left time: 294.0441s
	speed: 0.2001s/iter; left time: 273.5477s
	speed: 0.2001s/iter; left time: 253.4707s
	speed: 0.2002s/iter; left time: 233.6311s
	speed: 0.2008s/iter; left time: 214.2036s
	speed: 0.2001s/iter; left time: 193.5335s
	speed: 0.2001s/iter; left time: 173.5024s
	speed: 0.2003s/iter; left time: 153.6418s
	speed: 0.2002s/iter; left time: 133.5300s
	speed: 0.2001s/iter; left time: 113.4562s
	speed: 0.2004s/iter; left time: 93.5658s
	speed: 0.2003s/iter; left time: 73.5214s
	speed: 0.2002s/iter; left time: 53.4401s
	speed: 0.2000s/iter; left time: 33.4051s
	speed: 0.2005s/iter; left time: 13.4314s
Epoch: 3, Cost time: 778.768s 
Threshold : 0.9999659061431885
pa_accuracy           : 0.9886
pa_precision          : 0.8637
pa_recall             : 0.8644
pa_f_score            : 0.8640
MCC_score             : nan
Affiliation precision : 0.5089
Affiliation recall    : 0.8903
R_AUC_ROC             : 0.7046
R_AUC_PR              : 0.6601
VUS_ROC               : 0.6905
VUS_PR                : 0.6465
Accuracy : 0.9886, Precision : 0.8637, Recall : 0.8644, F-score : 0.8640 



2024-12-11 06:34:11
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 5
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4132s/iter; left time: 2816.4781s
	speed: 0.3935s/iter; left time: 2642.8628s
	speed: 0.3953s/iter; left time: 2615.3082s
	speed: 0.3960s/iter; left time: 2580.4175s
	speed: 0.3964s/iter; left time: 2543.2158s
	speed: 0.3964s/iter; left time: 2503.9632s
	speed: 0.3969s/iter; left time: 2467.4259s
	speed: 0.3970s/iter; left time: 2428.1072s
	speed: 0.3971s/iter; left time: 2389.1679s
	speed: 0.3970s/iter; left time: 2348.3940s
	speed: 0.3969s/iter; left time: 2308.1939s
	speed: 0.3970s/iter; left time: 2269.1798s
	speed: 0.3967s/iter; left time: 2227.9893s
Threshold : 0.9999659061431885



2024-12-11 07:00:32
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 5
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4131s/iter; left time: 2815.8873s
	speed: 0.3936s/iter; left time: 2643.4400s
	speed: 0.3950s/iter; left time: 2613.5567s
	speed: 0.3958s/iter; left time: 2579.2204s
	speed: 0.3960s/iter; left time: 2540.7815s
	speed: 0.3962s/iter; left time: 2502.6452s
	speed: 0.3962s/iter; left time: 2462.7473s
	speed: 0.3966s/iter; left time: 2425.6433s
	speed: 0.3967s/iter; left time: 2386.2508s
	speed: 0.3966s/iter; left time: 2346.2219s
	speed: 0.3966s/iter; left time: 2306.8362s
	speed: 0.3966s/iter; left time: 2267.0085s
	speed: 0.3965s/iter; left time: 2226.4720s
Epoch: 1, Cost time: 770.026s 
	speed: 2.9358s/iter; left time: 15950.2261s
	speed: 0.3969s/iter; left time: 2116.7945s
	speed: 0.3970s/iter; left time: 2077.3053s
	speed: 0.3969s/iter; left time: 2037.1592s
	speed: 0.3965s/iter; left time: 1995.7279s
	speed: 0.3965s/iter; left time: 1955.9400s
	speed: 0.3964s/iter; left time: 1915.9564s
	speed: 0.3962s/iter; left time: 1875.3081s
	speed: 0.3961s/iter; left time: 1835.1378s
	speed: 0.3961s/iter; left time: 1795.7175s
	speed: 0.3961s/iter; left time: 1755.9773s
	speed: 0.3963s/iter; left time: 1716.9692s
	speed: 0.3958s/iter; left time: 1675.5553s
Epoch: 2, Cost time: 768.940s 
	speed: 2.9319s/iter; left time: 11874.3320s
	speed: 0.3957s/iter; left time: 1562.9191s
	speed: 0.3957s/iter; left time: 1523.3103s
	speed: 0.3956s/iter; left time: 1483.6845s
	speed: 0.3958s/iter; left time: 1444.6833s
	speed: 0.3957s/iter; left time: 1404.5843s
	speed: 0.3953s/iter; left time: 1363.8604s
	speed: 0.3957s/iter; left time: 1325.5273s
	speed: 0.3956s/iter; left time: 1285.8131s
	speed: 0.3955s/iter; left time: 1245.9699s
	speed: 0.3955s/iter; left time: 1206.2119s
	speed: 0.3953s/iter; left time: 1166.0853s
	speed: 0.3954s/iter; left time: 1126.8440s
Epoch: 3, Cost time: 767.644s 
	speed: 2.9299s/iter; left time: 7814.1028s
	speed: 0.3952s/iter; left time: 1014.4711s
	speed: 0.3949s/iter; left time: 974.2547s
	speed: 0.3949s/iter; left time: 934.7810s
	speed: 0.3952s/iter; left time: 895.9708s
	speed: 0.3952s/iter; left time: 856.3376s
	speed: 0.3951s/iter; left time: 816.7629s
	speed: 0.3953s/iter; left time: 777.4675s
	speed: 0.3953s/iter; left time: 738.0923s
	speed: 0.3951s/iter; left time: 698.1099s
	speed: 0.3953s/iter; left time: 658.9885s
	speed: 0.3953s/iter; left time: 619.4874s
	speed: 0.3951s/iter; left time: 579.6154s
Epoch: 4, Cost time: 767.020s 
	speed: 2.9289s/iter; left time: 3760.7602s
	speed: 0.3947s/iter; left time: 467.2744s
	speed: 0.3951s/iter; left time: 428.2790s
	speed: 0.3951s/iter; left time: 388.7381s
	speed: 0.3949s/iter; left time: 349.1307s
	speed: 0.3947s/iter; left time: 309.4763s
	speed: 0.3948s/iter; left time: 270.0431s
	speed: 0.3950s/iter; left time: 230.6575s
	speed: 0.3947s/iter; left time: 191.0439s
	speed: 0.3949s/iter; left time: 151.6365s
	speed: 0.3950s/iter; left time: 112.1779s
	speed: 0.3951s/iter; left time: 72.7073s
	speed: 0.3950s/iter; left time: 33.1794s
Epoch: 5, Cost time: 767.100s 
Threshold : 0.01835494161769749
pa_accuracy           : 0.9842
pa_precision          : 0.8470
pa_recall             : 0.7617
pa_f_score            : 0.8021
MCC_score             : nan
Affiliation precision : 0.5164
Affiliation recall    : 0.8442
R_AUC_ROC             : 0.6520
R_AUC_PR              : 0.6033
VUS_ROC               : 0.6411
VUS_PR                : 0.5927
Accuracy : 0.9842, Precision : 0.8470, Recall : 0.7617, F-score : 0.8021 



2024-12-11 08:52:24
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.2281s/iter; left time: 1870.3800s
	speed: 0.1978s/iter; left time: 1602.0827s
	speed: 0.1985s/iter; left time: 1587.8955s
	speed: 0.1991s/iter; left time: 1572.5731s
	speed: 0.1996s/iter; left time: 1557.0409s
	speed: 0.1999s/iter; left time: 1538.7405s
	speed: 0.2003s/iter; left time: 1522.4334s
	speed: 0.2002s/iter; left time: 1501.5977s
	speed: 0.2003s/iter; left time: 1482.0571s
	speed: 0.2007s/iter; left time: 1464.7679s
	speed: 0.2007s/iter; left time: 1444.7559s
	speed: 0.2008s/iter; left time: 1425.5425s
	speed: 0.2007s/iter; left time: 1404.4855s
	speed: 0.2006s/iter; left time: 1384.0255s
	speed: 0.2006s/iter; left time: 1363.8269s
	speed: 0.2006s/iter; left time: 1343.8172s
	speed: 0.2007s/iter; left time: 1324.4115s
	speed: 0.2006s/iter; left time: 1303.8566s
	speed: 0.2006s/iter; left time: 1283.8524s
	speed: 0.2006s/iter; left time: 1263.6615s
	speed: 0.2007s/iter; left time: 1243.8612s
	speed: 0.2007s/iter; left time: 1224.1457s
	speed: 0.2006s/iter; left time: 1203.4590s
	speed: 0.2006s/iter; left time: 1183.4467s
	speed: 0.2007s/iter; left time: 1163.7523s
	speed: 0.2007s/iter; left time: 1143.6623s
	speed: 0.2008s/iter; left time: 1124.0947s
Epoch: 1, Cost time: 781.576s 
	speed: 2.5854s/iter; left time: 14046.7276s
	speed: 0.2003s/iter; left time: 1068.2649s
	speed: 0.2004s/iter; left time: 1048.9519s
	speed: 0.2001s/iter; left time: 1027.0393s
	speed: 0.2004s/iter; left time: 1008.6119s
	speed: 0.2003s/iter; left time: 988.2754s
	speed: 0.2003s/iter; left time: 967.8244s
	speed: 0.2002s/iter; left time: 947.4139s
	speed: 0.2003s/iter; left time: 927.8204s
	speed: 0.2001s/iter; left time: 906.9240s
	speed: 0.2001s/iter; left time: 886.9712s
	speed: 0.2004s/iter; left time: 868.4916s
	speed: 0.2002s/iter; left time: 847.2461s
	speed: 0.2000s/iter; left time: 826.7377s
	speed: 0.2002s/iter; left time: 807.2797s
	speed: 0.2001s/iter; left time: 786.9559s
	speed: 0.1998s/iter; left time: 765.7410s
	speed: 0.1999s/iter; left time: 746.2943s
	speed: 0.1999s/iter; left time: 726.2802s
	speed: 0.1999s/iter; left time: 706.2726s
	speed: 0.1999s/iter; left time: 686.2956s
	speed: 0.1999s/iter; left time: 666.1831s
	speed: 0.2002s/iter; left time: 647.1991s
	speed: 0.1997s/iter; left time: 625.5576s
	speed: 0.1997s/iter; left time: 605.8005s
	speed: 0.1997s/iter; left time: 585.7677s
	speed: 0.1998s/iter; left time: 566.0890s
Epoch: 2, Cost time: 779.157s 
	speed: 2.5909s/iter; left time: 6910.0124s
	speed: 0.1999s/iter; left time: 513.0935s
	speed: 0.1997s/iter; left time: 492.7523s
	speed: 0.1999s/iter; left time: 473.1612s
	speed: 0.1999s/iter; left time: 453.2362s
	speed: 0.1997s/iter; left time: 432.7702s
	speed: 0.1999s/iter; left time: 413.2611s
	speed: 0.1997s/iter; left time: 392.8353s
	speed: 0.1997s/iter; left time: 372.9031s
	speed: 0.1999s/iter; left time: 353.1669s
	speed: 0.1998s/iter; left time: 333.0710s
	speed: 0.1999s/iter; left time: 313.1661s
	speed: 0.1998s/iter; left time: 293.0828s
	speed: 0.1999s/iter; left time: 273.2416s
	speed: 0.1998s/iter; left time: 253.1414s
	speed: 0.1998s/iter; left time: 233.1905s
	speed: 0.1999s/iter; left time: 213.2734s
	speed: 0.1997s/iter; left time: 193.1055s
	speed: 0.1997s/iter; left time: 173.1678s
	speed: 0.1997s/iter; left time: 153.2076s
	speed: 0.1997s/iter; left time: 133.2071s
	speed: 0.1998s/iter; left time: 113.3135s
	speed: 0.1997s/iter; left time: 93.2415s
	speed: 0.1999s/iter; left time: 73.3455s
	speed: 0.1998s/iter; left time: 53.3444s
	speed: 0.1996s/iter; left time: 33.3304s
	speed: 0.1998s/iter; left time: 13.3873s
Epoch: 3, Cost time: 778.327s 
Threshold : 0.9915771484375
pa_accuracy           : 0.9892
pa_precision          : 0.8663
pa_recall             : 0.8780
pa_f_score            : 0.8721
MCC_score             : 9.9247
Affiliation precision : 0.5188
Affiliation recall    : 0.9071
R_AUC_ROC             : 0.7301
R_AUC_PR              : 0.6855
VUS_ROC               : 0.7083
VUS_PR                : 0.6647
Accuracy : 0.9892, Precision : 0.8663, Recall : 0.8780, F-score : 0.8721 



2024-12-11 09:59:01
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 1024
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-11 10:00:59
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4135s/iter; left time: 1674.7298s
	speed: 0.3948s/iter; left time: 1559.4574s
	speed: 0.3961s/iter; left time: 1524.9031s
	speed: 0.3970s/iter; left time: 1488.8000s
	speed: 0.3973s/iter; left time: 1450.2489s
	speed: 0.3974s/iter; left time: 1410.6693s
	speed: 0.3978s/iter; left time: 1372.5008s
	speed: 0.3977s/iter; left time: 1332.2370s
	speed: 0.3977s/iter; left time: 1292.6027s
	speed: 0.3978s/iter; left time: 1253.0292s
	speed: 0.3979s/iter; left time: 1213.4990s
	speed: 0.3980s/iter; left time: 1174.0220s
	speed: 0.3981s/iter; left time: 1134.6009s
Threshold : 0.9915770585536965



2024-12-11 10:21:15
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4138s/iter; left time: 1675.9082s
	speed: 0.3944s/iter; left time: 1557.9113s
	speed: 0.3962s/iter; left time: 1525.2499s
	speed: 0.3969s/iter; left time: 1488.3688s
	speed: 0.3973s/iter; left time: 1450.1771s
	speed: 0.3975s/iter; left time: 1411.2302s
	speed: 0.3979s/iter; left time: 1372.5868s
	speed: 0.3978s/iter; left time: 1332.4856s
	speed: 0.3978s/iter; left time: 1292.9168s
	speed: 0.3980s/iter; left time: 1253.8007s
	speed: 0.3981s/iter; left time: 1214.2458s
	speed: 0.3980s/iter; left time: 1174.0728s
	speed: 0.3980s/iter; left time: 1134.1639s
Epoch: 1, Cost time: 771.845s 
	speed: 2.9380s/iter; left time: 7835.5460s
	speed: 0.3982s/iter; left time: 1022.2796s
	speed: 0.3984s/iter; left time: 982.8022s
	speed: 0.3981s/iter; left time: 942.1951s
	speed: 0.3985s/iter; left time: 903.4489s
	speed: 0.3982s/iter; left time: 862.9383s
	speed: 0.3982s/iter; left time: 823.1282s
	speed: 0.3983s/iter; left time: 783.3883s
	speed: 0.3985s/iter; left time: 743.9709s
	speed: 0.3985s/iter; left time: 704.2362s
	speed: 0.3982s/iter; left time: 663.7576s
	speed: 0.3983s/iter; left time: 624.1713s
	speed: 0.3980s/iter; left time: 583.8035s
Epoch: 2, Cost time: 771.661s 
	speed: 2.9378s/iter; left time: 3772.1559s
	speed: 0.3979s/iter; left time: 471.0696s
	speed: 0.3979s/iter; left time: 431.3036s
	speed: 0.3980s/iter; left time: 391.6387s
	speed: 0.3977s/iter; left time: 351.5753s
	speed: 0.3978s/iter; left time: 311.8582s
	speed: 0.3982s/iter; left time: 272.3766s
	speed: 0.3980s/iter; left time: 232.4474s
	speed: 0.3980s/iter; left time: 192.6284s
	speed: 0.3982s/iter; left time: 152.9226s
	speed: 0.3979s/iter; left time: 113.0038s
	speed: 0.3979s/iter; left time: 73.2148s
	speed: 0.3977s/iter; left time: 33.4049s
Epoch: 3, Cost time: 771.062s 
Threshold : 0.9999938011169434
pa_accuracy           : 0.9888
pa_precision          : 0.8636
pa_recall             : 0.8706
pa_f_score            : 0.8671
MCC_score             : 17.0428
Affiliation precision : 0.5148
Affiliation recall    : 0.8985
R_AUC_ROC             : 0.7226
R_AUC_PR              : 0.6771
VUS_ROC               : 0.7012
VUS_PR                : 0.6566
Accuracy : 0.9888, Precision : 0.8636, Recall : 0.8706, F-score : 0.8671 



2024-12-11 11:26:49
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4137s/iter; left time: 1675.2888s
	speed: 0.3943s/iter; left time: 1557.3023s
	speed: 0.3959s/iter; left time: 1524.2550s
	speed: 0.3967s/iter; left time: 1487.5114s
	speed: 0.3971s/iter; left time: 1449.5805s
	speed: 0.3968s/iter; left time: 1408.7768s
	speed: 0.3975s/iter; left time: 1371.2075s
	speed: 0.3975s/iter; left time: 1331.7291s
	speed: 0.3976s/iter; left time: 1292.1599s
	speed: 0.3976s/iter; left time: 1252.4517s
	speed: 0.3977s/iter; left time: 1213.0717s
	speed: 0.3979s/iter; left time: 1173.8963s
	speed: 0.3979s/iter; left time: 1133.9087s
Epoch: 1, Cost time: 771.516s 
	speed: 2.9380s/iter; left time: 7835.6234s
	speed: 0.3981s/iter; left time: 1021.9124s
	speed: 0.3980s/iter; left time: 981.9452s
	speed: 0.3981s/iter; left time: 942.2972s
	speed: 0.3981s/iter; left time: 902.4857s
	speed: 0.3985s/iter; left time: 863.6574s
	speed: 0.3982s/iter; left time: 823.0574s
	speed: 0.3982s/iter; left time: 783.2264s
	speed: 0.3985s/iter; left time: 743.9374s
	speed: 0.3982s/iter; left time: 703.5329s
	speed: 0.3984s/iter; left time: 664.0504s
	speed: 0.3981s/iter; left time: 623.8142s
	speed: 0.3982s/iter; left time: 584.2219s
Epoch: 2, Cost time: 771.520s 
	speed: 2.9374s/iter; left time: 3771.6261s
	speed: 0.3981s/iter; left time: 471.3480s
	speed: 0.3980s/iter; left time: 431.4769s
	speed: 0.3981s/iter; left time: 391.7738s
	speed: 0.3982s/iter; left time: 352.0065s
	speed: 0.3979s/iter; left time: 311.9252s
	speed: 0.3980s/iter; left time: 272.2110s
	speed: 0.3978s/iter; left time: 232.3110s
	speed: 0.3983s/iter; left time: 192.7937s
	speed: 0.3978s/iter; left time: 152.7722s
	speed: 0.3978s/iter; left time: 112.9814s
	speed: 0.3979s/iter; left time: 73.2224s
	speed: 0.3979s/iter; left time: 33.4218s



2024-12-11 12:08:45
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4141s/iter; left time: 1677.0840s
	speed: 0.3935s/iter; left time: 1554.3550s
	speed: 0.3952s/iter; left time: 1521.4207s
	speed: 0.3959s/iter; left time: 1484.4727s
	speed: 0.3963s/iter; left time: 1446.3867s
	speed: 0.3967s/iter; left time: 1408.3731s
	speed: 0.3963s/iter; left time: 1367.3979s



2024-12-11 12:14:33
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4148s/iter; left time: 1680.0488s
	speed: 0.3940s/iter; left time: 1556.4389s
	speed: 0.3953s/iter; left time: 1521.8163s
	speed: 0.3955s/iter; left time: 1483.2365s
	speed: 0.3959s/iter; left time: 1445.1251s



2024-12-11 12:25:43
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-11 12:26:59
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4160s/iter; left time: 1684.9741s
	speed: 0.3946s/iter; left time: 1558.7403s
	speed: 0.3958s/iter; left time: 1523.9235s
	speed: 0.3963s/iter; left time: 1486.1837s
	speed: 0.3962s/iter; left time: 1446.2142s
	speed: 0.3968s/iter; left time: 1408.7005s
	speed: 0.3967s/iter; left time: 1368.7805s



2024-12-11 12:43:27
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4177s/iter; left time: 1691.5754s
	speed: 0.3990s/iter; left time: 1575.9938s
	speed: 0.4007s/iter; left time: 1542.8442s



2024-12-11 12:48:09
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4179s/iter; left time: 1692.6046s
	speed: 0.3991s/iter; left time: 1576.5528s
	speed: 0.4007s/iter; left time: 1542.5165s
	speed: 0.4016s/iter; left time: 1505.9172s
	speed: 0.4020s/iter; left time: 1467.2070s
	speed: 0.4022s/iter; left time: 1427.9285s
	speed: 0.4022s/iter; left time: 1387.4310s
	speed: 0.4024s/iter; left time: 1348.0312s
	speed: 0.4027s/iter; left time: 1308.7228s



2024-12-11 12:56:39
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4178s/iter; left time: 1692.1317s
	speed: 0.3990s/iter; left time: 1576.0069s
	speed: 0.4001s/iter; left time: 1540.3658s
	speed: 0.4009s/iter; left time: 1503.3267s
	speed: 0.4010s/iter; left time: 1463.7332s
	speed: 0.4015s/iter; left time: 1425.3909s
	speed: 0.4017s/iter; left time: 1385.8022s



2024-12-11 13:01:57
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Threshold : 0.9999803304672241



2024-12-11 13:07:16
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-11 13:11:48
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4195s/iter; left time: 1699.0113s
	speed: 0.3995s/iter; left time: 1577.9994s
	speed: 0.4004s/iter; left time: 1541.4665s
	speed: 0.4015s/iter; left time: 1505.6634s
	speed: 0.4016s/iter; left time: 1465.7941s
	speed: 0.4016s/iter; left time: 1425.6573s
	speed: 0.4015s/iter; left time: 1385.3442s
	speed: 0.4019s/iter; left time: 1346.4890s
	speed: 0.4020s/iter; left time: 1306.5261s



2024-12-11 13:33:37
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4175s/iter; left time: 1690.7730s
	speed: 0.3987s/iter; left time: 1574.6865s
	speed: 0.4001s/iter; left time: 1540.3508s
	speed: 0.4010s/iter; left time: 1503.7860s
	speed: 0.4015s/iter; left time: 1465.5807s
	speed: 0.4018s/iter; left time: 1426.4551s
	speed: 0.4017s/iter; left time: 1386.0126s
	speed: 0.4018s/iter; left time: 1346.1184s
	speed: 0.4013s/iter; left time: 1304.3611s
	speed: 0.4013s/iter; left time: 1263.9571s
	speed: 0.4010s/iter; left time: 1223.1578s
	speed: 0.4013s/iter; left time: 1183.7029s



2024-12-11 13:49:27
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-11 13:53:46
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4134s/iter; left time: 1674.1097s
	speed: 0.3933s/iter; left time: 1553.4526s
	speed: 0.3951s/iter; left time: 1521.1698s
	speed: 0.3957s/iter; left time: 1483.8506s
	speed: 0.3963s/iter; left time: 1446.5168s
	speed: 0.3965s/iter; left time: 1407.4936s
	speed: 0.3967s/iter; left time: 1368.6776s
	speed: 0.3967s/iter; left time: 1329.0587s
	speed: 0.3968s/iter; left time: 1289.5744s
	speed: 0.3969s/iter; left time: 1250.2418s
	speed: 0.3964s/iter; left time: 1209.1513s
	speed: 0.3968s/iter; left time: 1170.4329s
	speed: 0.3964s/iter; left time: 1129.7865s



2024-12-11 14:11:57
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4138s/iter; left time: 1675.7912s
	speed: 0.3933s/iter; left time: 1553.4128s



2024-12-11 14:14:56
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
	speed: 0.4191s/iter; left time: 1697.2761s
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
	speed: 0.3985s/iter; left time: 1573.9578s
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0
epoch 0, loss 0.0



2024-12-11 14:20:41
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
epoch 0, loss 9.459175109863281
epoch 1, loss 9.500575065612793
epoch 2, loss 9.555716514587402
epoch 3, loss 9.59129524230957
epoch 4, loss 9.667972564697266
epoch 5, loss 9.731304168701172
epoch 6, loss 9.793817520141602
epoch 7, loss 9.867395401000977
epoch 8, loss 9.94308090209961
epoch 9, loss 10.054634094238281
epoch 10, loss 10.128488540649414
epoch 11, loss 10.172633171081543
epoch 12, loss 10.286250114440918
epoch 13, loss 10.376405715942383
epoch 14, loss 10.422602653503418
epoch 15, loss 10.45576286315918
epoch 16, loss 10.562768936157227
epoch 17, loss 10.618806838989258
epoch 18, loss 10.749079704284668
epoch 19, loss 10.770323753356934
epoch 20, loss 10.887975692749023
epoch 21, loss 10.919780731201172
epoch 22, loss 11.050481796264648
epoch 23, loss 11.129656791687012
epoch 24, loss 11.17286205291748
epoch 25, loss 11.304427146911621
epoch 26, loss 11.355995178222656
epoch 27, loss 11.428805351257324
epoch 28, loss 11.452524185180664
epoch 29, loss 11.601451873779297
epoch 30, loss 11.719949722290039
epoch 31, loss 11.793360710144043
epoch 32, loss 11.962031364440918
epoch 33, loss 11.987098693847656
epoch 34, loss 12.105916976928711
epoch 35, loss 12.227816581726074
epoch 36, loss 12.390438079833984
epoch 37, loss 12.425737380981445
epoch 38, loss 12.580507278442383
epoch 39, loss 12.626715660095215
epoch 40, loss 12.852699279785156
epoch 41, loss 12.952795028686523
epoch 42, loss 13.142419815063477
epoch 43, loss 13.164412498474121
epoch 44, loss 13.356941223144531
epoch 45, loss 13.532552719116211
epoch 46, loss 13.706069946289062
epoch 47, loss 13.839411735534668
epoch 48, loss 13.923849105834961
epoch 49, loss 14.180715560913086
epoch 50, loss 14.277360916137695
epoch 51, loss 14.444994926452637
epoch 52, loss 14.590811729431152
epoch 53, loss 14.735034942626953
epoch 54, loss 14.91284465789795
epoch 55, loss 15.064478874206543
epoch 56, loss 15.239273071289062
epoch 57, loss 15.412420272827148
epoch 58, loss 15.64720344543457
epoch 59, loss 15.750216484069824
epoch 60, loss 15.92151165008545
epoch 61, loss 16.131467819213867
epoch 62, loss 16.280956268310547
epoch 63, loss 16.493589401245117
epoch 64, loss 16.699787139892578
epoch 65, loss 16.848730087280273
epoch 66, loss 17.010934829711914
epoch 67, loss 17.24627113342285
epoch 68, loss 17.442787170410156
epoch 69, loss 17.66228485107422
epoch 70, loss 17.773038864135742
epoch 71, loss 18.035085678100586
epoch 72, loss 18.263423919677734
epoch 73, loss 18.39846420288086
epoch 74, loss 18.69269371032715
epoch 75, loss 18.855487823486328
epoch 76, loss 19.06646728515625
epoch 77, loss 19.238513946533203
epoch 78, loss 19.450899124145508
epoch 79, loss 19.55258560180664
epoch 80, loss 19.79353141784668
epoch 81, loss 19.977073669433594
epoch 82, loss 20.107574462890625
epoch 83, loss 20.326196670532227
epoch 84, loss 20.52387046813965
epoch 85, loss 20.613327026367188
epoch 86, loss 20.760372161865234
epoch 87, loss 20.912797927856445
epoch 88, loss 21.069805145263672
epoch 89, loss 21.260255813598633
epoch 90, loss 21.369060516357422
epoch 91, loss 21.44396209716797
epoch 92, loss 21.605138778686523
epoch 93, loss 21.76449203491211
epoch 94, loss 21.848377227783203
epoch 95, loss 22.000112533569336
epoch 96, loss 22.075008392333984
epoch 97, loss 22.187461853027344
epoch 98, loss 22.435123443603516
	speed: 0.4180s/iter; left time: 1693.0448s
epoch 99, loss 22.506216049194336
epoch 100, loss 22.645055770874023
epoch 101, loss 22.778844833374023
epoch 102, loss 22.903989791870117
epoch 103, loss 22.983753204345703
epoch 104, loss 23.113811492919922
epoch 105, loss 23.262718200683594
epoch 106, loss 23.42877769470215
epoch 107, loss 23.552715301513672
epoch 108, loss 23.72585678100586
epoch 109, loss 23.795379638671875
epoch 110, loss 23.983673095703125
epoch 111, loss 24.0699462890625
epoch 112, loss 24.22609519958496
epoch 113, loss 24.37335968017578
epoch 114, loss 24.520763397216797
epoch 115, loss 24.637985229492188
epoch 116, loss 24.819255828857422
epoch 117, loss 24.923725128173828
epoch 118, loss 25.03378677368164
epoch 119, loss 25.174816131591797
epoch 120, loss 25.359466552734375
epoch 121, loss 25.43646812438965
epoch 122, loss 25.577919006347656
epoch 123, loss 25.688945770263672
epoch 124, loss 25.77633285522461
epoch 125, loss 25.83296012878418
epoch 126, loss 25.941579818725586
epoch 127, loss 26.052770614624023
epoch 128, loss 26.108190536499023
epoch 129, loss 26.224456787109375
epoch 130, loss 26.286579132080078
epoch 131, loss 26.370145797729492
epoch 132, loss 26.49928855895996
epoch 133, loss 26.55377769470215
epoch 134, loss 26.67458152770996
epoch 135, loss 26.746898651123047
epoch 136, loss 26.85401725769043
epoch 137, loss 26.965269088745117
epoch 138, loss 27.025352478027344
epoch 139, loss 27.099807739257812
epoch 140, loss 27.19314956665039
epoch 141, loss 27.232501983642578
epoch 142, loss 27.352216720581055
epoch 143, loss 27.415321350097656
epoch 144, loss 27.530746459960938
epoch 145, loss 27.600345611572266
epoch 146, loss 27.6651611328125
epoch 147, loss 27.77935791015625
epoch 148, loss 27.89491081237793
epoch 149, loss 28.002731323242188
epoch 150, loss 28.114103317260742
epoch 151, loss 28.205154418945312
epoch 152, loss 28.289466857910156
epoch 153, loss 28.379379272460938
epoch 154, loss 28.517009735107422
epoch 155, loss 28.593154907226562
epoch 156, loss 28.648345947265625
epoch 157, loss 28.699382781982422
epoch 158, loss 28.787839889526367
epoch 159, loss 28.898479461669922
epoch 160, loss 28.969722747802734
epoch 161, loss 29.04292869567871
epoch 162, loss 29.128841400146484
epoch 163, loss 29.2047176361084
epoch 164, loss 29.275726318359375
epoch 165, loss 29.33248519897461
epoch 166, loss 29.408292770385742
epoch 167, loss 29.484329223632812
epoch 168, loss 29.522563934326172
epoch 169, loss 29.63593864440918
epoch 170, loss 29.672658920288086
epoch 171, loss 29.729671478271484
epoch 172, loss 29.782379150390625
epoch 173, loss 29.83282470703125
epoch 174, loss 29.884227752685547
epoch 175, loss 29.935741424560547
epoch 176, loss 30.00555992126465
epoch 177, loss 30.0328369140625
epoch 178, loss 30.064510345458984
epoch 179, loss 30.119125366210938
epoch 180, loss 30.132610321044922
epoch 181, loss 30.18012237548828
epoch 182, loss 30.242694854736328
epoch 183, loss 30.26833152770996
epoch 184, loss 30.31597900390625
epoch 185, loss 30.363798141479492
epoch 186, loss 30.370420455932617
epoch 187, loss 30.454269409179688
epoch 188, loss 30.464488983154297
epoch 189, loss 30.521617889404297
epoch 190, loss 30.48896598815918
epoch 191, loss 30.55261993408203
epoch 192, loss 30.585269927978516
epoch 193, loss 30.595617294311523
epoch 194, loss 30.664737701416016
epoch 195, loss 30.67066192626953
epoch 196, loss 30.70625877380371
epoch 197, loss 30.721105575561523
epoch 198, loss 30.740713119506836
	speed: 0.3977s/iter; left time: 1571.0379s
epoch 199, loss 30.773433685302734
epoch 200, loss 30.825536727905273
epoch 201, loss 30.81793975830078
epoch 202, loss 30.838987350463867
epoch 203, loss 30.90870475769043
epoch 204, loss 30.904586791992188
epoch 205, loss 30.921850204467773
epoch 206, loss 30.949771881103516
epoch 207, loss 30.947521209716797
epoch 208, loss 31.00497055053711
epoch 209, loss 31.043014526367188
epoch 210, loss 31.0227108001709
epoch 211, loss 31.046688079833984
epoch 212, loss 31.073400497436523
epoch 213, loss 31.068838119506836
epoch 214, loss 31.09162712097168
epoch 215, loss 31.158605575561523
epoch 216, loss 31.168537139892578
epoch 217, loss 31.13768768310547
epoch 218, loss 31.182601928710938
epoch 219, loss 31.21605682373047
epoch 220, loss 31.240962982177734
epoch 221, loss 31.23604393005371
epoch 222, loss 31.242523193359375
epoch 223, loss 31.261341094970703
epoch 224, loss 31.295141220092773
epoch 225, loss 31.30042839050293
epoch 226, loss 31.328777313232422
epoch 227, loss 31.365406036376953
epoch 228, loss 31.37294578552246
epoch 229, loss 31.370880126953125
epoch 230, loss 31.418582916259766
epoch 231, loss 31.407611846923828
epoch 232, loss 31.406665802001953
epoch 233, loss 31.424560546875
epoch 234, loss 31.473155975341797
epoch 235, loss 31.47494125366211
epoch 236, loss 31.48977279663086
epoch 237, loss 31.520286560058594
epoch 238, loss 31.525997161865234
epoch 239, loss 31.58839225769043
epoch 240, loss 31.542936325073242
epoch 241, loss 31.557714462280273
epoch 242, loss 31.604583740234375
epoch 243, loss 31.58572006225586
epoch 244, loss 31.595245361328125
epoch 245, loss 31.61054801940918
epoch 246, loss 31.64825439453125
epoch 247, loss 31.634204864501953
epoch 248, loss 31.653356552124023
epoch 249, loss 31.680770874023438
epoch 250, loss 31.701614379882812
epoch 251, loss 31.710134506225586
epoch 252, loss 31.69620132446289
epoch 253, loss 31.70816421508789
epoch 254, loss 31.743694305419922
epoch 255, loss 31.736289978027344
epoch 256, loss 31.749752044677734
epoch 257, loss 31.78444480895996
epoch 258, loss 31.793807983398438
epoch 259, loss 31.838354110717773
epoch 260, loss 31.80353546142578
epoch 261, loss 31.790489196777344
epoch 262, loss 31.82960319519043
epoch 263, loss 31.867542266845703
epoch 264, loss 31.836544036865234
epoch 265, loss 31.855438232421875
epoch 266, loss 31.85109519958496
epoch 267, loss 31.894323348999023
epoch 268, loss 31.89300537109375
epoch 269, loss 31.891584396362305
epoch 270, loss 31.8875789642334
epoch 271, loss 31.92519187927246
epoch 272, loss 31.93509292602539
epoch 273, loss 31.940444946289062
epoch 274, loss 31.954748153686523
epoch 275, loss 31.936965942382812
epoch 276, loss 31.969058990478516
epoch 277, loss 31.949813842773438
epoch 278, loss 32.0101432800293
epoch 279, loss 31.99457550048828
epoch 280, loss 31.965007781982422
epoch 281, loss 31.989181518554688
epoch 282, loss 32.00969696044922
epoch 283, loss 32.019866943359375
epoch 284, loss 32.0236930847168
epoch 285, loss 32.02276611328125
epoch 286, loss 32.06256103515625
epoch 287, loss 32.11346435546875
epoch 288, loss 32.071685791015625
epoch 289, loss 32.067848205566406
epoch 290, loss 32.0968017578125
epoch 291, loss 32.075950622558594
epoch 292, loss 32.10982131958008
epoch 293, loss 32.0753059387207
epoch 294, loss 32.103206634521484
epoch 295, loss 32.091331481933594
epoch 296, loss 32.11212158203125
epoch 297, loss 32.11137390136719
epoch 298, loss 32.11814880371094
	speed: 0.3992s/iter; left time: 1537.0164s
epoch 299, loss 32.11582565307617
epoch 300, loss 32.14501190185547
epoch 301, loss 32.12308883666992
epoch 302, loss 32.16261291503906
epoch 303, loss 32.1645622253418
epoch 304, loss 32.126426696777344
epoch 305, loss 32.19144058227539
epoch 306, loss 32.1843376159668
epoch 307, loss 32.21758270263672
epoch 308, loss 32.1803092956543
epoch 309, loss 32.18046188354492
epoch 310, loss 32.20411682128906
epoch 311, loss 32.216976165771484
epoch 312, loss 32.21721649169922
epoch 313, loss 32.22579574584961
epoch 314, loss 32.23165512084961
epoch 315, loss 32.236480712890625
epoch 316, loss 32.249454498291016
epoch 317, loss 32.21329879760742
epoch 318, loss 32.25328063964844
epoch 319, loss 32.269657135009766
epoch 320, loss 32.242889404296875
epoch 321, loss 32.236000061035156
epoch 322, loss 32.25083923339844
epoch 323, loss 32.252193450927734
epoch 324, loss 32.241249084472656
epoch 325, loss 32.295013427734375
epoch 326, loss 32.29888153076172
epoch 327, loss 32.28828430175781
epoch 328, loss 32.31315994262695
epoch 329, loss 32.31163787841797
epoch 330, loss 32.29893112182617
epoch 331, loss 32.32530975341797
epoch 332, loss 32.30070495605469
epoch 333, loss 32.33147430419922
epoch 334, loss 32.30476760864258
epoch 335, loss 32.33352279663086
epoch 336, loss 32.34076690673828
epoch 337, loss 32.353919982910156
epoch 338, loss 32.338714599609375
epoch 339, loss 32.33836364746094
epoch 340, loss 32.32307434082031
epoch 341, loss 32.37812042236328
epoch 342, loss 32.35837173461914
epoch 343, loss 32.36763000488281
epoch 344, loss 32.35484313964844
epoch 345, loss 32.354591369628906
epoch 346, loss 32.35859680175781
epoch 347, loss 32.35289764404297
epoch 348, loss 32.38792037963867
epoch 349, loss 32.374732971191406
epoch 350, loss 32.369529724121094
epoch 351, loss 32.41468811035156
epoch 352, loss 32.368919372558594
epoch 353, loss 32.37999725341797
epoch 354, loss 32.3939208984375
epoch 355, loss 32.40235900878906
epoch 356, loss 32.38703918457031
epoch 357, loss 32.43386459350586
epoch 358, loss 32.42671203613281
epoch 359, loss 32.392356872558594
epoch 360, loss 32.42963409423828
epoch 361, loss 32.39928436279297
epoch 362, loss 32.43959045410156
epoch 363, loss 32.427101135253906
epoch 364, loss 32.40448760986328
epoch 365, loss 32.405792236328125
epoch 366, loss 32.43852996826172
epoch 367, loss 32.434017181396484
epoch 368, loss 32.46070861816406
epoch 369, loss 32.418724060058594
epoch 370, loss 32.45602035522461
epoch 371, loss 32.45372772216797
epoch 372, loss 32.462554931640625
epoch 373, loss 32.446571350097656
epoch 374, loss 32.440006256103516
epoch 375, loss 32.480751037597656
epoch 376, loss 32.50756072998047
epoch 377, loss 32.49614715576172
epoch 378, loss 32.468040466308594
epoch 379, loss 32.47721862792969
epoch 380, loss 32.515769958496094
epoch 381, loss 32.48017501831055
epoch 382, loss 32.47170639038086
epoch 383, loss 32.509010314941406
epoch 384, loss 32.480445861816406
epoch 385, loss 32.47300338745117
epoch 386, loss 32.502159118652344
epoch 387, loss 32.497886657714844
epoch 388, loss 32.528648376464844
epoch 389, loss 32.506195068359375
epoch 390, loss 32.52668762207031
epoch 391, loss 32.53407669067383
epoch 392, loss 32.520748138427734
epoch 393, loss 32.51609802246094
epoch 394, loss 32.55059814453125
epoch 395, loss 32.52764892578125
epoch 396, loss 32.56391525268555
epoch 397, loss 32.5248908996582
epoch 398, loss 32.5506591796875
	speed: 0.3997s/iter; left time: 1499.0437s
epoch 399, loss 32.50799560546875
epoch 400, loss 32.50376892089844
epoch 401, loss 32.54792785644531
epoch 402, loss 32.525657653808594
epoch 403, loss 32.520206451416016
epoch 404, loss 32.53791809082031
epoch 405, loss 32.52766036987305
epoch 406, loss 32.511749267578125
epoch 407, loss 32.52878952026367
epoch 408, loss 32.52752685546875
epoch 409, loss 32.539852142333984
epoch 410, loss 32.54020690917969
epoch 411, loss 32.54578399658203
epoch 412, loss 32.55644989013672
epoch 413, loss 32.54460906982422
epoch 414, loss 32.52903747558594
epoch 415, loss 32.568511962890625
epoch 416, loss 32.5391845703125
epoch 417, loss 32.59577178955078
epoch 418, loss 32.54499053955078
epoch 419, loss 32.551456451416016
epoch 420, loss 32.55451583862305
epoch 421, loss 32.560279846191406
epoch 422, loss 32.59831237792969
epoch 423, loss 32.558319091796875
epoch 424, loss 32.567508697509766
epoch 425, loss 32.609519958496094
epoch 426, loss 32.58130645751953
epoch 427, loss 32.58212661743164
epoch 428, loss 32.608802795410156
epoch 429, loss 32.56226348876953
epoch 430, loss 32.58430862426758
epoch 431, loss 32.593963623046875
epoch 432, loss 32.611873626708984
epoch 433, loss 32.60302734375
epoch 434, loss 32.58753967285156
epoch 435, loss 32.60314178466797
epoch 436, loss 32.598018646240234
epoch 437, loss 32.594478607177734
epoch 438, loss 32.61628723144531
epoch 439, loss 32.6247444152832
epoch 440, loss 32.572628021240234
epoch 441, loss 32.62528610229492
epoch 442, loss 32.62059020996094
epoch 443, loss 32.599246978759766
epoch 444, loss 32.66807174682617
epoch 445, loss 32.611053466796875
epoch 446, loss 32.63386535644531
epoch 447, loss 32.640987396240234
epoch 448, loss 32.629249572753906
epoch 449, loss 32.60640335083008
epoch 450, loss 32.63227844238281
epoch 451, loss 32.64674377441406
epoch 452, loss 32.632686614990234
epoch 453, loss 32.66082000732422
epoch 454, loss 32.606414794921875
epoch 455, loss 32.61582565307617
epoch 456, loss 32.61762237548828
epoch 457, loss 32.63491439819336
epoch 458, loss 32.6114501953125
epoch 459, loss 32.627235412597656
epoch 460, loss 32.62607955932617
epoch 461, loss 32.619510650634766
epoch 462, loss 32.643096923828125
epoch 463, loss 32.63348388671875
epoch 464, loss 32.65769958496094
epoch 465, loss 32.64869689941406
epoch 466, loss 32.631534576416016
epoch 467, loss 32.63328170776367
epoch 468, loss 32.65620040893555
epoch 469, loss 32.65308380126953
epoch 470, loss 32.700103759765625
epoch 471, loss 32.67559051513672
epoch 472, loss 32.655052185058594
epoch 473, loss 32.673858642578125
epoch 474, loss 32.65363311767578
epoch 475, loss 32.65193176269531
epoch 476, loss 32.65386962890625
epoch 477, loss 32.6754150390625
epoch 478, loss 32.6703987121582
epoch 479, loss 32.672855377197266
epoch 480, loss 32.652793884277344
epoch 481, loss 32.6727409362793
epoch 482, loss 32.661888122558594
epoch 483, loss 32.647762298583984
epoch 484, loss 32.669097900390625
epoch 485, loss 32.688018798828125
epoch 486, loss 32.7005615234375
epoch 487, loss 32.67638397216797
epoch 488, loss 32.67449188232422
epoch 489, loss 32.688690185546875
epoch 490, loss 32.68326950073242
epoch 491, loss 32.722774505615234
epoch 492, loss 32.68410873413086
epoch 493, loss 32.70880126953125
epoch 494, loss 32.67462921142578
epoch 495, loss 32.70204162597656
epoch 496, loss 32.68275833129883
epoch 497, loss 32.67937469482422
epoch 498, loss 32.72279357910156
	speed: 0.4002s/iter; left time: 1460.8691s
epoch 499, loss 32.69179916381836
epoch 500, loss 32.702667236328125
epoch 501, loss 32.67022705078125
epoch 502, loss 32.69776153564453
epoch 503, loss 32.69123077392578
epoch 504, loss 32.688453674316406
epoch 505, loss 32.712890625
epoch 506, loss 32.73577880859375
epoch 507, loss 32.699668884277344
epoch 508, loss 32.72758483886719
epoch 509, loss 32.70158004760742
epoch 510, loss 32.71522521972656
epoch 511, loss 32.687843322753906
epoch 512, loss 32.72307586669922
epoch 513, loss 32.71118927001953
epoch 514, loss 32.695709228515625
epoch 515, loss 32.65242004394531
epoch 516, loss 32.698822021484375
epoch 517, loss 32.695556640625
epoch 518, loss 32.72479248046875
epoch 519, loss 32.67242431640625
epoch 520, loss 32.71841049194336
epoch 521, loss 32.67988586425781
epoch 522, loss 32.733009338378906
epoch 523, loss 32.709266662597656
epoch 524, loss 32.70771026611328
epoch 525, loss 32.687103271484375
epoch 526, loss 32.72791290283203
epoch 527, loss 32.71178436279297
epoch 528, loss 32.690853118896484
epoch 529, loss 32.70460510253906
epoch 530, loss 32.66758728027344
epoch 531, loss 32.708290100097656
epoch 532, loss 32.69898223876953
epoch 533, loss 32.73367691040039
epoch 534, loss 32.7319450378418
epoch 535, loss 32.71833419799805
epoch 536, loss 32.73139953613281
epoch 537, loss 32.68745422363281
epoch 538, loss 32.70647430419922
epoch 539, loss 32.75385665893555
epoch 540, loss 32.74087142944336
epoch 541, loss 32.728397369384766
epoch 542, loss 32.731266021728516
epoch 543, loss 32.716331481933594
epoch 544, loss 32.735191345214844
epoch 545, loss 32.717262268066406
epoch 546, loss 32.73314666748047
epoch 547, loss 32.722015380859375
epoch 548, loss 32.72882843017578
epoch 549, loss 32.740875244140625
epoch 550, loss 32.69458770751953
epoch 551, loss 32.75197982788086
epoch 552, loss 32.725425720214844
epoch 553, loss 32.72516632080078
epoch 554, loss 32.69805145263672
epoch 555, loss 32.748138427734375
epoch 556, loss 32.729366302490234
epoch 557, loss 32.73893737792969
epoch 558, loss 32.731380462646484
epoch 559, loss 32.74140930175781
epoch 560, loss 32.7730712890625
epoch 561, loss 32.753013610839844
epoch 562, loss 32.7301139831543
epoch 563, loss 32.73936080932617
epoch 564, loss 32.74030685424805
epoch 565, loss 32.73431396484375
epoch 566, loss 32.706905364990234
epoch 567, loss 32.74583053588867
epoch 568, loss 32.73914337158203
epoch 569, loss 32.751991271972656
epoch 570, loss 32.75054168701172
epoch 571, loss 32.737876892089844
epoch 572, loss 32.72462463378906
epoch 573, loss 32.70866775512695
epoch 574, loss 32.72358322143555
epoch 575, loss 32.73199462890625
epoch 576, loss 32.75365447998047
epoch 577, loss 32.74519348144531



2024-12-11 14:38:04
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
epoch 0, loss 9.423739433288574
epoch 1, loss 9.463780403137207
epoch 2, loss 9.490121841430664
epoch 3, loss 9.554213523864746
epoch 4, loss 9.665349006652832
epoch 5, loss 9.727090835571289
epoch 6, loss 9.781908988952637
epoch 7, loss 9.88001823425293
epoch 8, loss 9.956514358520508
epoch 9, loss 10.04286003112793
epoch 10, loss 10.100695610046387
epoch 11, loss 10.173013687133789
epoch 12, loss 10.297701835632324
epoch 13, loss 10.328242301940918
epoch 14, loss 10.388628005981445
epoch 15, loss 10.459817886352539
epoch 16, loss 10.60407829284668
epoch 17, loss 10.651302337646484
epoch 18, loss 10.745355606079102
epoch 19, loss 10.831449508666992
epoch 20, loss 10.887500762939453
epoch 21, loss 10.973230361938477
epoch 22, loss 11.038320541381836
epoch 23, loss 11.165763854980469
epoch 24, loss 11.219328880310059
epoch 25, loss 11.367193222045898
epoch 26, loss 11.388114929199219
epoch 27, loss 11.452812194824219
epoch 28, loss 11.529638290405273
epoch 29, loss 11.688777923583984
epoch 30, loss 11.816972732543945
epoch 31, loss 11.855853080749512
epoch 32, loss 11.961462020874023
epoch 33, loss 12.050898551940918
epoch 34, loss 12.160597801208496
epoch 35, loss 12.33053970336914
epoch 36, loss 12.38922119140625
epoch 37, loss 12.52929973602295
epoch 38, loss 12.697540283203125
epoch 39, loss 12.773394584655762
epoch 40, loss 12.83515453338623
epoch 41, loss 13.028875350952148
epoch 42, loss 13.167945861816406
epoch 43, loss 13.238592147827148
epoch 44, loss 13.404997825622559
epoch 45, loss 13.534320831298828
epoch 46, loss 13.663986206054688
epoch 47, loss 13.823179244995117
epoch 48, loss 13.991561889648438
epoch 49, loss 14.102396965026855
epoch 50, loss 14.284111022949219
epoch 51, loss 14.465547561645508
epoch 52, loss 14.514312744140625
epoch 53, loss 14.757582664489746
epoch 54, loss 14.886222839355469
epoch 55, loss 15.03480339050293
epoch 56, loss 15.23593521118164
epoch 57, loss 15.414397239685059
epoch 58, loss 15.525070190429688
epoch 59, loss 15.759872436523438
epoch 60, loss 15.863126754760742
epoch 61, loss 16.093069076538086
epoch 62, loss 16.21712303161621
epoch 63, loss 16.412368774414062
epoch 64, loss 16.55817413330078
epoch 65, loss 16.72122573852539
epoch 66, loss 16.846004486083984
epoch 67, loss 17.029571533203125
epoch 68, loss 17.224905014038086
epoch 69, loss 17.349544525146484
epoch 70, loss 17.52174186706543
epoch 71, loss 17.669483184814453
epoch 72, loss 17.80866050720215
epoch 73, loss 18.009414672851562
epoch 74, loss 18.151323318481445
epoch 75, loss 18.271015167236328
epoch 76, loss 18.380422592163086
epoch 77, loss 18.631454467773438
epoch 78, loss 18.7862548828125
epoch 79, loss 18.87903594970703
epoch 80, loss 19.023584365844727
epoch 81, loss 19.175418853759766
epoch 82, loss 19.262557983398438
epoch 83, loss 19.417499542236328
epoch 84, loss 19.568906784057617
epoch 85, loss 19.7491512298584
epoch 86, loss 19.860864639282227
epoch 87, loss 20.06045150756836
epoch 88, loss 20.17232322692871
epoch 89, loss 20.32953643798828
epoch 90, loss 20.41254425048828
epoch 91, loss 20.569543838500977
epoch 92, loss 20.688995361328125
epoch 93, loss 20.858701705932617
epoch 94, loss 21.059932708740234
epoch 95, loss 21.15987777709961
epoch 96, loss 21.302753448486328
epoch 97, loss 21.4091854095459
epoch 98, loss 21.614797592163086
	speed: 0.4181s/iter; left time: 1693.3957s
epoch 99, loss 21.73517417907715
epoch 100, loss 21.96175765991211
epoch 101, loss 22.03987693786621
epoch 102, loss 22.28969383239746
epoch 103, loss 22.401081085205078
epoch 104, loss 22.60327911376953
epoch 105, loss 22.75027084350586
epoch 106, loss 22.89143943786621
epoch 107, loss 23.01190757751465
epoch 108, loss 23.147945404052734
epoch 109, loss 23.260364532470703
epoch 110, loss 23.403030395507812
epoch 111, loss 23.512451171875
epoch 112, loss 23.60639190673828
epoch 113, loss 23.746623992919922
epoch 114, loss 23.824207305908203
epoch 115, loss 23.91720962524414
epoch 116, loss 24.037044525146484
epoch 117, loss 24.113323211669922
epoch 118, loss 24.207733154296875
epoch 119, loss 24.28204917907715
epoch 120, loss 24.39049530029297
epoch 121, loss 24.496747970581055
epoch 122, loss 24.554227828979492
epoch 123, loss 24.68885040283203
epoch 124, loss 24.753307342529297
epoch 125, loss 24.844350814819336
epoch 126, loss 24.958776473999023
epoch 127, loss 25.058589935302734
epoch 128, loss 25.140745162963867
epoch 129, loss 25.246381759643555
epoch 130, loss 25.345256805419922
epoch 131, loss 25.420787811279297
epoch 132, loss 25.531036376953125
epoch 133, loss 25.61341094970703
epoch 134, loss 25.699628829956055
epoch 135, loss 25.786657333374023
epoch 136, loss 25.86652183532715
epoch 137, loss 25.90962028503418
epoch 138, loss 25.969287872314453
epoch 139, loss 26.028926849365234
epoch 140, loss 26.07611846923828
epoch 141, loss 26.14070701599121
epoch 142, loss 26.153654098510742
epoch 143, loss 26.211894989013672
epoch 144, loss 26.275909423828125
epoch 145, loss 26.310928344726562
epoch 146, loss 26.372085571289062
epoch 147, loss 26.423748016357422
epoch 148, loss 26.44904136657715
epoch 149, loss 26.500274658203125
epoch 150, loss 26.5434627532959
epoch 151, loss 26.602584838867188
epoch 152, loss 26.648849487304688
epoch 153, loss 26.69458770751953
epoch 154, loss 26.733434677124023
epoch 155, loss 26.788244247436523
epoch 156, loss 26.845375061035156
epoch 157, loss 26.8967342376709
epoch 158, loss 26.944934844970703
epoch 159, loss 26.995595932006836
epoch 160, loss 27.025863647460938
epoch 161, loss 27.04085350036621
epoch 162, loss 27.106155395507812
epoch 163, loss 27.131366729736328
epoch 164, loss 27.200414657592773
epoch 165, loss 27.237823486328125
epoch 166, loss 27.2733154296875
epoch 167, loss 27.30335235595703
epoch 168, loss 27.358871459960938
epoch 169, loss 27.41356658935547
epoch 170, loss 27.45428466796875
epoch 171, loss 27.50844383239746
epoch 172, loss 27.55177879333496
epoch 173, loss 27.600364685058594
epoch 174, loss 27.649639129638672
epoch 175, loss 27.713363647460938
epoch 176, loss 27.732261657714844
epoch 177, loss 27.782766342163086
epoch 178, loss 27.822994232177734
epoch 179, loss 27.880966186523438
epoch 180, loss 27.94698715209961
epoch 181, loss 27.969213485717773
epoch 182, loss 28.020702362060547
epoch 183, loss 28.091983795166016
epoch 184, loss 28.119037628173828
epoch 185, loss 28.159215927124023
epoch 186, loss 28.21480941772461
epoch 187, loss 28.298269271850586
epoch 188, loss 28.35421371459961
epoch 189, loss 28.399070739746094
epoch 190, loss 28.46759605407715
epoch 191, loss 28.524093627929688
epoch 192, loss 28.561199188232422
epoch 193, loss 28.63134765625
epoch 194, loss 28.690078735351562
epoch 195, loss 28.759002685546875
epoch 196, loss 28.78705406188965
epoch 197, loss 28.831157684326172
epoch 198, loss 28.86525535583496
	speed: 0.3977s/iter; left time: 1570.8784s
epoch 199, loss 28.92328453063965
epoch 200, loss 28.941497802734375
epoch 201, loss 29.017906188964844
epoch 202, loss 29.036603927612305
epoch 203, loss 29.05575180053711
epoch 204, loss 29.085683822631836
epoch 205, loss 29.162353515625
epoch 206, loss 29.148494720458984
epoch 207, loss 29.194927215576172
epoch 208, loss 29.21261978149414
epoch 209, loss 29.234107971191406
epoch 210, loss 29.31148338317871
epoch 211, loss 29.300199508666992
epoch 212, loss 29.329421997070312
epoch 213, loss 29.37995719909668
epoch 214, loss 29.405996322631836
epoch 215, loss 29.433029174804688
epoch 216, loss 29.45669937133789
epoch 217, loss 29.493694305419922
epoch 218, loss 29.51844024658203
epoch 219, loss 29.575302124023438
epoch 220, loss 29.59444808959961
epoch 221, loss 29.63631820678711
epoch 222, loss 29.667343139648438
epoch 223, loss 29.70920181274414
epoch 224, loss 29.73794174194336
epoch 225, loss 29.761625289916992
epoch 226, loss 29.773815155029297
epoch 227, loss 29.804580688476562
epoch 228, loss 29.823169708251953
epoch 229, loss 29.862470626831055
epoch 230, loss 29.863834381103516
epoch 231, loss 29.901721954345703
epoch 232, loss 29.90914535522461
epoch 233, loss 29.946531295776367
epoch 234, loss 29.97408676147461
epoch 235, loss 29.9605770111084
epoch 236, loss 30.017303466796875
epoch 237, loss 30.043996810913086
epoch 238, loss 30.08123779296875
epoch 239, loss 30.096410751342773
epoch 240, loss 30.11595916748047
epoch 241, loss 30.130125045776367
epoch 242, loss 30.147830963134766
epoch 243, loss 30.17733383178711
epoch 244, loss 30.194015502929688
epoch 245, loss 30.2042293548584
epoch 246, loss 30.22395133972168
epoch 247, loss 30.230392456054688
epoch 248, loss 30.237258911132812
epoch 249, loss 30.266210556030273
epoch 250, loss 30.29232406616211
epoch 251, loss 30.308324813842773
epoch 252, loss 30.31635093688965
epoch 253, loss 30.36025047302246
epoch 254, loss 30.356916427612305
epoch 255, loss 30.39522933959961
epoch 256, loss 30.395719528198242
epoch 257, loss 30.39325523376465
epoch 258, loss 30.44144058227539
epoch 259, loss 30.430423736572266
epoch 260, loss 30.469640731811523
epoch 261, loss 30.474502563476562
epoch 262, loss 30.504262924194336
epoch 263, loss 30.501285552978516
epoch 264, loss 30.532711029052734
epoch 265, loss 30.535533905029297
epoch 266, loss 30.55721664428711
epoch 267, loss 30.55791473388672
epoch 268, loss 30.569089889526367
epoch 269, loss 30.59356689453125
epoch 270, loss 30.58116912841797
epoch 271, loss 30.62990951538086
epoch 272, loss 30.638687133789062
epoch 273, loss 30.65205955505371
epoch 274, loss 30.659900665283203
epoch 275, loss 30.67291831970215
epoch 276, loss 30.707340240478516
epoch 277, loss 30.711576461791992
epoch 278, loss 30.737625122070312
epoch 279, loss 30.73907470703125
epoch 280, loss 30.763029098510742
epoch 281, loss 30.781295776367188
epoch 282, loss 30.802101135253906
epoch 283, loss 30.812877655029297
epoch 284, loss 30.830093383789062
epoch 285, loss 30.85080337524414
epoch 286, loss 30.831546783447266
epoch 287, loss 30.85851287841797
epoch 288, loss 30.862722396850586
epoch 289, loss 30.865432739257812
epoch 290, loss 30.894222259521484
epoch 291, loss 30.915037155151367
epoch 292, loss 30.929279327392578
epoch 293, loss 30.951148986816406
epoch 294, loss 30.981300354003906
epoch 295, loss 30.981781005859375
epoch 296, loss 31.001415252685547
epoch 297, loss 31.008750915527344
epoch 298, loss 31.05324363708496
	speed: 0.3991s/iter; left time: 1536.4734s
epoch 299, loss 31.043088912963867
epoch 300, loss 31.077518463134766
epoch 301, loss 31.08820915222168
epoch 302, loss 31.103307723999023
epoch 303, loss 31.146718978881836
epoch 304, loss 31.146451950073242
epoch 305, loss 31.154918670654297
epoch 306, loss 31.178564071655273
epoch 307, loss 31.187841415405273
epoch 308, loss 31.19471549987793
epoch 309, loss 31.227413177490234
epoch 310, loss 31.258808135986328
epoch 311, loss 31.261754989624023
epoch 312, loss 31.249706268310547
epoch 313, loss 31.25649642944336
epoch 314, loss 31.296585083007812
epoch 315, loss 31.31235122680664
epoch 316, loss 31.312694549560547
epoch 317, loss 31.31344223022461
epoch 318, loss 31.34699249267578
epoch 319, loss 31.353561401367188
epoch 320, loss 31.374217987060547
epoch 321, loss 31.385086059570312
epoch 322, loss 31.391332626342773
epoch 323, loss 31.414030075073242
epoch 324, loss 31.44972801208496
epoch 325, loss 31.442428588867188
epoch 326, loss 31.463468551635742
epoch 327, loss 31.45993423461914
epoch 328, loss 31.501157760620117
epoch 329, loss 31.51253318786621
epoch 330, loss 31.55890464782715
epoch 331, loss 31.56539535522461
epoch 332, loss 31.573894500732422
epoch 333, loss 31.562850952148438
epoch 334, loss 31.643688201904297
epoch 335, loss 31.62409019470215
epoch 336, loss 31.592947006225586
epoch 337, loss 31.630828857421875
epoch 338, loss 31.67369842529297
epoch 339, loss 31.649700164794922
epoch 340, loss 31.703582763671875
epoch 341, loss 31.672897338867188
epoch 342, loss 31.703224182128906
epoch 343, loss 31.726964950561523
epoch 344, loss 31.726375579833984
epoch 345, loss 31.737302780151367
epoch 346, loss 31.76632308959961
epoch 347, loss 31.756404876708984
epoch 348, loss 31.743988037109375
epoch 349, loss 31.75966453552246
epoch 350, loss 31.81719207763672
epoch 351, loss 31.833282470703125
epoch 352, loss 31.842504501342773
epoch 353, loss 31.851688385009766
epoch 354, loss 31.857887268066406
epoch 355, loss 31.87116241455078
epoch 356, loss 31.85690689086914
epoch 357, loss 31.889373779296875
epoch 358, loss 31.898069381713867
epoch 359, loss 31.90279769897461
epoch 360, loss 31.934520721435547
epoch 361, loss 31.934215545654297
epoch 362, loss 31.98166275024414
epoch 363, loss 31.98444938659668
epoch 364, loss 31.99212646484375
epoch 365, loss 31.981887817382812
epoch 366, loss 31.97341537475586
epoch 367, loss 32.01367950439453
epoch 368, loss 31.984516143798828
epoch 369, loss 32.00900650024414
epoch 370, loss 32.032081604003906
epoch 371, loss 32.021514892578125
epoch 372, loss 32.03778076171875
epoch 373, loss 32.033477783203125
epoch 374, loss 32.04338455200195
epoch 375, loss 32.08357238769531
epoch 376, loss 32.09206771850586
epoch 377, loss 32.076499938964844
epoch 378, loss 32.08013153076172
epoch 379, loss 32.10051345825195
epoch 380, loss 32.08778381347656
epoch 381, loss 32.10264587402344
epoch 382, loss 32.0929069519043
epoch 383, loss 32.10285186767578
epoch 384, loss 32.15772247314453
epoch 385, loss 32.152767181396484
epoch 386, loss 32.157508850097656
epoch 387, loss 32.14472961425781
epoch 388, loss 32.158653259277344
epoch 389, loss 32.17310333251953
epoch 390, loss 32.14577865600586
epoch 391, loss 32.12257385253906
epoch 392, loss 32.17129135131836
epoch 393, loss 32.158565521240234
epoch 394, loss 32.20237350463867
epoch 395, loss 32.17592239379883
epoch 396, loss 32.17236328125
epoch 397, loss 32.205963134765625
epoch 398, loss 32.19927978515625
	speed: 0.3999s/iter; left time: 1499.4826s
epoch 399, loss 32.22861099243164
epoch 400, loss 32.210906982421875
epoch 401, loss 32.211021423339844
epoch 402, loss 32.208045959472656
epoch 403, loss 32.24997329711914
epoch 404, loss 32.22411346435547
epoch 405, loss 32.261390686035156
epoch 406, loss 32.27081298828125
epoch 407, loss 32.27429962158203
epoch 408, loss 32.249549865722656
epoch 409, loss 32.26941680908203
epoch 410, loss 32.24758529663086
epoch 411, loss 32.278953552246094
epoch 412, loss 32.293975830078125
epoch 413, loss 32.302207946777344
epoch 414, loss 32.28630447387695
epoch 415, loss 32.274776458740234
epoch 416, loss 32.30754089355469
epoch 417, loss 32.309654235839844
epoch 418, loss 32.276512145996094
epoch 419, loss 32.31136703491211
epoch 420, loss 32.292545318603516
epoch 421, loss 32.32920837402344
epoch 422, loss 32.32128143310547
epoch 423, loss 32.3093147277832
epoch 424, loss 32.33172607421875
epoch 425, loss 32.324092864990234
epoch 426, loss 32.32270812988281
epoch 427, loss 32.330657958984375
epoch 428, loss 32.339447021484375
epoch 429, loss 32.361236572265625
epoch 430, loss 32.360809326171875
epoch 431, loss 32.36888122558594
epoch 432, loss 32.34031295776367
epoch 433, loss 32.360992431640625
epoch 434, loss 32.37793731689453
epoch 435, loss 32.35303497314453
epoch 436, loss 32.365577697753906
epoch 437, loss 32.37146759033203
epoch 438, loss 32.37250518798828
epoch 439, loss 32.36379623413086
epoch 440, loss 32.37013244628906
epoch 441, loss 32.38752746582031
epoch 442, loss 32.38093566894531
epoch 443, loss 32.39710235595703
epoch 444, loss 32.38951110839844
epoch 445, loss 32.376197814941406
epoch 446, loss 32.40699768066406
epoch 447, loss 32.36554718017578
epoch 448, loss 32.41777801513672
epoch 449, loss 32.40153884887695
epoch 450, loss 32.397457122802734
epoch 451, loss 32.40110778808594
epoch 452, loss 32.432159423828125
epoch 453, loss 32.39734649658203
epoch 454, loss 32.395835876464844
epoch 455, loss 32.408172607421875
epoch 456, loss 32.42301559448242
epoch 457, loss 32.42304992675781
epoch 458, loss 32.414146423339844
epoch 459, loss 32.428436279296875
epoch 460, loss 32.447105407714844
epoch 461, loss 32.45050048828125
epoch 462, loss 32.414939880371094
epoch 463, loss 32.45023727416992
epoch 464, loss 32.4508056640625
epoch 465, loss 32.44378662109375
epoch 466, loss 32.47016906738281
epoch 467, loss 32.43212127685547
epoch 468, loss 32.456825256347656
epoch 469, loss 32.46775436401367
epoch 470, loss 32.44178771972656
epoch 471, loss 32.46517562866211
epoch 472, loss 32.44564437866211
epoch 473, loss 32.45806884765625
epoch 474, loss 32.46839904785156
epoch 475, loss 32.48930358886719
epoch 476, loss 32.4831657409668
epoch 477, loss 32.49483108520508
epoch 478, loss 32.48715591430664
epoch 479, loss 32.450714111328125
epoch 480, loss 32.468360900878906
epoch 481, loss 32.503055572509766
epoch 482, loss 32.47742462158203
epoch 483, loss 32.46556091308594
epoch 484, loss 32.484962463378906
epoch 485, loss 32.48066329956055
epoch 486, loss 32.48219680786133
epoch 487, loss 32.479488372802734
epoch 488, loss 32.50022888183594
epoch 489, loss 32.51864242553711
epoch 490, loss 32.493194580078125
epoch 491, loss 32.485923767089844
epoch 492, loss 32.494239807128906
epoch 493, loss 32.519412994384766
epoch 494, loss 32.516944885253906
epoch 495, loss 32.521583557128906
epoch 496, loss 32.5565185546875
epoch 497, loss 32.51909637451172
epoch 498, loss 32.50311279296875
	speed: 0.4001s/iter; left time: 1460.5394s
epoch 499, loss 32.528053283691406
epoch 500, loss 32.52747344970703
epoch 501, loss 32.531349182128906
epoch 502, loss 32.530792236328125
epoch 503, loss 32.5367317199707
epoch 504, loss 32.55532455444336
epoch 505, loss 32.50978088378906
epoch 506, loss 32.56876754760742
epoch 507, loss 32.520050048828125
epoch 508, loss 32.52002716064453
epoch 509, loss 32.5429801940918
epoch 510, loss 32.52272415161133
epoch 511, loss 32.569252014160156
epoch 512, loss 32.544525146484375
epoch 513, loss 32.538700103759766
epoch 514, loss 32.56861114501953
epoch 515, loss 32.551902770996094
epoch 516, loss 32.55626678466797
epoch 517, loss 32.548255920410156
epoch 518, loss 32.56298065185547
epoch 519, loss 32.545928955078125
epoch 520, loss 32.54292297363281
epoch 521, loss 32.54841613769531
epoch 522, loss 32.55447006225586
epoch 523, loss 32.551361083984375
epoch 524, loss 32.53667068481445
epoch 525, loss 32.54060363769531
epoch 526, loss 32.55485916137695
epoch 527, loss 32.5641975402832
epoch 528, loss 32.56734848022461
epoch 529, loss 32.56523132324219
epoch 530, loss 32.56269073486328
epoch 531, loss 32.603572845458984
epoch 532, loss 32.56604766845703
epoch 533, loss 32.578853607177734
epoch 534, loss 32.55572509765625
epoch 535, loss 32.582252502441406
epoch 536, loss 32.5740966796875
epoch 537, loss 32.58543395996094
epoch 538, loss 32.60053634643555
epoch 539, loss 32.580047607421875
epoch 540, loss 32.58771896362305
epoch 541, loss 32.6158332824707
epoch 542, loss 32.580604553222656
epoch 543, loss 32.594139099121094
epoch 544, loss 32.57795333862305
epoch 545, loss 32.6142692565918
epoch 546, loss 32.608543395996094
epoch 547, loss 32.601776123046875
epoch 548, loss 32.59773635864258
epoch 549, loss 32.61678695678711
epoch 550, loss 32.61418914794922
epoch 551, loss 32.589656829833984
epoch 552, loss 32.59666061401367
epoch 553, loss 32.609107971191406
epoch 554, loss 32.607295989990234
epoch 555, loss 32.5886344909668
epoch 556, loss 32.63080978393555
epoch 557, loss 32.600013732910156
epoch 558, loss 32.59313201904297
epoch 559, loss 32.604217529296875
epoch 560, loss 32.597930908203125
epoch 561, loss 32.5966911315918
epoch 562, loss 32.6049919128418
epoch 563, loss 32.63445281982422
epoch 564, loss 32.59922409057617
epoch 565, loss 32.63880920410156
epoch 566, loss 32.623756408691406
epoch 567, loss 32.64633560180664
epoch 568, loss 32.62683868408203
epoch 569, loss 32.63142395019531
epoch 570, loss 32.62300109863281
epoch 571, loss 32.634117126464844
epoch 572, loss 32.6373291015625
epoch 573, loss 32.633785247802734
epoch 574, loss 32.6284294128418
epoch 575, loss 32.620765686035156
epoch 576, loss 32.6306037902832
epoch 577, loss 32.63774108886719
epoch 578, loss 32.62870788574219
epoch 579, loss 32.61384201049805
epoch 580, loss 32.654052734375
epoch 581, loss 32.653709411621094
epoch 582, loss 32.634124755859375
epoch 583, loss 32.6318473815918
epoch 584, loss 32.62669372558594
epoch 585, loss 32.64286804199219
epoch 586, loss 32.6260871887207
epoch 587, loss 32.68593215942383
epoch 588, loss 32.633052825927734
epoch 589, loss 32.6253662109375
epoch 590, loss 32.63065719604492
epoch 591, loss 32.660701751708984
epoch 592, loss 32.62726593017578
epoch 593, loss 32.6493034362793
epoch 594, loss 32.62548828125
epoch 595, loss 32.62582778930664
epoch 596, loss 32.660640716552734
epoch 597, loss 32.63501739501953
epoch 598, loss 32.6494140625
	speed: 0.4004s/iter; left time: 1421.2735s
epoch 599, loss 32.67606735229492
epoch 600, loss 32.64378356933594
epoch 601, loss 32.65302658081055
epoch 602, loss 32.63515090942383
epoch 603, loss 32.672393798828125
epoch 604, loss 32.645294189453125
epoch 605, loss 32.68610382080078
epoch 606, loss 32.653804779052734
epoch 607, loss 32.64340591430664
epoch 608, loss 32.659271240234375
epoch 609, loss 32.660240173339844
epoch 610, loss 32.68020248413086
epoch 611, loss 32.63384246826172
epoch 612, loss 32.684288024902344
epoch 613, loss 32.66419982910156
epoch 614, loss 32.663536071777344
epoch 615, loss 32.69538116455078
epoch 616, loss 32.654396057128906
epoch 617, loss 32.69312286376953
epoch 618, loss 32.67308044433594
epoch 619, loss 32.686492919921875
epoch 620, loss 32.6943473815918
epoch 621, loss 32.65279769897461
epoch 622, loss 32.720008850097656
epoch 623, loss 32.69877624511719
epoch 624, loss 32.682273864746094
epoch 625, loss 32.67713165283203
epoch 626, loss 32.664398193359375
epoch 627, loss 32.67247772216797
epoch 628, loss 32.65974807739258
epoch 629, loss 32.675594329833984
epoch 630, loss 32.69471740722656
epoch 631, loss 32.70233917236328
epoch 632, loss 32.66071319580078
epoch 633, loss 32.66674041748047
epoch 634, loss 32.67454528808594
epoch 635, loss 32.66276168823242
epoch 636, loss 32.65721893310547
epoch 637, loss 32.65857696533203
epoch 638, loss 32.67189025878906
epoch 639, loss 32.68803787231445
epoch 640, loss 32.68901824951172
epoch 641, loss 32.66138458251953
epoch 642, loss 32.699851989746094
epoch 643, loss 32.69244384765625
epoch 644, loss 32.68663787841797
epoch 645, loss 32.665077209472656
epoch 646, loss 32.65477752685547
epoch 647, loss 32.684871673583984
epoch 648, loss 32.65900421142578
epoch 649, loss 32.704524993896484
epoch 650, loss 32.67792510986328
epoch 651, loss 32.67664337158203
epoch 652, loss 32.67348861694336
epoch 653, loss 32.69758224487305
epoch 654, loss 32.70173645019531
epoch 655, loss 32.68184280395508
epoch 656, loss 32.689064025878906
epoch 657, loss 32.70914077758789



2024-12-11 14:44:18
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-11 14:48:48
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
epoch 0, loss_perior 9.399408340454102, loss_series 9.399408340454102
epoch 1, loss_perior 9.422945022583008, loss_series 9.422945022583008
epoch 2, loss_perior 9.479437828063965, loss_series 9.479437828063965
epoch 3, loss_perior 9.540647506713867, loss_series 9.540647506713867
epoch 4, loss_perior 9.623390197753906, loss_series 9.623390197753906
epoch 5, loss_perior 9.648191452026367, loss_series 9.648191452026367
epoch 6, loss_perior 9.726957321166992, loss_series 9.726957321166992
epoch 7, loss_perior 9.821206092834473, loss_series 9.821206092834473
epoch 8, loss_perior 9.923301696777344, loss_series 9.923301696777344
epoch 9, loss_perior 9.980430603027344, loss_series 9.980430603027344
epoch 10, loss_perior 10.065862655639648, loss_series 10.065862655639648
epoch 11, loss_perior 10.15376091003418, loss_series 10.15376091003418
epoch 12, loss_perior 10.248385429382324, loss_series 10.248385429382324
epoch 13, loss_perior 10.340351104736328, loss_series 10.340351104736328
epoch 14, loss_perior 10.387032508850098, loss_series 10.387032508850098
epoch 15, loss_perior 10.459756851196289, loss_series 10.459756851196289
epoch 16, loss_perior 10.536989212036133, loss_series 10.536989212036133
epoch 17, loss_perior 10.576684951782227, loss_series 10.576684951782227
epoch 18, loss_perior 10.687873840332031, loss_series 10.687873840332031
epoch 19, loss_perior 10.789243698120117, loss_series 10.789243698120117
epoch 20, loss_perior 10.789206504821777, loss_series 10.789206504821777
epoch 21, loss_perior 10.883232116699219, loss_series 10.883232116699219
epoch 22, loss_perior 10.97751235961914, loss_series 10.97751235961914
epoch 23, loss_perior 11.082606315612793, loss_series 11.082606315612793
epoch 24, loss_perior 11.119281768798828, loss_series 11.119281768798828
epoch 25, loss_perior 11.216653823852539, loss_series 11.216653823852539
epoch 26, loss_perior 11.300108909606934, loss_series 11.300108909606934
epoch 27, loss_perior 11.373046875, loss_series 11.373046875
epoch 28, loss_perior 11.402122497558594, loss_series 11.402122497558594
epoch 29, loss_perior 11.485075950622559, loss_series 11.485075950622559
epoch 30, loss_perior 11.534902572631836, loss_series 11.534902572631836
epoch 31, loss_perior 11.751136779785156, loss_series 11.751136779785156
epoch 32, loss_perior 11.73546028137207, loss_series 11.73546028137207
epoch 33, loss_perior 11.771410942077637, loss_series 11.771410942077637
epoch 34, loss_perior 11.901782989501953, loss_series 11.901782989501953
epoch 35, loss_perior 12.095919609069824, loss_series 12.095919609069824
epoch 36, loss_perior 12.154020309448242, loss_series 12.154020309448242
epoch 37, loss_perior 12.174091339111328, loss_series 12.174091339111328
epoch 38, loss_perior 12.302202224731445, loss_series 12.302202224731445
epoch 39, loss_perior 12.413213729858398, loss_series 12.413213729858398
epoch 40, loss_perior 12.47864818572998, loss_series 12.47864818572998
epoch 41, loss_perior 12.593469619750977, loss_series 12.593469619750977
epoch 42, loss_perior 12.78486156463623, loss_series 12.78486156463623
epoch 43, loss_perior 12.900851249694824, loss_series 12.900851249694824
epoch 44, loss_perior 12.974592208862305, loss_series 12.974592208862305
epoch 45, loss_perior 13.05771541595459, loss_series 13.05771541595459
epoch 46, loss_perior 13.166807174682617, loss_series 13.166807174682617
epoch 47, loss_perior 13.332685470581055, loss_series 13.332685470581055
epoch 48, loss_perior 13.447101593017578, loss_series 13.447101593017578
epoch 49, loss_perior 13.641170501708984, loss_series 13.641170501708984
epoch 50, loss_perior 13.812780380249023, loss_series 13.812780380249023
epoch 51, loss_perior 13.834753036499023, loss_series 13.834753036499023
epoch 52, loss_perior 14.07655143737793, loss_series 14.07655143737793
epoch 53, loss_perior 14.208613395690918, loss_series 14.208613395690918
epoch 54, loss_perior 14.300939559936523, loss_series 14.300939559936523
epoch 55, loss_perior 14.478059768676758, loss_series 14.478059768676758
epoch 56, loss_perior 14.58600902557373, loss_series 14.58600902557373
epoch 57, loss_perior 14.741510391235352, loss_series 14.741510391235352
epoch 58, loss_perior 14.86744499206543, loss_series 14.86744499206543
epoch 59, loss_perior 15.110209465026855, loss_series 15.110209465026855
epoch 60, loss_perior 15.200738906860352, loss_series 15.200738906860352
epoch 61, loss_perior 15.395124435424805, loss_series 15.395124435424805
epoch 62, loss_perior 15.54731559753418, loss_series 15.54731559753418
epoch 63, loss_perior 15.675407409667969, loss_series 15.675407409667969
epoch 64, loss_perior 15.942859649658203, loss_series 15.942859649658203
epoch 65, loss_perior 15.983515739440918, loss_series 15.983515739440918
epoch 66, loss_perior 16.211153030395508, loss_series 16.211153030395508
epoch 67, loss_perior 16.340534210205078, loss_series 16.340534210205078
epoch 68, loss_perior 16.50111961364746, loss_series 16.50111961364746
epoch 69, loss_perior 16.601966857910156, loss_series 16.601966857910156
epoch 70, loss_perior 16.847270965576172, loss_series 16.847270965576172
epoch 71, loss_perior 16.946239471435547, loss_series 16.946239471435547
epoch 72, loss_perior 17.116968154907227, loss_series 17.116968154907227
epoch 73, loss_perior 17.190290451049805, loss_series 17.190290451049805
epoch 74, loss_perior 17.457904815673828, loss_series 17.457904815673828
epoch 75, loss_perior 17.543441772460938, loss_series 17.543441772460938
epoch 76, loss_perior 17.68829345703125, loss_series 17.68829345703125
epoch 77, loss_perior 17.784149169921875, loss_series 17.784149169921875
epoch 78, loss_perior 17.937400817871094, loss_series 17.937400817871094
epoch 79, loss_perior 18.160362243652344, loss_series 18.160362243652344
epoch 80, loss_perior 18.134174346923828, loss_series 18.134174346923828
epoch 81, loss_perior 18.325340270996094, loss_series 18.325340270996094
epoch 82, loss_perior 18.556570053100586, loss_series 18.556570053100586
epoch 83, loss_perior 18.56451416015625, loss_series 18.56451416015625
epoch 84, loss_perior 18.634883880615234, loss_series 18.634883880615234
epoch 85, loss_perior 18.845420837402344, loss_series 18.845420837402344
epoch 86, loss_perior 18.995723724365234, loss_series 18.995723724365234
epoch 87, loss_perior 19.154430389404297, loss_series 19.154430389404297
epoch 88, loss_perior 19.295251846313477, loss_series 19.295251846313477
epoch 89, loss_perior 19.320751190185547, loss_series 19.320751190185547
epoch 90, loss_perior 19.506790161132812, loss_series 19.506790161132812
epoch 91, loss_perior 19.592573165893555, loss_series 19.592573165893555
epoch 92, loss_perior 19.764480590820312, loss_series 19.764480590820312
epoch 93, loss_perior 19.898075103759766, loss_series 19.898075103759766
epoch 94, loss_perior 20.027423858642578, loss_series 20.027423858642578
epoch 95, loss_perior 20.064109802246094, loss_series 20.064109802246094
epoch 96, loss_perior 20.274335861206055, loss_series 20.274335861206055
epoch 97, loss_perior 20.477306365966797, loss_series 20.477306365966797
epoch 98, loss_perior 20.548383712768555, loss_series 20.548383712768555
	speed: 0.4190s/iter; left time: 1696.8454s
epoch 99, loss_perior 20.659530639648438, loss_series 20.659530639648438
epoch 100, loss_perior 20.80400276184082, loss_series 20.80400276184082
epoch 101, loss_perior 20.934656143188477, loss_series 20.934656143188477
epoch 102, loss_perior 21.130233764648438, loss_series 21.130233764648438
epoch 103, loss_perior 21.17275619506836, loss_series 21.17275619506836
epoch 104, loss_perior 21.307876586914062, loss_series 21.307876586914062
epoch 105, loss_perior 21.464401245117188, loss_series 21.464401245117188
epoch 106, loss_perior 21.512868881225586, loss_series 21.512868881225586
epoch 107, loss_perior 21.670623779296875, loss_series 21.670623779296875
epoch 108, loss_perior 21.75954246520996, loss_series 21.75954246520996
epoch 109, loss_perior 21.882186889648438, loss_series 21.882186889648438
epoch 110, loss_perior 21.99520492553711, loss_series 21.99520492553711
epoch 111, loss_perior 22.078092575073242, loss_series 22.078092575073242
epoch 112, loss_perior 22.13087272644043, loss_series 22.13087272644043
epoch 113, loss_perior 22.30617332458496, loss_series 22.30617332458496
epoch 114, loss_perior 22.3602352142334, loss_series 22.3602352142334
epoch 115, loss_perior 22.458515167236328, loss_series 22.458515167236328
epoch 116, loss_perior 22.558162689208984, loss_series 22.558162689208984
epoch 117, loss_perior 22.64937973022461, loss_series 22.64937973022461
epoch 118, loss_perior 22.853702545166016, loss_series 22.853702545166016
epoch 119, loss_perior 22.931175231933594, loss_series 22.931175231933594
epoch 120, loss_perior 23.02987289428711, loss_series 23.02987289428711
epoch 121, loss_perior 23.184120178222656, loss_series 23.184120178222656
epoch 122, loss_perior 23.263967514038086, loss_series 23.263967514038086
epoch 123, loss_perior 23.433547973632812, loss_series 23.433547973632812
epoch 124, loss_perior 23.590206146240234, loss_series 23.590206146240234
epoch 125, loss_perior 23.6871337890625, loss_series 23.6871337890625
epoch 126, loss_perior 23.76551055908203, loss_series 23.76551055908203
epoch 127, loss_perior 23.961105346679688, loss_series 23.961105346679688
epoch 128, loss_perior 24.131195068359375, loss_series 24.131195068359375
epoch 129, loss_perior 24.25076675415039, loss_series 24.25076675415039
epoch 130, loss_perior 24.367259979248047, loss_series 24.367259979248047
epoch 131, loss_perior 24.460561752319336, loss_series 24.460561752319336
epoch 132, loss_perior 24.56009292602539, loss_series 24.56009292602539
epoch 133, loss_perior 24.69628143310547, loss_series 24.69628143310547
epoch 134, loss_perior 24.755931854248047, loss_series 24.755931854248047
epoch 135, loss_perior 24.834598541259766, loss_series 24.834598541259766
epoch 136, loss_perior 24.952442169189453, loss_series 24.952442169189453
epoch 137, loss_perior 25.069799423217773, loss_series 25.069799423217773
epoch 138, loss_perior 25.154056549072266, loss_series 25.154056549072266
epoch 139, loss_perior 25.242977142333984, loss_series 25.242977142333984
epoch 140, loss_perior 25.377727508544922, loss_series 25.377727508544922
epoch 141, loss_perior 25.426916122436523, loss_series 25.426916122436523
epoch 142, loss_perior 25.556119918823242, loss_series 25.556119918823242
epoch 143, loss_perior 25.66139793395996, loss_series 25.66139793395996
epoch 144, loss_perior 25.76250457763672, loss_series 25.76250457763672
epoch 145, loss_perior 25.89093780517578, loss_series 25.89093780517578
epoch 146, loss_perior 25.995100021362305, loss_series 25.995100021362305
epoch 147, loss_perior 26.10253143310547, loss_series 26.10253143310547
epoch 148, loss_perior 26.17425537109375, loss_series 26.17425537109375
epoch 149, loss_perior 26.299999237060547, loss_series 26.299999237060547
epoch 150, loss_perior 26.38186264038086, loss_series 26.38186264038086
epoch 151, loss_perior 26.489910125732422, loss_series 26.489910125732422
epoch 152, loss_perior 26.622777938842773, loss_series 26.622777938842773
epoch 153, loss_perior 26.69692611694336, loss_series 26.69692611694336
epoch 154, loss_perior 26.794668197631836, loss_series 26.794668197631836
epoch 155, loss_perior 26.951080322265625, loss_series 26.951080322265625
epoch 156, loss_perior 27.014076232910156, loss_series 27.014076232910156
epoch 157, loss_perior 27.113895416259766, loss_series 27.113895416259766
epoch 158, loss_perior 27.228065490722656, loss_series 27.228065490722656
epoch 159, loss_perior 27.338241577148438, loss_series 27.338241577148438
epoch 160, loss_perior 27.441726684570312, loss_series 27.441726684570312
epoch 161, loss_perior 27.57752227783203, loss_series 27.57752227783203
epoch 162, loss_perior 27.68250846862793, loss_series 27.68250846862793
epoch 163, loss_perior 27.800731658935547, loss_series 27.800731658935547
epoch 164, loss_perior 27.857402801513672, loss_series 27.857402801513672
epoch 165, loss_perior 27.962766647338867, loss_series 27.962766647338867
epoch 166, loss_perior 28.056884765625, loss_series 28.056884765625
epoch 167, loss_perior 28.175018310546875, loss_series 28.175018310546875
epoch 168, loss_perior 28.347257614135742, loss_series 28.347257614135742
epoch 169, loss_perior 28.389163970947266, loss_series 28.389163970947266
epoch 170, loss_perior 28.487361907958984, loss_series 28.487361907958984
epoch 171, loss_perior 28.57228660583496, loss_series 28.57228660583496
epoch 172, loss_perior 28.691747665405273, loss_series 28.691747665405273
epoch 173, loss_perior 28.76883316040039, loss_series 28.76883316040039
epoch 174, loss_perior 28.932249069213867, loss_series 28.932249069213867
epoch 175, loss_perior 29.017471313476562, loss_series 29.017471313476562
epoch 176, loss_perior 29.143917083740234, loss_series 29.143917083740234
epoch 177, loss_perior 29.188627243041992, loss_series 29.188627243041992
epoch 178, loss_perior 29.290206909179688, loss_series 29.290206909179688
epoch 179, loss_perior 29.39656639099121, loss_series 29.39656639099121
epoch 180, loss_perior 29.461868286132812, loss_series 29.461868286132812
epoch 181, loss_perior 29.592823028564453, loss_series 29.592823028564453
epoch 182, loss_perior 29.7979793548584, loss_series 29.7979793548584
epoch 183, loss_perior 29.844676971435547, loss_series 29.844676971435547
epoch 184, loss_perior 29.887157440185547, loss_series 29.887157440185547
epoch 185, loss_perior 29.93768310546875, loss_series 29.93768310546875
epoch 186, loss_perior 30.030391693115234, loss_series 30.030391693115234
epoch 187, loss_perior 30.03223419189453, loss_series 30.03223419189453
epoch 188, loss_perior 30.14382553100586, loss_series 30.14382553100586
epoch 189, loss_perior 30.19762420654297, loss_series 30.19762420654297
epoch 190, loss_perior 30.269878387451172, loss_series 30.269878387451172
epoch 191, loss_perior 30.335966110229492, loss_series 30.335966110229492
epoch 192, loss_perior 30.400245666503906, loss_series 30.400245666503906
epoch 193, loss_perior 30.490909576416016, loss_series 30.490909576416016
epoch 194, loss_perior 30.583110809326172, loss_series 30.583110809326172
epoch 195, loss_perior 30.603628158569336, loss_series 30.603628158569336
epoch 196, loss_perior 30.633277893066406, loss_series 30.633277893066406
epoch 197, loss_perior 30.75445556640625, loss_series 30.75445556640625
epoch 198, loss_perior 30.682220458984375, loss_series 30.682220458984375
	speed: 0.3978s/iter; left time: 1571.1906s
epoch 199, loss_perior 30.696462631225586, loss_series 30.696462631225586
epoch 200, loss_perior 30.837570190429688, loss_series 30.837570190429688
epoch 201, loss_perior 30.844892501831055, loss_series 30.844892501831055
epoch 202, loss_perior 30.9328670501709, loss_series 30.9328670501709
epoch 203, loss_perior 30.900333404541016, loss_series 30.900333404541016
epoch 204, loss_perior 30.936988830566406, loss_series 30.936988830566406
epoch 205, loss_perior 30.985973358154297, loss_series 30.985973358154297
epoch 206, loss_perior 31.06441879272461, loss_series 31.06441879272461
epoch 207, loss_perior 31.054006576538086, loss_series 31.054006576538086
epoch 208, loss_perior 31.057912826538086, loss_series 31.057912826538086
epoch 209, loss_perior 31.156021118164062, loss_series 31.156021118164062
epoch 210, loss_perior 31.183456420898438, loss_series 31.183456420898438
epoch 211, loss_perior 31.22456932067871, loss_series 31.22456932067871
epoch 212, loss_perior 31.245399475097656, loss_series 31.245399475097656
epoch 213, loss_perior 31.338865280151367, loss_series 31.338865280151367
epoch 214, loss_perior 31.270427703857422, loss_series 31.270427703857422
epoch 215, loss_perior 31.348770141601562, loss_series 31.348770141601562
epoch 216, loss_perior 31.370189666748047, loss_series 31.370189666748047
epoch 217, loss_perior 31.372472763061523, loss_series 31.372472763061523
epoch 218, loss_perior 31.408916473388672, loss_series 31.408916473388672
epoch 219, loss_perior 31.423236846923828, loss_series 31.423236846923828
epoch 220, loss_perior 31.45009994506836, loss_series 31.45009994506836
epoch 221, loss_perior 31.46492576599121, loss_series 31.46492576599121
epoch 222, loss_perior 31.51788330078125, loss_series 31.51788330078125
epoch 223, loss_perior 31.564929962158203, loss_series 31.564929962158203
epoch 224, loss_perior 31.50247573852539, loss_series 31.50247573852539
epoch 225, loss_perior 31.596752166748047, loss_series 31.596752166748047



2024-12-15 12:16:08
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
epoch 0, loss_perior 9.432511329650879, loss_series 9.432511329650879
epoch 1, loss_perior 9.484650611877441, loss_series 9.484650611877441
epoch 2, loss_perior 9.528371810913086, loss_series 9.528371810913086
epoch 3, loss_perior 9.615568161010742, loss_series 9.615568161010742
epoch 4, loss_perior 9.69365119934082, loss_series 9.69365119934082
epoch 5, loss_perior 9.791790008544922, loss_series 9.791790008544922
epoch 6, loss_perior 9.860719680786133, loss_series 9.860719680786133
epoch 7, loss_perior 9.909806251525879, loss_series 9.909806251525879
epoch 8, loss_perior 9.990978240966797, loss_series 9.990978240966797
epoch 9, loss_perior 10.069263458251953, loss_series 10.069263458251953
epoch 10, loss_perior 10.136974334716797, loss_series 10.136974334716797
epoch 11, loss_perior 10.198229789733887, loss_series 10.198229789733887
epoch 12, loss_perior 10.309871673583984, loss_series 10.309871673583984
epoch 13, loss_perior 10.384164810180664, loss_series 10.384164810180664
epoch 14, loss_perior 10.493642807006836, loss_series 10.493642807006836
epoch 15, loss_perior 10.444985389709473, loss_series 10.444985389709473
epoch 16, loss_perior 10.604057312011719, loss_series 10.604057312011719
epoch 17, loss_perior 10.57463264465332, loss_series 10.57463264465332
epoch 18, loss_perior 10.642522811889648, loss_series 10.642522811889648
epoch 19, loss_perior 10.707950592041016, loss_series 10.707950592041016
epoch 20, loss_perior 10.773478507995605, loss_series 10.773478507995605
epoch 21, loss_perior 10.861830711364746, loss_series 10.861830711364746
epoch 22, loss_perior 10.91788387298584, loss_series 10.91788387298584
epoch 23, loss_perior 10.956454277038574, loss_series 10.956454277038574
epoch 24, loss_perior 11.058333396911621, loss_series 11.058333396911621
epoch 25, loss_perior 11.092336654663086, loss_series 11.092336654663086
epoch 26, loss_perior 11.211146354675293, loss_series 11.211146354675293
epoch 27, loss_perior 11.219428062438965, loss_series 11.219428062438965
epoch 28, loss_perior 11.24344539642334, loss_series 11.24344539642334
epoch 29, loss_perior 11.319816589355469, loss_series 11.319816589355469
epoch 30, loss_perior 11.443476676940918, loss_series 11.443476676940918
epoch 31, loss_perior 11.427299499511719, loss_series 11.427299499511719
epoch 32, loss_perior 11.525320053100586, loss_series 11.525320053100586
epoch 33, loss_perior 11.637628555297852, loss_series 11.637628555297852
epoch 34, loss_perior 11.65842056274414, loss_series 11.65842056274414
epoch 35, loss_perior 11.708105087280273, loss_series 11.708105087280273
epoch 36, loss_perior 11.849885940551758, loss_series 11.849885940551758
epoch 37, loss_perior 11.88297176361084, loss_series 11.88297176361084
epoch 38, loss_perior 11.894617080688477, loss_series 11.894617080688477
epoch 39, loss_perior 12.042582511901855, loss_series 12.042582511901855
epoch 40, loss_perior 12.128973960876465, loss_series 12.128973960876465
epoch 41, loss_perior 12.165580749511719, loss_series 12.165580749511719
epoch 42, loss_perior 12.212817192077637, loss_series 12.212817192077637
epoch 43, loss_perior 12.355658531188965, loss_series 12.355658531188965
epoch 44, loss_perior 12.486747741699219, loss_series 12.486747741699219
epoch 45, loss_perior 12.515676498413086, loss_series 12.515676498413086
epoch 46, loss_perior 12.65393352508545, loss_series 12.65393352508545
epoch 47, loss_perior 12.705404281616211, loss_series 12.705404281616211
epoch 48, loss_perior 12.824758529663086, loss_series 12.824758529663086
epoch 49, loss_perior 12.873156547546387, loss_series 12.873156547546387
epoch 50, loss_perior 13.03641414642334, loss_series 13.03641414642334
epoch 51, loss_perior 13.204216003417969, loss_series 13.204216003417969
epoch 52, loss_perior 13.191123962402344, loss_series 13.191123962402344
epoch 53, loss_perior 13.402320861816406, loss_series 13.402320861816406
epoch 54, loss_perior 13.530817031860352, loss_series 13.530817031860352
epoch 55, loss_perior 13.578939437866211, loss_series 13.578939437866211
epoch 56, loss_perior 13.71821117401123, loss_series 13.71821117401123
epoch 57, loss_perior 13.84083080291748, loss_series 13.84083080291748
epoch 58, loss_perior 13.994352340698242, loss_series 13.994352340698242
epoch 59, loss_perior 14.150260925292969, loss_series 14.150260925292969
epoch 60, loss_perior 14.276565551757812, loss_series 14.276565551757812
epoch 61, loss_perior 14.389516830444336, loss_series 14.389516830444336
epoch 62, loss_perior 14.554401397705078, loss_series 14.554401397705078
epoch 63, loss_perior 14.72492504119873, loss_series 14.72492504119873
epoch 64, loss_perior 14.843816757202148, loss_series 14.843816757202148
epoch 65, loss_perior 15.004481315612793, loss_series 15.004481315612793
epoch 66, loss_perior 15.158750534057617, loss_series 15.158750534057617
epoch 67, loss_perior 15.285295486450195, loss_series 15.285295486450195
epoch 68, loss_perior 15.423380851745605, loss_series 15.423380851745605
epoch 69, loss_perior 15.659482955932617, loss_series 15.659482955932617
epoch 70, loss_perior 15.782135009765625, loss_series 15.782135009765625
epoch 71, loss_perior 15.977396965026855, loss_series 15.977396965026855
epoch 72, loss_perior 16.088865280151367, loss_series 16.088865280151367
epoch 73, loss_perior 16.3193359375, loss_series 16.3193359375
epoch 74, loss_perior 16.469602584838867, loss_series 16.469602584838867
epoch 75, loss_perior 16.638338088989258, loss_series 16.638338088989258
epoch 76, loss_perior 16.73627471923828, loss_series 16.73627471923828
epoch 77, loss_perior 17.002826690673828, loss_series 17.002826690673828
epoch 78, loss_perior 17.112648010253906, loss_series 17.112648010253906
epoch 79, loss_perior 17.23973846435547, loss_series 17.23973846435547
epoch 80, loss_perior 17.47545051574707, loss_series 17.47545051574707
epoch 81, loss_perior 17.65194320678711, loss_series 17.65194320678711
epoch 82, loss_perior 17.794960021972656, loss_series 17.794960021972656
epoch 83, loss_perior 17.912126541137695, loss_series 17.912126541137695
epoch 84, loss_perior 18.053817749023438, loss_series 18.053817749023438
epoch 85, loss_perior 18.214445114135742, loss_series 18.214445114135742
epoch 86, loss_perior 18.39625358581543, loss_series 18.39625358581543
epoch 87, loss_perior 18.598175048828125, loss_series 18.598175048828125
epoch 88, loss_perior 18.587635040283203, loss_series 18.587635040283203
epoch 89, loss_perior 18.789100646972656, loss_series 18.789100646972656
epoch 90, loss_perior 18.92104721069336, loss_series 18.92104721069336
epoch 91, loss_perior 18.984146118164062, loss_series 18.984146118164062
epoch 92, loss_perior 19.199010848999023, loss_series 19.199010848999023
epoch 93, loss_perior 19.208070755004883, loss_series 19.208070755004883
epoch 94, loss_perior 19.448287963867188, loss_series 19.448287963867188
epoch 95, loss_perior 19.499698638916016, loss_series 19.499698638916016
epoch 96, loss_perior 19.630722045898438, loss_series 19.630722045898438
epoch 97, loss_perior 19.71222686767578, loss_series 19.71222686767578
epoch 98, loss_perior 19.891136169433594, loss_series 19.891136169433594
	speed: 0.4183s/iter; left time: 1693.9355s
epoch 99, loss_perior 19.94137191772461, loss_series 19.94137191772461
epoch 100, loss_perior 20.066246032714844, loss_series 20.066246032714844
epoch 101, loss_perior 20.182083129882812, loss_series 20.182083129882812
epoch 102, loss_perior 20.277053833007812, loss_series 20.277053833007812
epoch 103, loss_perior 20.456302642822266, loss_series 20.456302642822266
epoch 104, loss_perior 20.500293731689453, loss_series 20.500293731689453
epoch 105, loss_perior 20.6375675201416, loss_series 20.6375675201416
epoch 106, loss_perior 20.753774642944336, loss_series 20.753774642944336
epoch 107, loss_perior 20.9157657623291, loss_series 20.9157657623291
epoch 108, loss_perior 21.095903396606445, loss_series 21.095903396606445
epoch 109, loss_perior 21.209604263305664, loss_series 21.209604263305664
epoch 110, loss_perior 21.346221923828125, loss_series 21.346221923828125
epoch 111, loss_perior 21.483448028564453, loss_series 21.483448028564453
epoch 112, loss_perior 21.54645538330078, loss_series 21.54645538330078
epoch 113, loss_perior 21.661468505859375, loss_series 21.661468505859375
epoch 114, loss_perior 21.83486557006836, loss_series 21.83486557006836
epoch 115, loss_perior 21.965679168701172, loss_series 21.965679168701172
epoch 116, loss_perior 22.140060424804688, loss_series 22.140060424804688
epoch 117, loss_perior 22.249746322631836, loss_series 22.249746322631836
epoch 118, loss_perior 22.38796043395996, loss_series 22.38796043395996
epoch 119, loss_perior 22.496620178222656, loss_series 22.496620178222656
epoch 120, loss_perior 22.635526657104492, loss_series 22.635526657104492
epoch 121, loss_perior 22.821813583374023, loss_series 22.821813583374023
epoch 122, loss_perior 22.884212493896484, loss_series 22.884212493896484
epoch 123, loss_perior 23.011869430541992, loss_series 23.011869430541992
epoch 124, loss_perior 23.13192367553711, loss_series 23.13192367553711
epoch 125, loss_perior 23.271833419799805, loss_series 23.271833419799805
epoch 126, loss_perior 23.440902709960938, loss_series 23.440902709960938
epoch 127, loss_perior 23.511688232421875, loss_series 23.511688232421875
epoch 128, loss_perior 23.654922485351562, loss_series 23.654922485351562
epoch 129, loss_perior 23.739070892333984, loss_series 23.739070892333984
epoch 130, loss_perior 23.83852767944336, loss_series 23.83852767944336
epoch 131, loss_perior 23.959028244018555, loss_series 23.959028244018555
epoch 132, loss_perior 24.09164810180664, loss_series 24.09164810180664
epoch 133, loss_perior 24.15947723388672, loss_series 24.15947723388672
epoch 134, loss_perior 24.268362045288086, loss_series 24.268362045288086
epoch 135, loss_perior 24.345678329467773, loss_series 24.345678329467773
epoch 136, loss_perior 24.428363800048828, loss_series 24.428363800048828
epoch 137, loss_perior 24.554906845092773, loss_series 24.554906845092773
epoch 138, loss_perior 24.70728874206543, loss_series 24.70728874206543
epoch 139, loss_perior 24.748764038085938, loss_series 24.748764038085938
epoch 140, loss_perior 24.821134567260742, loss_series 24.821134567260742
epoch 141, loss_perior 24.893407821655273, loss_series 24.893407821655273
epoch 142, loss_perior 25.049419403076172, loss_series 25.049419403076172
epoch 143, loss_perior 25.114891052246094, loss_series 25.114891052246094
epoch 144, loss_perior 25.248096466064453, loss_series 25.248096466064453
epoch 145, loss_perior 25.33349609375, loss_series 25.33349609375
epoch 146, loss_perior 25.421062469482422, loss_series 25.421062469482422
epoch 147, loss_perior 25.50087547302246, loss_series 25.50087547302246
epoch 148, loss_perior 25.57769775390625, loss_series 25.57769775390625
epoch 149, loss_perior 25.674152374267578, loss_series 25.674152374267578
epoch 150, loss_perior 25.73287582397461, loss_series 25.73287582397461
epoch 151, loss_perior 25.83232879638672, loss_series 25.83232879638672
epoch 152, loss_perior 25.935317993164062, loss_series 25.935317993164062
epoch 153, loss_perior 26.03163719177246, loss_series 26.03163719177246
epoch 154, loss_perior 26.143461227416992, loss_series 26.143461227416992
epoch 155, loss_perior 26.210371017456055, loss_series 26.210371017456055
epoch 156, loss_perior 26.286720275878906, loss_series 26.286720275878906
epoch 157, loss_perior 26.390737533569336, loss_series 26.390737533569336
epoch 158, loss_perior 26.51313591003418, loss_series 26.51313591003418
epoch 159, loss_perior 26.554821014404297, loss_series 26.554821014404297
epoch 160, loss_perior 26.663799285888672, loss_series 26.663799285888672
epoch 161, loss_perior 26.73396873474121, loss_series 26.73396873474121
epoch 162, loss_perior 26.822418212890625, loss_series 26.822418212890625
epoch 163, loss_perior 26.895671844482422, loss_series 26.895671844482422
epoch 164, loss_perior 26.993633270263672, loss_series 26.993633270263672
epoch 165, loss_perior 27.10382652282715, loss_series 27.10382652282715
epoch 166, loss_perior 27.210615158081055, loss_series 27.210615158081055
epoch 167, loss_perior 27.32472801208496, loss_series 27.32472801208496
epoch 168, loss_perior 27.41927146911621, loss_series 27.41927146911621
epoch 169, loss_perior 27.51778221130371, loss_series 27.51778221130371
epoch 170, loss_perior 27.637868881225586, loss_series 27.637868881225586
epoch 171, loss_perior 27.679424285888672, loss_series 27.679424285888672
epoch 172, loss_perior 27.7661190032959, loss_series 27.7661190032959
epoch 173, loss_perior 27.880224227905273, loss_series 27.880224227905273
epoch 174, loss_perior 27.984790802001953, loss_series 27.984790802001953
epoch 175, loss_perior 28.05963134765625, loss_series 28.05963134765625
epoch 176, loss_perior 28.154996871948242, loss_series 28.154996871948242
epoch 177, loss_perior 28.19484519958496, loss_series 28.19484519958496
epoch 178, loss_perior 28.300846099853516, loss_series 28.300846099853516
epoch 179, loss_perior 28.36758804321289, loss_series 28.36758804321289
epoch 180, loss_perior 28.43195343017578, loss_series 28.43195343017578
epoch 181, loss_perior 28.444480895996094, loss_series 28.444480895996094
epoch 182, loss_perior 28.551166534423828, loss_series 28.551166534423828
epoch 183, loss_perior 28.61672592163086, loss_series 28.61672592163086
epoch 184, loss_perior 28.702983856201172, loss_series 28.702983856201172
epoch 185, loss_perior 28.769973754882812, loss_series 28.769973754882812
epoch 186, loss_perior 28.839614868164062, loss_series 28.839614868164062
epoch 187, loss_perior 28.92597198486328, loss_series 28.92597198486328
epoch 188, loss_perior 29.024063110351562, loss_series 29.024063110351562
epoch 189, loss_perior 29.11433982849121, loss_series 29.11433982849121
epoch 190, loss_perior 29.181671142578125, loss_series 29.181671142578125
epoch 191, loss_perior 29.22421646118164, loss_series 29.22421646118164
epoch 192, loss_perior 29.330785751342773, loss_series 29.330785751342773
epoch 193, loss_perior 29.43990135192871, loss_series 29.43990135192871
epoch 194, loss_perior 29.517959594726562, loss_series 29.517959594726562
epoch 195, loss_perior 29.578105926513672, loss_series 29.578105926513672
epoch 196, loss_perior 29.622177124023438, loss_series 29.622177124023438
epoch 197, loss_perior 29.671682357788086, loss_series 29.671682357788086
epoch 198, loss_perior 29.697553634643555, loss_series 29.697553634643555
	speed: 0.3983s/iter; left time: 1573.3482s
epoch 199, loss_perior 29.748252868652344, loss_series 29.748252868652344
epoch 200, loss_perior 29.842947006225586, loss_series 29.842947006225586
epoch 201, loss_perior 29.888839721679688, loss_series 29.888839721679688
epoch 202, loss_perior 29.97229766845703, loss_series 29.97229766845703
epoch 203, loss_perior 29.98232078552246, loss_series 29.98232078552246
epoch 204, loss_perior 30.126270294189453, loss_series 30.126270294189453
epoch 205, loss_perior 30.12359619140625, loss_series 30.12359619140625
epoch 206, loss_perior 30.103607177734375, loss_series 30.103607177734375
epoch 207, loss_perior 30.192440032958984, loss_series 30.192440032958984
epoch 208, loss_perior 30.242504119873047, loss_series 30.242504119873047
epoch 209, loss_perior 30.265735626220703, loss_series 30.265735626220703
epoch 210, loss_perior 30.321630477905273, loss_series 30.321630477905273
epoch 211, loss_perior 30.292327880859375, loss_series 30.292327880859375
epoch 212, loss_perior 30.406234741210938, loss_series 30.406234741210938
epoch 213, loss_perior 30.414993286132812, loss_series 30.414993286132812
epoch 214, loss_perior 30.496715545654297, loss_series 30.496715545654297
epoch 215, loss_perior 30.539642333984375, loss_series 30.539642333984375
epoch 216, loss_perior 30.582311630249023, loss_series 30.582311630249023
epoch 217, loss_perior 30.6663875579834, loss_series 30.6663875579834
epoch 218, loss_perior 30.666011810302734, loss_series 30.666011810302734
epoch 219, loss_perior 30.70258903503418, loss_series 30.70258903503418
epoch 220, loss_perior 30.682994842529297, loss_series 30.682994842529297
epoch 221, loss_perior 30.780141830444336, loss_series 30.780141830444336
epoch 222, loss_perior 30.825605392456055, loss_series 30.825605392456055
epoch 223, loss_perior 30.848134994506836, loss_series 30.848134994506836
epoch 224, loss_perior 30.87196159362793, loss_series 30.87196159362793
epoch 225, loss_perior 30.900907516479492, loss_series 30.900907516479492
epoch 226, loss_perior 30.970531463623047, loss_series 30.970531463623047
epoch 227, loss_perior 30.98914909362793, loss_series 30.98914909362793
epoch 228, loss_perior 31.012977600097656, loss_series 31.012977600097656
epoch 229, loss_perior 31.007061004638672, loss_series 31.007061004638672
epoch 230, loss_perior 31.055906295776367, loss_series 31.055906295776367
epoch 231, loss_perior 31.080917358398438, loss_series 31.080917358398438
epoch 232, loss_perior 31.1302547454834, loss_series 31.1302547454834
epoch 233, loss_perior 31.21133041381836, loss_series 31.21133041381836
epoch 234, loss_perior 31.243728637695312, loss_series 31.243728637695312
epoch 235, loss_perior 31.20452880859375, loss_series 31.20452880859375
epoch 236, loss_perior 31.31073570251465, loss_series 31.31073570251465
epoch 237, loss_perior 31.328189849853516, loss_series 31.328189849853516
epoch 238, loss_perior 31.376861572265625, loss_series 31.376861572265625
epoch 239, loss_perior 31.373729705810547, loss_series 31.373729705810547
epoch 240, loss_perior 31.4227352142334, loss_series 31.4227352142334
epoch 241, loss_perior 31.447277069091797, loss_series 31.447277069091797
epoch 242, loss_perior 31.456905364990234, loss_series 31.456905364990234
epoch 243, loss_perior 31.552915573120117, loss_series 31.552915573120117
epoch 244, loss_perior 31.61322784423828, loss_series 31.61322784423828
epoch 245, loss_perior 31.647476196289062, loss_series 31.647476196289062
epoch 246, loss_perior 31.653350830078125, loss_series 31.653350830078125
epoch 247, loss_perior 31.694656372070312, loss_series 31.694656372070312
epoch 248, loss_perior 31.808683395385742, loss_series 31.808683395385742
epoch 249, loss_perior 31.831218719482422, loss_series 31.831218719482422
epoch 250, loss_perior 31.822689056396484, loss_series 31.822689056396484
epoch 251, loss_perior 31.8618221282959, loss_series 31.8618221282959
epoch 252, loss_perior 31.850204467773438, loss_series 31.850204467773438
epoch 253, loss_perior 31.943557739257812, loss_series 31.943557739257812
epoch 254, loss_perior 31.986312866210938, loss_series 31.986312866210938
epoch 255, loss_perior 32.04769515991211, loss_series 32.04769515991211
epoch 256, loss_perior 32.06396484375, loss_series 32.06396484375
epoch 257, loss_perior 32.08539581298828, loss_series 32.08539581298828
epoch 258, loss_perior 32.125972747802734, loss_series 32.125972747802734
epoch 259, loss_perior 32.227840423583984, loss_series 32.227840423583984
epoch 260, loss_perior 32.197540283203125, loss_series 32.197540283203125
epoch 261, loss_perior 32.174564361572266, loss_series 32.174564361572266
epoch 262, loss_perior 32.2163200378418, loss_series 32.2163200378418
epoch 263, loss_perior 32.29277420043945, loss_series 32.29277420043945
epoch 264, loss_perior 32.343475341796875, loss_series 32.343475341796875
epoch 265, loss_perior 32.31941604614258, loss_series 32.31941604614258
epoch 266, loss_perior 32.36090850830078, loss_series 32.36090850830078
epoch 267, loss_perior 32.404476165771484, loss_series 32.404476165771484
epoch 268, loss_perior 32.42774200439453, loss_series 32.42774200439453
epoch 269, loss_perior 32.50197982788086, loss_series 32.50197982788086
epoch 270, loss_perior 32.44049835205078, loss_series 32.44049835205078
epoch 271, loss_perior 32.47881317138672, loss_series 32.47881317138672
epoch 272, loss_perior 32.567588806152344, loss_series 32.567588806152344
epoch 273, loss_perior 32.49122619628906, loss_series 32.49122619628906
epoch 274, loss_perior 32.56500244140625, loss_series 32.56500244140625
epoch 275, loss_perior 32.53034210205078, loss_series 32.53034210205078
epoch 276, loss_perior 32.54165267944336, loss_series 32.54165267944336
epoch 277, loss_perior 32.62757873535156, loss_series 32.62757873535156
epoch 278, loss_perior 32.613433837890625, loss_series 32.613433837890625
epoch 279, loss_perior 32.63768005371094, loss_series 32.63768005371094
epoch 280, loss_perior 32.68931198120117, loss_series 32.68931198120117
epoch 281, loss_perior 32.71743392944336, loss_series 32.71743392944336
epoch 282, loss_perior 32.70118713378906, loss_series 32.70118713378906
epoch 283, loss_perior 32.69285583496094, loss_series 32.69285583496094
epoch 284, loss_perior 32.800987243652344, loss_series 32.800987243652344
epoch 285, loss_perior 32.76000213623047, loss_series 32.76000213623047
epoch 286, loss_perior 32.72186279296875, loss_series 32.72186279296875
epoch 287, loss_perior 32.79478454589844, loss_series 32.79478454589844
epoch 288, loss_perior 32.86636734008789, loss_series 32.86636734008789
epoch 289, loss_perior 32.78256607055664, loss_series 32.78256607055664
epoch 290, loss_perior 32.79528045654297, loss_series 32.79528045654297
epoch 291, loss_perior 32.79596710205078, loss_series 32.79596710205078
epoch 292, loss_perior 32.84477233886719, loss_series 32.84477233886719
epoch 293, loss_perior 32.84375, loss_series 32.84375
epoch 294, loss_perior 32.880706787109375, loss_series 32.880706787109375
epoch 295, loss_perior 32.93244934082031, loss_series 32.93244934082031
epoch 296, loss_perior 32.927345275878906, loss_series 32.927345275878906
epoch 297, loss_perior 32.94550704956055, loss_series 32.94550704956055
epoch 298, loss_perior 32.93943405151367, loss_series 32.93943405151367
	speed: 0.3995s/iter; left time: 1538.1544s
epoch 299, loss_perior 32.89392852783203, loss_series 32.89392852783203
epoch 300, loss_perior 32.9039306640625, loss_series 32.9039306640625
epoch 301, loss_perior 33.028961181640625, loss_series 33.028961181640625
epoch 302, loss_perior 32.97312545776367, loss_series 32.97312545776367
epoch 303, loss_perior 32.989967346191406, loss_series 32.989967346191406
epoch 304, loss_perior 33.04723358154297, loss_series 33.04723358154297
epoch 305, loss_perior 33.034095764160156, loss_series 33.034095764160156
epoch 306, loss_perior 32.98116683959961, loss_series 32.98116683959961
epoch 307, loss_perior 33.00691223144531, loss_series 33.00691223144531
epoch 308, loss_perior 33.07927703857422, loss_series 33.07927703857422
epoch 309, loss_perior 33.004249572753906, loss_series 33.004249572753906
epoch 310, loss_perior 33.00685119628906, loss_series 33.00685119628906
epoch 311, loss_perior 33.02400588989258, loss_series 33.02400588989258
epoch 312, loss_perior 33.0452766418457, loss_series 33.0452766418457
epoch 313, loss_perior 33.06340789794922, loss_series 33.06340789794922
epoch 314, loss_perior 33.138004302978516, loss_series 33.138004302978516
epoch 315, loss_perior 33.07682800292969, loss_series 33.07682800292969
epoch 316, loss_perior 33.099517822265625, loss_series 33.099517822265625
epoch 317, loss_perior 33.152549743652344, loss_series 33.152549743652344
epoch 318, loss_perior 33.14887619018555, loss_series 33.14887619018555
epoch 319, loss_perior 33.101749420166016, loss_series 33.101749420166016
epoch 320, loss_perior 33.14405059814453, loss_series 33.14405059814453
epoch 321, loss_perior 33.15007781982422, loss_series 33.15007781982422
epoch 322, loss_perior 33.08863067626953, loss_series 33.08863067626953
epoch 323, loss_perior 33.12777328491211, loss_series 33.12777328491211
epoch 324, loss_perior 33.22177505493164, loss_series 33.22177505493164
epoch 325, loss_perior 33.190406799316406, loss_series 33.190406799316406
epoch 326, loss_perior 33.180213928222656, loss_series 33.180213928222656
epoch 327, loss_perior 33.20774459838867, loss_series 33.20774459838867
epoch 328, loss_perior 33.18404006958008, loss_series 33.18404006958008
epoch 329, loss_perior 33.17778015136719, loss_series 33.17778015136719
epoch 330, loss_perior 33.18669509887695, loss_series 33.18669509887695
epoch 331, loss_perior 33.165855407714844, loss_series 33.165855407714844
epoch 332, loss_perior 33.155513763427734, loss_series 33.155513763427734
epoch 333, loss_perior 33.15434646606445, loss_series 33.15434646606445
epoch 334, loss_perior 33.19639587402344, loss_series 33.19639587402344
epoch 335, loss_perior 33.23981475830078, loss_series 33.23981475830078
epoch 336, loss_perior 33.236427307128906, loss_series 33.236427307128906
epoch 337, loss_perior 33.26220703125, loss_series 33.26220703125
epoch 338, loss_perior 33.333831787109375, loss_series 33.333831787109375
epoch 339, loss_perior 33.19563674926758, loss_series 33.19563674926758
epoch 340, loss_perior 33.265724182128906, loss_series 33.265724182128906
epoch 341, loss_perior 33.261287689208984, loss_series 33.261287689208984
epoch 342, loss_perior 33.2601432800293, loss_series 33.2601432800293
epoch 343, loss_perior 33.250938415527344, loss_series 33.250938415527344
epoch 344, loss_perior 33.30122375488281, loss_series 33.30122375488281
epoch 345, loss_perior 33.29545211791992, loss_series 33.29545211791992
epoch 346, loss_perior 33.32610321044922, loss_series 33.32610321044922
epoch 347, loss_perior 33.31492233276367, loss_series 33.31492233276367
epoch 348, loss_perior 33.33124923706055, loss_series 33.33124923706055
epoch 349, loss_perior 33.34172058105469, loss_series 33.34172058105469
epoch 350, loss_perior 33.31565856933594, loss_series 33.31565856933594
epoch 351, loss_perior 33.285682678222656, loss_series 33.285682678222656
epoch 352, loss_perior 33.301490783691406, loss_series 33.301490783691406
epoch 353, loss_perior 33.32036590576172, loss_series 33.32036590576172
epoch 354, loss_perior 33.29618835449219, loss_series 33.29618835449219
epoch 355, loss_perior 33.341552734375, loss_series 33.341552734375
epoch 356, loss_perior 33.32439041137695, loss_series 33.32439041137695
epoch 357, loss_perior 33.321266174316406, loss_series 33.321266174316406
epoch 358, loss_perior 33.33066940307617, loss_series 33.33066940307617
epoch 359, loss_perior 33.35279846191406, loss_series 33.35279846191406
epoch 360, loss_perior 33.33807373046875, loss_series 33.33807373046875
epoch 361, loss_perior 33.373836517333984, loss_series 33.373836517333984
epoch 362, loss_perior 33.378177642822266, loss_series 33.378177642822266
epoch 363, loss_perior 33.35637664794922, loss_series 33.35637664794922
epoch 364, loss_perior 33.41062927246094, loss_series 33.41062927246094
epoch 365, loss_perior 33.45832824707031, loss_series 33.45832824707031
epoch 366, loss_perior 33.419517517089844, loss_series 33.419517517089844
epoch 367, loss_perior 33.430931091308594, loss_series 33.430931091308594
epoch 368, loss_perior 33.414581298828125, loss_series 33.414581298828125
epoch 369, loss_perior 33.44678497314453, loss_series 33.44678497314453
epoch 370, loss_perior 33.437477111816406, loss_series 33.437477111816406
epoch 371, loss_perior 33.465179443359375, loss_series 33.465179443359375
epoch 372, loss_perior 33.45899200439453, loss_series 33.45899200439453
epoch 373, loss_perior 33.44181823730469, loss_series 33.44181823730469
epoch 374, loss_perior 33.40116500854492, loss_series 33.40116500854492
epoch 375, loss_perior 33.460418701171875, loss_series 33.460418701171875
epoch 376, loss_perior 33.45482635498047, loss_series 33.45482635498047
epoch 377, loss_perior 33.47113800048828, loss_series 33.47113800048828
epoch 378, loss_perior 33.41181182861328, loss_series 33.41181182861328
epoch 379, loss_perior 33.49382781982422, loss_series 33.49382781982422
epoch 380, loss_perior 33.46014404296875, loss_series 33.46014404296875
epoch 381, loss_perior 33.42457580566406, loss_series 33.42457580566406
epoch 382, loss_perior 33.39060592651367, loss_series 33.39060592651367
epoch 383, loss_perior 33.444007873535156, loss_series 33.444007873535156
epoch 384, loss_perior 33.39183807373047, loss_series 33.39183807373047
epoch 385, loss_perior 33.52523422241211, loss_series 33.52523422241211
epoch 386, loss_perior 33.438819885253906, loss_series 33.438819885253906
epoch 387, loss_perior 33.509925842285156, loss_series 33.509925842285156
epoch 388, loss_perior 33.50874328613281, loss_series 33.50874328613281
epoch 389, loss_perior 33.465641021728516, loss_series 33.465641021728516
epoch 390, loss_perior 33.48997497558594, loss_series 33.48997497558594
epoch 391, loss_perior 33.479766845703125, loss_series 33.479766845703125
epoch 392, loss_perior 33.48224639892578, loss_series 33.48224639892578
epoch 393, loss_perior 33.51598358154297, loss_series 33.51598358154297
epoch 394, loss_perior 33.50243377685547, loss_series 33.50243377685547
epoch 395, loss_perior 33.547943115234375, loss_series 33.547943115234375
epoch 396, loss_perior 33.50996398925781, loss_series 33.50996398925781
epoch 397, loss_perior 33.51225662231445, loss_series 33.51225662231445
epoch 398, loss_perior 33.44288635253906, loss_series 33.44288635253906
	speed: 0.4003s/iter; left time: 1501.0605s
epoch 399, loss_perior 33.476593017578125, loss_series 33.476593017578125
epoch 400, loss_perior 33.50580596923828, loss_series 33.50580596923828
epoch 401, loss_perior 33.455177307128906, loss_series 33.455177307128906
epoch 402, loss_perior 33.569252014160156, loss_series 33.569252014160156
epoch 403, loss_perior 33.542903900146484, loss_series 33.542903900146484
epoch 404, loss_perior 33.494239807128906, loss_series 33.494239807128906
epoch 405, loss_perior 33.51015090942383, loss_series 33.51015090942383
epoch 406, loss_perior 33.56182098388672, loss_series 33.56182098388672
epoch 407, loss_perior 33.49687957763672, loss_series 33.49687957763672
epoch 408, loss_perior 33.54954147338867, loss_series 33.54954147338867
epoch 409, loss_perior 33.54264831542969, loss_series 33.54264831542969
epoch 410, loss_perior 33.561668395996094, loss_series 33.561668395996094
epoch 411, loss_perior 33.48907470703125, loss_series 33.48907470703125
epoch 412, loss_perior 33.49147033691406, loss_series 33.49147033691406
epoch 413, loss_perior 33.6090087890625, loss_series 33.6090087890625
epoch 414, loss_perior 33.524932861328125, loss_series 33.524932861328125
epoch 415, loss_perior 33.55804443359375, loss_series 33.55804443359375
epoch 416, loss_perior 33.55912780761719, loss_series 33.55912780761719
epoch 417, loss_perior 33.61125183105469, loss_series 33.61125183105469
epoch 418, loss_perior 33.59735107421875, loss_series 33.59735107421875
epoch 419, loss_perior 33.59174728393555, loss_series 33.59174728393555
epoch 420, loss_perior 33.557472229003906, loss_series 33.557472229003906
epoch 421, loss_perior 33.59759521484375, loss_series 33.59759521484375
epoch 422, loss_perior 33.55320739746094, loss_series 33.55320739746094
epoch 423, loss_perior 33.610862731933594, loss_series 33.610862731933594
epoch 424, loss_perior 33.56180953979492, loss_series 33.56180953979492
epoch 425, loss_perior 33.59284973144531, loss_series 33.59284973144531
epoch 426, loss_perior 33.62366485595703, loss_series 33.62366485595703
epoch 427, loss_perior 33.52824401855469, loss_series 33.52824401855469
epoch 428, loss_perior 33.633872985839844, loss_series 33.633872985839844
epoch 429, loss_perior 33.603607177734375, loss_series 33.603607177734375
epoch 430, loss_perior 33.58722686767578, loss_series 33.58722686767578
epoch 431, loss_perior 33.51904296875, loss_series 33.51904296875
epoch 432, loss_perior 33.650856018066406, loss_series 33.650856018066406
epoch 433, loss_perior 33.66620635986328, loss_series 33.66620635986328
epoch 434, loss_perior 33.616607666015625, loss_series 33.616607666015625
epoch 435, loss_perior 33.661643981933594, loss_series 33.661643981933594
epoch 436, loss_perior 33.541114807128906, loss_series 33.541114807128906
epoch 437, loss_perior 33.62771987915039, loss_series 33.62771987915039
epoch 438, loss_perior 33.57301330566406, loss_series 33.57301330566406
epoch 439, loss_perior 33.6128044128418, loss_series 33.6128044128418
epoch 440, loss_perior 33.61152267456055, loss_series 33.61152267456055
epoch 441, loss_perior 33.601417541503906, loss_series 33.601417541503906
epoch 442, loss_perior 33.601173400878906, loss_series 33.601173400878906
epoch 443, loss_perior 33.60443878173828, loss_series 33.60443878173828
epoch 444, loss_perior 33.62379455566406, loss_series 33.62379455566406
epoch 445, loss_perior 33.63045120239258, loss_series 33.63045120239258
epoch 446, loss_perior 33.59300994873047, loss_series 33.59300994873047
epoch 447, loss_perior 33.670928955078125, loss_series 33.670928955078125
epoch 448, loss_perior 33.67357635498047, loss_series 33.67357635498047
epoch 449, loss_perior 33.66099548339844, loss_series 33.66099548339844
epoch 450, loss_perior 33.64116668701172, loss_series 33.64116668701172
epoch 451, loss_perior 33.703285217285156, loss_series 33.703285217285156
epoch 452, loss_perior 33.61433410644531, loss_series 33.61433410644531
epoch 453, loss_perior 33.63113784790039, loss_series 33.63113784790039
epoch 454, loss_perior 33.69993209838867, loss_series 33.69993209838867
epoch 455, loss_perior 33.6843376159668, loss_series 33.6843376159668
epoch 456, loss_perior 33.72268295288086, loss_series 33.72268295288086
epoch 457, loss_perior 33.67694091796875, loss_series 33.67694091796875
epoch 458, loss_perior 33.68367004394531, loss_series 33.68367004394531
epoch 459, loss_perior 33.609100341796875, loss_series 33.609100341796875
epoch 460, loss_perior 33.65007781982422, loss_series 33.65007781982422
epoch 461, loss_perior 33.64146041870117, loss_series 33.64146041870117
epoch 462, loss_perior 33.595924377441406, loss_series 33.595924377441406
epoch 463, loss_perior 33.63993835449219, loss_series 33.63993835449219
epoch 464, loss_perior 33.665122985839844, loss_series 33.665122985839844
epoch 465, loss_perior 33.66893005371094, loss_series 33.66893005371094
epoch 466, loss_perior 33.66652297973633, loss_series 33.66652297973633
epoch 467, loss_perior 33.653160095214844, loss_series 33.653160095214844
epoch 468, loss_perior 33.627525329589844, loss_series 33.627525329589844
epoch 469, loss_perior 33.687713623046875, loss_series 33.687713623046875
epoch 470, loss_perior 33.635292053222656, loss_series 33.635292053222656
epoch 471, loss_perior 33.69485092163086, loss_series 33.69485092163086
epoch 472, loss_perior 33.66188049316406, loss_series 33.66188049316406
epoch 473, loss_perior 33.716270446777344, loss_series 33.716270446777344
epoch 474, loss_perior 33.67654037475586, loss_series 33.67654037475586
epoch 475, loss_perior 33.68376922607422, loss_series 33.68376922607422
epoch 476, loss_perior 33.70320129394531, loss_series 33.70320129394531
epoch 477, loss_perior 33.68201446533203, loss_series 33.68201446533203
epoch 478, loss_perior 33.64691925048828, loss_series 33.64691925048828
epoch 479, loss_perior 33.632381439208984, loss_series 33.632381439208984
epoch 480, loss_perior 33.67265319824219, loss_series 33.67265319824219
epoch 481, loss_perior 33.65355682373047, loss_series 33.65355682373047
epoch 482, loss_perior 33.68096160888672, loss_series 33.68096160888672
epoch 483, loss_perior 33.64618682861328, loss_series 33.64618682861328
epoch 484, loss_perior 33.67343521118164, loss_series 33.67343521118164
epoch 485, loss_perior 33.7085075378418, loss_series 33.7085075378418
epoch 486, loss_perior 33.715553283691406, loss_series 33.715553283691406
epoch 487, loss_perior 33.703182220458984, loss_series 33.703182220458984
epoch 488, loss_perior 33.705543518066406, loss_series 33.705543518066406
epoch 489, loss_perior 33.77268981933594, loss_series 33.77268981933594
epoch 490, loss_perior 33.71925354003906, loss_series 33.71925354003906
epoch 491, loss_perior 33.72872543334961, loss_series 33.72872543334961
epoch 492, loss_perior 33.67124938964844, loss_series 33.67124938964844
epoch 493, loss_perior 33.75798416137695, loss_series 33.75798416137695
epoch 494, loss_perior 33.68629837036133, loss_series 33.68629837036133
epoch 495, loss_perior 33.66242218017578, loss_series 33.66242218017578
epoch 496, loss_perior 33.7757568359375, loss_series 33.7757568359375
epoch 497, loss_perior 33.700721740722656, loss_series 33.700721740722656
epoch 498, loss_perior 33.700836181640625, loss_series 33.700836181640625
	speed: 0.4007s/iter; left time: 1462.6228s
epoch 499, loss_perior 33.749664306640625, loss_series 33.749664306640625
epoch 500, loss_perior 33.694461822509766, loss_series 33.694461822509766
epoch 501, loss_perior 33.662864685058594, loss_series 33.662864685058594
epoch 502, loss_perior 33.66361618041992, loss_series 33.66361618041992
epoch 503, loss_perior 33.7603759765625, loss_series 33.7603759765625
epoch 504, loss_perior 33.736412048339844, loss_series 33.736412048339844
epoch 505, loss_perior 33.65034866333008, loss_series 33.65034866333008
epoch 506, loss_perior 33.70712661743164, loss_series 33.70712661743164
epoch 507, loss_perior 33.73447799682617, loss_series 33.73447799682617
epoch 508, loss_perior 33.71229553222656, loss_series 33.71229553222656
epoch 509, loss_perior 33.72086715698242, loss_series 33.72086715698242
epoch 510, loss_perior 33.64866638183594, loss_series 33.64866638183594
epoch 511, loss_perior 33.70745086669922, loss_series 33.70745086669922
epoch 512, loss_perior 33.68052673339844, loss_series 33.68052673339844
epoch 513, loss_perior 33.646793365478516, loss_series 33.646793365478516
epoch 514, loss_perior 33.65132141113281, loss_series 33.65132141113281
epoch 515, loss_perior 33.72502899169922, loss_series 33.72502899169922
epoch 516, loss_perior 33.661312103271484, loss_series 33.661312103271484
epoch 517, loss_perior 33.74736404418945, loss_series 33.74736404418945
epoch 518, loss_perior 33.710227966308594, loss_series 33.710227966308594
epoch 519, loss_perior 33.73640060424805, loss_series 33.73640060424805
epoch 520, loss_perior 33.771568298339844, loss_series 33.771568298339844
epoch 521, loss_perior 33.728126525878906, loss_series 33.728126525878906
epoch 522, loss_perior 33.7674560546875, loss_series 33.7674560546875
epoch 523, loss_perior 33.661468505859375, loss_series 33.661468505859375
epoch 524, loss_perior 33.692649841308594, loss_series 33.692649841308594
epoch 525, loss_perior 33.69572830200195, loss_series 33.69572830200195
epoch 526, loss_perior 33.77431106567383, loss_series 33.77431106567383
epoch 527, loss_perior 33.702667236328125, loss_series 33.702667236328125
epoch 528, loss_perior 33.673397064208984, loss_series 33.673397064208984
epoch 529, loss_perior 33.694435119628906, loss_series 33.694435119628906
epoch 530, loss_perior 33.769996643066406, loss_series 33.769996643066406
epoch 531, loss_perior 33.76127624511719, loss_series 33.76127624511719
epoch 532, loss_perior 33.74798583984375, loss_series 33.74798583984375
epoch 533, loss_perior 33.73162078857422, loss_series 33.73162078857422
epoch 534, loss_perior 33.73029708862305, loss_series 33.73029708862305
epoch 535, loss_perior 33.73794937133789, loss_series 33.73794937133789
epoch 536, loss_perior 33.770042419433594, loss_series 33.770042419433594
epoch 537, loss_perior 33.71492004394531, loss_series 33.71492004394531
epoch 538, loss_perior 33.82654571533203, loss_series 33.82654571533203
epoch 539, loss_perior 33.76198196411133, loss_series 33.76198196411133
epoch 540, loss_perior 33.7779541015625, loss_series 33.7779541015625
epoch 541, loss_perior 33.712135314941406, loss_series 33.712135314941406
epoch 542, loss_perior 33.741851806640625, loss_series 33.741851806640625
epoch 543, loss_perior 33.739559173583984, loss_series 33.739559173583984
epoch 544, loss_perior 33.74200439453125, loss_series 33.74200439453125
epoch 545, loss_perior 33.73472595214844, loss_series 33.73472595214844
epoch 546, loss_perior 33.74491882324219, loss_series 33.74491882324219
epoch 547, loss_perior 33.775550842285156, loss_series 33.775550842285156
epoch 548, loss_perior 33.70130157470703, loss_series 33.70130157470703
epoch 549, loss_perior 33.712120056152344, loss_series 33.712120056152344
epoch 550, loss_perior 33.766845703125, loss_series 33.766845703125
epoch 551, loss_perior 33.73747253417969, loss_series 33.73747253417969
epoch 552, loss_perior 33.74579620361328, loss_series 33.74579620361328
epoch 553, loss_perior 33.781890869140625, loss_series 33.781890869140625
epoch 554, loss_perior 33.74073791503906, loss_series 33.74073791503906
epoch 555, loss_perior 33.68281173706055, loss_series 33.68281173706055
epoch 556, loss_perior 33.768272399902344, loss_series 33.768272399902344
epoch 557, loss_perior 33.78976821899414, loss_series 33.78976821899414
epoch 558, loss_perior 33.763126373291016, loss_series 33.763126373291016
epoch 559, loss_perior 33.775630950927734, loss_series 33.775630950927734
epoch 560, loss_perior 33.72730255126953, loss_series 33.72730255126953
epoch 561, loss_perior 33.7805290222168, loss_series 33.7805290222168
epoch 562, loss_perior 33.75673294067383, loss_series 33.75673294067383
epoch 563, loss_perior 33.75547409057617, loss_series 33.75547409057617
epoch 564, loss_perior 33.76802062988281, loss_series 33.76802062988281
epoch 565, loss_perior 33.796329498291016, loss_series 33.796329498291016
epoch 566, loss_perior 33.75646209716797, loss_series 33.75646209716797
epoch 567, loss_perior 33.75677490234375, loss_series 33.75677490234375
epoch 568, loss_perior 33.72978973388672, loss_series 33.72978973388672
epoch 569, loss_perior 33.755489349365234, loss_series 33.755489349365234
epoch 570, loss_perior 33.761287689208984, loss_series 33.761287689208984
epoch 571, loss_perior 33.75555419921875, loss_series 33.75555419921875
epoch 572, loss_perior 33.74538040161133, loss_series 33.74538040161133
epoch 573, loss_perior 33.75654602050781, loss_series 33.75654602050781
epoch 574, loss_perior 33.813655853271484, loss_series 33.813655853271484
epoch 575, loss_perior 33.76080322265625, loss_series 33.76080322265625
epoch 576, loss_perior 33.76701354980469, loss_series 33.76701354980469
epoch 577, loss_perior 33.78754425048828, loss_series 33.78754425048828
epoch 578, loss_perior 33.713443756103516, loss_series 33.713443756103516
epoch 579, loss_perior 33.78369903564453, loss_series 33.78369903564453
epoch 580, loss_perior 33.756629943847656, loss_series 33.756629943847656
epoch 581, loss_perior 33.730892181396484, loss_series 33.730892181396484
epoch 582, loss_perior 33.7185173034668, loss_series 33.7185173034668
epoch 583, loss_perior 33.77338409423828, loss_series 33.77338409423828
epoch 584, loss_perior 33.76335906982422, loss_series 33.76335906982422
epoch 585, loss_perior 33.83430862426758, loss_series 33.83430862426758
epoch 586, loss_perior 33.76555633544922, loss_series 33.76555633544922
epoch 587, loss_perior 33.72438430786133, loss_series 33.72438430786133
epoch 588, loss_perior 33.748504638671875, loss_series 33.748504638671875
epoch 589, loss_perior 33.77404022216797, loss_series 33.77404022216797
epoch 590, loss_perior 33.77479553222656, loss_series 33.77479553222656
epoch 591, loss_perior 33.74189376831055, loss_series 33.74189376831055
epoch 592, loss_perior 33.7994384765625, loss_series 33.7994384765625
epoch 593, loss_perior 33.752079010009766, loss_series 33.752079010009766
epoch 594, loss_perior 33.78575134277344, loss_series 33.78575134277344
epoch 595, loss_perior 33.776390075683594, loss_series 33.776390075683594
epoch 596, loss_perior 33.74387741088867, loss_series 33.74387741088867
epoch 597, loss_perior 33.77432632446289, loss_series 33.77432632446289
epoch 598, loss_perior 33.75341796875, loss_series 33.75341796875
	speed: 0.4008s/iter; left time: 1422.9064s
epoch 599, loss_perior 33.77045822143555, loss_series 33.77045822143555
epoch 600, loss_perior 33.77361297607422, loss_series 33.77361297607422
epoch 601, loss_perior 33.72246170043945, loss_series 33.72246170043945
epoch 602, loss_perior 33.8133544921875, loss_series 33.8133544921875
epoch 603, loss_perior 33.76573944091797, loss_series 33.76573944091797
epoch 604, loss_perior 33.75395965576172, loss_series 33.75395965576172
epoch 605, loss_perior 33.77659225463867, loss_series 33.77659225463867
epoch 606, loss_perior 33.78588104248047, loss_series 33.78588104248047
epoch 607, loss_perior 33.78449630737305, loss_series 33.78449630737305
epoch 608, loss_perior 33.721885681152344, loss_series 33.721885681152344
epoch 609, loss_perior 33.76945495605469, loss_series 33.76945495605469
epoch 610, loss_perior 33.775482177734375, loss_series 33.775482177734375
epoch 611, loss_perior 33.803199768066406, loss_series 33.803199768066406
epoch 612, loss_perior 33.8525505065918, loss_series 33.8525505065918
epoch 613, loss_perior 33.8176383972168, loss_series 33.8176383972168
epoch 614, loss_perior 33.73323059082031, loss_series 33.73323059082031
epoch 615, loss_perior 33.747257232666016, loss_series 33.747257232666016
epoch 616, loss_perior 33.75545883178711, loss_series 33.75545883178711
epoch 617, loss_perior 33.817893981933594, loss_series 33.817893981933594
epoch 618, loss_perior 33.78041076660156, loss_series 33.78041076660156
epoch 619, loss_perior 33.812950134277344, loss_series 33.812950134277344
epoch 620, loss_perior 33.766082763671875, loss_series 33.766082763671875
epoch 621, loss_perior 33.7703857421875, loss_series 33.7703857421875
epoch 622, loss_perior 33.78789520263672, loss_series 33.78789520263672
epoch 623, loss_perior 33.78187561035156, loss_series 33.78187561035156
epoch 624, loss_perior 33.74636459350586, loss_series 33.74636459350586
epoch 625, loss_perior 33.796112060546875, loss_series 33.796112060546875
epoch 626, loss_perior 33.704200744628906, loss_series 33.704200744628906
epoch 627, loss_perior 33.78131103515625, loss_series 33.78131103515625
epoch 628, loss_perior 33.791351318359375, loss_series 33.791351318359375
epoch 629, loss_perior 33.76910400390625, loss_series 33.76910400390625
epoch 630, loss_perior 33.81627655029297, loss_series 33.81627655029297
epoch 631, loss_perior 33.82911682128906, loss_series 33.82911682128906
epoch 632, loss_perior 33.76887130737305, loss_series 33.76887130737305
epoch 633, loss_perior 33.79054260253906, loss_series 33.79054260253906
epoch 634, loss_perior 33.75905227661133, loss_series 33.75905227661133
epoch 635, loss_perior 33.81232452392578, loss_series 33.81232452392578
epoch 636, loss_perior 33.74922180175781, loss_series 33.74922180175781
epoch 637, loss_perior 33.77521514892578, loss_series 33.77521514892578
epoch 638, loss_perior 33.77104949951172, loss_series 33.77104949951172
epoch 639, loss_perior 33.805824279785156, loss_series 33.805824279785156
epoch 640, loss_perior 33.86267852783203, loss_series 33.86267852783203
epoch 641, loss_perior 33.784549713134766, loss_series 33.784549713134766
epoch 642, loss_perior 33.8021240234375, loss_series 33.8021240234375
epoch 643, loss_perior 33.80983352661133, loss_series 33.80983352661133
epoch 644, loss_perior 33.79341125488281, loss_series 33.79341125488281
epoch 645, loss_perior 33.78050231933594, loss_series 33.78050231933594
epoch 646, loss_perior 33.741554260253906, loss_series 33.741554260253906
epoch 647, loss_perior 33.81922149658203, loss_series 33.81922149658203
epoch 648, loss_perior 33.78095626831055, loss_series 33.78095626831055
epoch 649, loss_perior 33.79548645019531, loss_series 33.79548645019531
epoch 650, loss_perior 33.771934509277344, loss_series 33.771934509277344
epoch 651, loss_perior 33.80013656616211, loss_series 33.80013656616211
epoch 652, loss_perior 33.839881896972656, loss_series 33.839881896972656
epoch 653, loss_perior 33.791709899902344, loss_series 33.791709899902344
epoch 654, loss_perior 33.82550811767578, loss_series 33.82550811767578
epoch 655, loss_perior 33.775352478027344, loss_series 33.775352478027344
epoch 656, loss_perior 33.760475158691406, loss_series 33.760475158691406
epoch 657, loss_perior 33.75273132324219, loss_series 33.75273132324219
epoch 658, loss_perior 33.77630615234375, loss_series 33.77630615234375
epoch 659, loss_perior 33.77021026611328, loss_series 33.77021026611328
epoch 660, loss_perior 33.738616943359375, loss_series 33.738616943359375
epoch 661, loss_perior 33.763763427734375, loss_series 33.763763427734375
epoch 662, loss_perior 33.775543212890625, loss_series 33.775543212890625
epoch 663, loss_perior 33.78822326660156, loss_series 33.78822326660156
epoch 664, loss_perior 33.816246032714844, loss_series 33.816246032714844
epoch 665, loss_perior 33.809913635253906, loss_series 33.809913635253906
epoch 666, loss_perior 33.80976486206055, loss_series 33.80976486206055
epoch 667, loss_perior 33.820865631103516, loss_series 33.820865631103516
epoch 668, loss_perior 33.78789520263672, loss_series 33.78789520263672
epoch 669, loss_perior 33.82270812988281, loss_series 33.82270812988281
epoch 670, loss_perior 33.78355407714844, loss_series 33.78355407714844
epoch 671, loss_perior 33.752464294433594, loss_series 33.752464294433594
epoch 672, loss_perior 33.778106689453125, loss_series 33.778106689453125
epoch 673, loss_perior 33.78385543823242, loss_series 33.78385543823242
epoch 674, loss_perior 33.77360534667969, loss_series 33.77360534667969
epoch 675, loss_perior 33.78520584106445, loss_series 33.78520584106445
epoch 676, loss_perior 33.78883743286133, loss_series 33.78883743286133
epoch 677, loss_perior 33.76069259643555, loss_series 33.76069259643555
epoch 678, loss_perior 33.83650207519531, loss_series 33.83650207519531
epoch 679, loss_perior 33.83942794799805, loss_series 33.83942794799805
epoch 680, loss_perior 33.765625, loss_series 33.765625
epoch 681, loss_perior 33.73491668701172, loss_series 33.73491668701172
epoch 682, loss_perior 33.82844543457031, loss_series 33.82844543457031
epoch 683, loss_perior 33.81475067138672, loss_series 33.81475067138672
epoch 684, loss_perior 33.78473663330078, loss_series 33.78473663330078
epoch 685, loss_perior 33.810638427734375, loss_series 33.810638427734375
epoch 686, loss_perior 33.739776611328125, loss_series 33.739776611328125
epoch 687, loss_perior 33.76025390625, loss_series 33.76025390625
epoch 688, loss_perior 33.812076568603516, loss_series 33.812076568603516
epoch 689, loss_perior 33.83379364013672, loss_series 33.83379364013672
epoch 690, loss_perior 33.76299285888672, loss_series 33.76299285888672
epoch 691, loss_perior 33.787044525146484, loss_series 33.787044525146484
epoch 692, loss_perior 33.80731964111328, loss_series 33.80731964111328
epoch 693, loss_perior 33.82006072998047, loss_series 33.82006072998047
epoch 694, loss_perior 33.82196044921875, loss_series 33.82196044921875
epoch 695, loss_perior 33.76637268066406, loss_series 33.76637268066406
epoch 696, loss_perior 33.78995895385742, loss_series 33.78995895385742
epoch 697, loss_perior 33.82000732421875, loss_series 33.82000732421875
epoch 698, loss_perior 33.80508041381836, loss_series 33.80508041381836
	speed: 0.4008s/iter; left time: 1382.6087s
epoch 699, loss_perior 33.76466369628906, loss_series 33.76466369628906
epoch 700, loss_perior 33.802886962890625, loss_series 33.802886962890625
epoch 701, loss_perior 33.776512145996094, loss_series 33.776512145996094
epoch 702, loss_perior 33.749820709228516, loss_series 33.749820709228516
epoch 703, loss_perior 33.80104064941406, loss_series 33.80104064941406
epoch 704, loss_perior 33.78350830078125, loss_series 33.78350830078125
epoch 705, loss_perior 33.76139831542969, loss_series 33.76139831542969
epoch 706, loss_perior 33.81561279296875, loss_series 33.81561279296875
epoch 707, loss_perior 33.80160140991211, loss_series 33.80160140991211
epoch 708, loss_perior 33.788612365722656, loss_series 33.788612365722656
epoch 709, loss_perior 33.79188537597656, loss_series 33.79188537597656
epoch 710, loss_perior 33.808353424072266, loss_series 33.808353424072266
epoch 711, loss_perior 33.761959075927734, loss_series 33.761959075927734
epoch 712, loss_perior 33.846866607666016, loss_series 33.846866607666016
epoch 713, loss_perior 33.81373596191406, loss_series 33.81373596191406
epoch 714, loss_perior 33.740501403808594, loss_series 33.740501403808594
epoch 715, loss_perior 33.78037643432617, loss_series 33.78037643432617
epoch 716, loss_perior 33.80359649658203, loss_series 33.80359649658203
epoch 717, loss_perior 33.78309631347656, loss_series 33.78309631347656
epoch 718, loss_perior 33.8255729675293, loss_series 33.8255729675293
epoch 719, loss_perior 33.8205680847168, loss_series 33.8205680847168
epoch 720, loss_perior 33.81658935546875, loss_series 33.81658935546875
epoch 721, loss_perior 33.78995895385742, loss_series 33.78995895385742
epoch 722, loss_perior 33.7905387878418, loss_series 33.7905387878418
epoch 723, loss_perior 33.81576919555664, loss_series 33.81576919555664
epoch 724, loss_perior 33.82073211669922, loss_series 33.82073211669922
epoch 725, loss_perior 33.804927825927734, loss_series 33.804927825927734
epoch 726, loss_perior 33.8360595703125, loss_series 33.8360595703125
epoch 727, loss_perior 33.801513671875, loss_series 33.801513671875
epoch 728, loss_perior 33.78266143798828, loss_series 33.78266143798828
epoch 729, loss_perior 33.76508331298828, loss_series 33.76508331298828
epoch 730, loss_perior 33.77694320678711, loss_series 33.77694320678711
epoch 731, loss_perior 33.842979431152344, loss_series 33.842979431152344
epoch 732, loss_perior 33.80670166015625, loss_series 33.80670166015625
epoch 733, loss_perior 33.78472900390625, loss_series 33.78472900390625
epoch 734, loss_perior 33.792335510253906, loss_series 33.792335510253906
epoch 735, loss_perior 33.777198791503906, loss_series 33.777198791503906
epoch 736, loss_perior 33.820457458496094, loss_series 33.820457458496094
epoch 737, loss_perior 33.808372497558594, loss_series 33.808372497558594
epoch 738, loss_perior 33.82527542114258, loss_series 33.82527542114258
epoch 739, loss_perior 33.78150177001953, loss_series 33.78150177001953
epoch 740, loss_perior 33.79728698730469, loss_series 33.79728698730469
epoch 741, loss_perior 33.79328918457031, loss_series 33.79328918457031
epoch 742, loss_perior 33.751731872558594, loss_series 33.751731872558594
epoch 743, loss_perior 33.766876220703125, loss_series 33.766876220703125
epoch 744, loss_perior 33.84223175048828, loss_series 33.84223175048828
epoch 745, loss_perior 33.8065185546875, loss_series 33.8065185546875
epoch 746, loss_perior 33.802207946777344, loss_series 33.802207946777344
epoch 747, loss_perior 33.798980712890625, loss_series 33.798980712890625
epoch 748, loss_perior 33.785057067871094, loss_series 33.785057067871094
epoch 749, loss_perior 33.75043869018555, loss_series 33.75043869018555
epoch 750, loss_perior 33.81666946411133, loss_series 33.81666946411133
epoch 751, loss_perior 33.73461151123047, loss_series 33.73461151123047
epoch 752, loss_perior 33.80918502807617, loss_series 33.80918502807617
epoch 753, loss_perior 33.78190612792969, loss_series 33.78190612792969
epoch 754, loss_perior 33.76213836669922, loss_series 33.76213836669922
epoch 755, loss_perior 33.76646423339844, loss_series 33.76646423339844
epoch 756, loss_perior 33.82701110839844, loss_series 33.82701110839844
epoch 757, loss_perior 33.772430419921875, loss_series 33.772430419921875
epoch 758, loss_perior 33.81700134277344, loss_series 33.81700134277344
epoch 759, loss_perior 33.78968811035156, loss_series 33.78968811035156
epoch 760, loss_perior 33.73653793334961, loss_series 33.73653793334961
epoch 761, loss_perior 33.79563903808594, loss_series 33.79563903808594
epoch 762, loss_perior 33.81585693359375, loss_series 33.81585693359375
epoch 763, loss_perior 33.84422302246094, loss_series 33.84422302246094
epoch 764, loss_perior 33.82270431518555, loss_series 33.82270431518555
epoch 765, loss_perior 33.806854248046875, loss_series 33.806854248046875
epoch 766, loss_perior 33.874267578125, loss_series 33.874267578125
epoch 767, loss_perior 33.804439544677734, loss_series 33.804439544677734
epoch 768, loss_perior 33.74053192138672, loss_series 33.74053192138672
epoch 769, loss_perior 33.819602966308594, loss_series 33.819602966308594
epoch 770, loss_perior 33.768226623535156, loss_series 33.768226623535156
epoch 771, loss_perior 33.807369232177734, loss_series 33.807369232177734
epoch 772, loss_perior 33.79798126220703, loss_series 33.79798126220703
epoch 773, loss_perior 33.84473419189453, loss_series 33.84473419189453
epoch 774, loss_perior 33.79658126831055, loss_series 33.79658126831055
epoch 775, loss_perior 33.8011474609375, loss_series 33.8011474609375
epoch 776, loss_perior 33.79029083251953, loss_series 33.79029083251953
epoch 777, loss_perior 33.79867172241211, loss_series 33.79867172241211
epoch 778, loss_perior 33.836631774902344, loss_series 33.836631774902344
epoch 779, loss_perior 33.832889556884766, loss_series 33.832889556884766
epoch 780, loss_perior 33.71791458129883, loss_series 33.71791458129883
epoch 781, loss_perior 33.772705078125, loss_series 33.772705078125
epoch 782, loss_perior 33.79325485229492, loss_series 33.79325485229492
epoch 783, loss_perior 33.80430603027344, loss_series 33.80430603027344
epoch 784, loss_perior 33.78028869628906, loss_series 33.78028869628906
epoch 785, loss_perior 33.80052185058594, loss_series 33.80052185058594
epoch 786, loss_perior 33.77553176879883, loss_series 33.77553176879883
epoch 787, loss_perior 33.8021240234375, loss_series 33.8021240234375
epoch 788, loss_perior 33.74231719970703, loss_series 33.74231719970703
epoch 789, loss_perior 33.798248291015625, loss_series 33.798248291015625
epoch 790, loss_perior 33.78388977050781, loss_series 33.78388977050781
epoch 791, loss_perior 33.77534484863281, loss_series 33.77534484863281
epoch 792, loss_perior 33.74364471435547, loss_series 33.74364471435547
epoch 793, loss_perior 33.768314361572266, loss_series 33.768314361572266
epoch 794, loss_perior 33.78574752807617, loss_series 33.78574752807617
epoch 795, loss_perior 33.78383255004883, loss_series 33.78383255004883
epoch 796, loss_perior 33.80654525756836, loss_series 33.80654525756836
epoch 797, loss_perior 33.770591735839844, loss_series 33.770591735839844
epoch 798, loss_perior 33.788490295410156, loss_series 33.788490295410156
	speed: 0.4009s/iter; left time: 1342.8807s
epoch 799, loss_perior 33.82295227050781, loss_series 33.82295227050781
epoch 800, loss_perior 33.82122039794922, loss_series 33.82122039794922
epoch 801, loss_perior 33.76427459716797, loss_series 33.76427459716797
epoch 802, loss_perior 33.78428649902344, loss_series 33.78428649902344
epoch 803, loss_perior 33.81826400756836, loss_series 33.81826400756836
epoch 804, loss_perior 33.724632263183594, loss_series 33.724632263183594
epoch 805, loss_perior 33.7374267578125, loss_series 33.7374267578125
epoch 806, loss_perior 33.75340270996094, loss_series 33.75340270996094
epoch 807, loss_perior 33.75240707397461, loss_series 33.75240707397461
epoch 808, loss_perior 33.77407455444336, loss_series 33.77407455444336
epoch 809, loss_perior 33.783447265625, loss_series 33.783447265625
epoch 810, loss_perior 33.80066680908203, loss_series 33.80066680908203
epoch 811, loss_perior 33.83733367919922, loss_series 33.83733367919922
epoch 812, loss_perior 33.835941314697266, loss_series 33.835941314697266
epoch 813, loss_perior 33.786048889160156, loss_series 33.786048889160156
epoch 814, loss_perior 33.81582260131836, loss_series 33.81582260131836
epoch 815, loss_perior 33.849769592285156, loss_series 33.849769592285156
epoch 816, loss_perior 33.78028869628906, loss_series 33.78028869628906
epoch 817, loss_perior 33.79461669921875, loss_series 33.79461669921875
epoch 818, loss_perior 33.78716278076172, loss_series 33.78716278076172
epoch 819, loss_perior 33.80164337158203, loss_series 33.80164337158203
epoch 820, loss_perior 33.73322296142578, loss_series 33.73322296142578
epoch 821, loss_perior 33.763572692871094, loss_series 33.763572692871094
epoch 822, loss_perior 33.83290100097656, loss_series 33.83290100097656
epoch 823, loss_perior 33.79401779174805, loss_series 33.79401779174805
epoch 824, loss_perior 33.86089324951172, loss_series 33.86089324951172
epoch 825, loss_perior 33.79410934448242, loss_series 33.79410934448242
epoch 826, loss_perior 33.78919982910156, loss_series 33.78919982910156
epoch 827, loss_perior 33.7404899597168, loss_series 33.7404899597168
epoch 828, loss_perior 33.75465393066406, loss_series 33.75465393066406
epoch 829, loss_perior 33.789161682128906, loss_series 33.789161682128906
epoch 830, loss_perior 33.759925842285156, loss_series 33.759925842285156
epoch 831, loss_perior 33.825897216796875, loss_series 33.825897216796875
epoch 832, loss_perior 33.78025436401367, loss_series 33.78025436401367
epoch 833, loss_perior 33.78714370727539, loss_series 33.78714370727539
epoch 834, loss_perior 33.754249572753906, loss_series 33.754249572753906
epoch 835, loss_perior 33.782325744628906, loss_series 33.782325744628906
epoch 836, loss_perior 33.797576904296875, loss_series 33.797576904296875
epoch 837, loss_perior 33.755577087402344, loss_series 33.755577087402344
epoch 838, loss_perior 33.7331428527832, loss_series 33.7331428527832
epoch 839, loss_perior 33.82367706298828, loss_series 33.82367706298828
epoch 840, loss_perior 33.82047653198242, loss_series 33.82047653198242
epoch 841, loss_perior 33.83555603027344, loss_series 33.83555603027344
epoch 842, loss_perior 33.81458282470703, loss_series 33.81458282470703
epoch 843, loss_perior 33.77461624145508, loss_series 33.77461624145508
epoch 844, loss_perior 33.81563949584961, loss_series 33.81563949584961
epoch 845, loss_perior 33.75841522216797, loss_series 33.75841522216797
epoch 846, loss_perior 33.760528564453125, loss_series 33.760528564453125
epoch 847, loss_perior 33.80442810058594, loss_series 33.80442810058594
epoch 848, loss_perior 33.795616149902344, loss_series 33.795616149902344
epoch 849, loss_perior 33.77629089355469, loss_series 33.77629089355469
epoch 850, loss_perior 33.868247985839844, loss_series 33.868247985839844
epoch 851, loss_perior 33.79859924316406, loss_series 33.79859924316406
epoch 852, loss_perior 33.76631164550781, loss_series 33.76631164550781
epoch 853, loss_perior 33.7829704284668, loss_series 33.7829704284668
epoch 854, loss_perior 33.77286911010742, loss_series 33.77286911010742
epoch 855, loss_perior 33.808921813964844, loss_series 33.808921813964844
epoch 856, loss_perior 33.80392074584961, loss_series 33.80392074584961
epoch 857, loss_perior 33.819602966308594, loss_series 33.819602966308594
epoch 858, loss_perior 33.762603759765625, loss_series 33.762603759765625
epoch 859, loss_perior 33.76899719238281, loss_series 33.76899719238281
epoch 860, loss_perior 33.8013916015625, loss_series 33.8013916015625
epoch 861, loss_perior 33.790950775146484, loss_series 33.790950775146484
epoch 862, loss_perior 33.789581298828125, loss_series 33.789581298828125
epoch 863, loss_perior 33.83234405517578, loss_series 33.83234405517578
epoch 864, loss_perior 33.84878158569336, loss_series 33.84878158569336
epoch 865, loss_perior 33.809410095214844, loss_series 33.809410095214844
epoch 866, loss_perior 33.80866241455078, loss_series 33.80866241455078
epoch 867, loss_perior 33.7990837097168, loss_series 33.7990837097168
epoch 868, loss_perior 33.760986328125, loss_series 33.760986328125
epoch 869, loss_perior 33.75678253173828, loss_series 33.75678253173828
epoch 870, loss_perior 33.7908935546875, loss_series 33.7908935546875
epoch 871, loss_perior 33.806888580322266, loss_series 33.806888580322266
epoch 872, loss_perior 33.81717300415039, loss_series 33.81717300415039
epoch 873, loss_perior 33.809226989746094, loss_series 33.809226989746094
epoch 874, loss_perior 33.78238296508789, loss_series 33.78238296508789
epoch 875, loss_perior 33.75984191894531, loss_series 33.75984191894531
epoch 876, loss_perior 33.78279113769531, loss_series 33.78279113769531
epoch 877, loss_perior 33.81842041015625, loss_series 33.81842041015625
epoch 878, loss_perior 33.75496292114258, loss_series 33.75496292114258
epoch 879, loss_perior 33.80579376220703, loss_series 33.80579376220703
epoch 880, loss_perior 33.80849075317383, loss_series 33.80849075317383
epoch 881, loss_perior 33.77787399291992, loss_series 33.77787399291992
epoch 882, loss_perior 33.79003143310547, loss_series 33.79003143310547
epoch 883, loss_perior 33.760169982910156, loss_series 33.760169982910156
epoch 884, loss_perior 33.79003143310547, loss_series 33.79003143310547
epoch 885, loss_perior 33.793792724609375, loss_series 33.793792724609375
epoch 886, loss_perior 33.75502014160156, loss_series 33.75502014160156
epoch 887, loss_perior 33.73556137084961, loss_series 33.73556137084961
epoch 888, loss_perior 33.811031341552734, loss_series 33.811031341552734
epoch 889, loss_perior 33.760372161865234, loss_series 33.760372161865234
epoch 890, loss_perior 33.81325149536133, loss_series 33.81325149536133
epoch 891, loss_perior 33.807613372802734, loss_series 33.807613372802734
epoch 892, loss_perior 33.8255500793457, loss_series 33.8255500793457
epoch 893, loss_perior 33.750003814697266, loss_series 33.750003814697266
epoch 894, loss_perior 33.791175842285156, loss_series 33.791175842285156
epoch 895, loss_perior 33.792938232421875, loss_series 33.792938232421875
epoch 896, loss_perior 33.82168197631836, loss_series 33.82168197631836
epoch 897, loss_perior 33.81195831298828, loss_series 33.81195831298828
epoch 898, loss_perior 33.78050231933594, loss_series 33.78050231933594
	speed: 0.4012s/iter; left time: 1303.8817s
epoch 899, loss_perior 33.840599060058594, loss_series 33.840599060058594
epoch 900, loss_perior 33.8031005859375, loss_series 33.8031005859375
epoch 901, loss_perior 33.7922477722168, loss_series 33.7922477722168
epoch 902, loss_perior 33.76594543457031, loss_series 33.76594543457031
epoch 903, loss_perior 33.78801727294922, loss_series 33.78801727294922
epoch 904, loss_perior 33.81587219238281, loss_series 33.81587219238281
epoch 905, loss_perior 33.791412353515625, loss_series 33.791412353515625
epoch 906, loss_perior 33.783607482910156, loss_series 33.783607482910156
epoch 907, loss_perior 33.78855895996094, loss_series 33.78855895996094
epoch 908, loss_perior 33.753211975097656, loss_series 33.753211975097656
epoch 909, loss_perior 33.75605773925781, loss_series 33.75605773925781
epoch 910, loss_perior 33.75822448730469, loss_series 33.75822448730469
epoch 911, loss_perior 33.79248809814453, loss_series 33.79248809814453
epoch 912, loss_perior 33.769325256347656, loss_series 33.769325256347656
epoch 913, loss_perior 33.81473922729492, loss_series 33.81473922729492
epoch 914, loss_perior 33.808555603027344, loss_series 33.808555603027344
epoch 915, loss_perior 33.757080078125, loss_series 33.757080078125
epoch 916, loss_perior 33.7709846496582, loss_series 33.7709846496582
epoch 917, loss_perior 33.76731872558594, loss_series 33.76731872558594
epoch 918, loss_perior 33.78759765625, loss_series 33.78759765625
epoch 919, loss_perior 33.80613708496094, loss_series 33.80613708496094
epoch 920, loss_perior 33.764404296875, loss_series 33.764404296875
epoch 921, loss_perior 33.74104690551758, loss_series 33.74104690551758
epoch 922, loss_perior 33.803497314453125, loss_series 33.803497314453125
epoch 923, loss_perior 33.79410171508789, loss_series 33.79410171508789
epoch 924, loss_perior 33.755958557128906, loss_series 33.755958557128906
epoch 925, loss_perior 33.757972717285156, loss_series 33.757972717285156
epoch 926, loss_perior 33.86429214477539, loss_series 33.86429214477539
epoch 927, loss_perior 33.78350830078125, loss_series 33.78350830078125
epoch 928, loss_perior 33.7791748046875, loss_series 33.7791748046875
epoch 929, loss_perior 33.78936004638672, loss_series 33.78936004638672
epoch 930, loss_perior 33.862152099609375, loss_series 33.862152099609375
epoch 931, loss_perior 33.74877166748047, loss_series 33.74877166748047
epoch 932, loss_perior 33.794219970703125, loss_series 33.794219970703125
epoch 933, loss_perior 33.7967643737793, loss_series 33.7967643737793
epoch 934, loss_perior 33.83036804199219, loss_series 33.83036804199219
epoch 935, loss_perior 33.78190612792969, loss_series 33.78190612792969
epoch 936, loss_perior 33.77577209472656, loss_series 33.77577209472656
epoch 937, loss_perior 33.79453659057617, loss_series 33.79453659057617
epoch 938, loss_perior 33.741233825683594, loss_series 33.741233825683594
epoch 939, loss_perior 33.739830017089844, loss_series 33.739830017089844
epoch 940, loss_perior 33.77381134033203, loss_series 33.77381134033203
epoch 941, loss_perior 33.77100372314453, loss_series 33.77100372314453
epoch 942, loss_perior 33.708953857421875, loss_series 33.708953857421875
epoch 943, loss_perior 33.79045104980469, loss_series 33.79045104980469
epoch 944, loss_perior 33.80638885498047, loss_series 33.80638885498047
epoch 945, loss_perior 33.72040939331055, loss_series 33.72040939331055
epoch 946, loss_perior 33.747047424316406, loss_series 33.747047424316406
epoch 947, loss_perior 33.794246673583984, loss_series 33.794246673583984
epoch 948, loss_perior 33.71860885620117, loss_series 33.71860885620117
epoch 949, loss_perior 33.846397399902344, loss_series 33.846397399902344
epoch 950, loss_perior 33.75490951538086, loss_series 33.75490951538086
epoch 951, loss_perior 33.801292419433594, loss_series 33.801292419433594
epoch 952, loss_perior 33.816776275634766, loss_series 33.816776275634766
epoch 953, loss_perior 33.70506286621094, loss_series 33.70506286621094
epoch 954, loss_perior 33.78016662597656, loss_series 33.78016662597656
epoch 955, loss_perior 33.792320251464844, loss_series 33.792320251464844
epoch 956, loss_perior 33.759117126464844, loss_series 33.759117126464844
epoch 957, loss_perior 33.805599212646484, loss_series 33.805599212646484
epoch 958, loss_perior 33.76069259643555, loss_series 33.76069259643555
epoch 959, loss_perior 33.77777099609375, loss_series 33.77777099609375
epoch 960, loss_perior 33.811119079589844, loss_series 33.811119079589844
epoch 961, loss_perior 33.763648986816406, loss_series 33.763648986816406
epoch 962, loss_perior 33.77931213378906, loss_series 33.77931213378906
epoch 963, loss_perior 33.79545593261719, loss_series 33.79545593261719
epoch 964, loss_perior 33.79377746582031, loss_series 33.79377746582031
epoch 965, loss_perior 33.772186279296875, loss_series 33.772186279296875
epoch 966, loss_perior 33.72785568237305, loss_series 33.72785568237305
epoch 967, loss_perior 33.82759094238281, loss_series 33.82759094238281
epoch 968, loss_perior 33.76823425292969, loss_series 33.76823425292969
epoch 969, loss_perior 33.81744384765625, loss_series 33.81744384765625
epoch 970, loss_perior 33.76312255859375, loss_series 33.76312255859375
epoch 971, loss_perior 33.79726791381836, loss_series 33.79726791381836
epoch 972, loss_perior 33.74819564819336, loss_series 33.74819564819336
epoch 973, loss_perior 33.73020935058594, loss_series 33.73020935058594
epoch 974, loss_perior 33.76134490966797, loss_series 33.76134490966797
epoch 975, loss_perior 33.774776458740234, loss_series 33.774776458740234
epoch 976, loss_perior 33.76841354370117, loss_series 33.76841354370117
epoch 977, loss_perior 33.81653594970703, loss_series 33.81653594970703
epoch 978, loss_perior 33.82318878173828, loss_series 33.82318878173828
epoch 979, loss_perior 33.822261810302734, loss_series 33.822261810302734
epoch 980, loss_perior 33.82884979248047, loss_series 33.82884979248047
epoch 981, loss_perior 33.778289794921875, loss_series 33.778289794921875
epoch 982, loss_perior 33.85923767089844, loss_series 33.85923767089844
epoch 983, loss_perior 33.81303024291992, loss_series 33.81303024291992
epoch 984, loss_perior 33.82511520385742, loss_series 33.82511520385742
epoch 985, loss_perior 33.773651123046875, loss_series 33.773651123046875
epoch 986, loss_perior 33.79736328125, loss_series 33.79736328125
epoch 987, loss_perior 33.805091857910156, loss_series 33.805091857910156
epoch 988, loss_perior 33.75272750854492, loss_series 33.75272750854492
epoch 989, loss_perior 33.766807556152344, loss_series 33.766807556152344
epoch 990, loss_perior 33.75120544433594, loss_series 33.75120544433594
epoch 991, loss_perior 33.816680908203125, loss_series 33.816680908203125
epoch 992, loss_perior 33.75443649291992, loss_series 33.75443649291992
epoch 993, loss_perior 33.762916564941406, loss_series 33.762916564941406
epoch 994, loss_perior 33.783355712890625, loss_series 33.783355712890625
epoch 995, loss_perior 33.726722717285156, loss_series 33.726722717285156
epoch 996, loss_perior 33.74488067626953, loss_series 33.74488067626953
epoch 997, loss_perior 33.750518798828125, loss_series 33.750518798828125
epoch 998, loss_perior 33.73582077026367, loss_series 33.73582077026367
	speed: 0.4010s/iter; left time: 1263.2056s
epoch 999, loss_perior 33.73542785644531, loss_series 33.73542785644531
epoch 1000, loss_perior 33.787841796875, loss_series 33.787841796875
epoch 1001, loss_perior 33.812103271484375, loss_series 33.812103271484375
epoch 1002, loss_perior 33.82217788696289, loss_series 33.82217788696289
epoch 1003, loss_perior 33.75608825683594, loss_series 33.75608825683594
epoch 1004, loss_perior 33.78590393066406, loss_series 33.78590393066406
epoch 1005, loss_perior 33.783958435058594, loss_series 33.783958435058594
epoch 1006, loss_perior 33.797828674316406, loss_series 33.797828674316406
epoch 1007, loss_perior 33.78053665161133, loss_series 33.78053665161133
epoch 1008, loss_perior 33.81705856323242, loss_series 33.81705856323242
epoch 1009, loss_perior 33.824546813964844, loss_series 33.824546813964844
epoch 1010, loss_perior 33.763248443603516, loss_series 33.763248443603516
epoch 1011, loss_perior 33.794395446777344, loss_series 33.794395446777344
epoch 1012, loss_perior 33.74951171875, loss_series 33.74951171875
epoch 1013, loss_perior 33.69997024536133, loss_series 33.69997024536133
epoch 1014, loss_perior 33.75526809692383, loss_series 33.75526809692383
epoch 1015, loss_perior 33.78633499145508, loss_series 33.78633499145508
epoch 1016, loss_perior 33.769405364990234, loss_series 33.769405364990234
epoch 1017, loss_perior 33.79055404663086, loss_series 33.79055404663086
epoch 1018, loss_perior 33.718482971191406, loss_series 33.718482971191406
epoch 1019, loss_perior 33.805328369140625, loss_series 33.805328369140625
epoch 1020, loss_perior 33.778690338134766, loss_series 33.778690338134766
epoch 1021, loss_perior 33.792144775390625, loss_series 33.792144775390625
epoch 1022, loss_perior 33.79169845581055, loss_series 33.79169845581055
epoch 1023, loss_perior 33.772613525390625, loss_series 33.772613525390625
epoch 1024, loss_perior 33.79496765136719, loss_series 33.79496765136719
epoch 1025, loss_perior 33.791839599609375, loss_series 33.791839599609375
epoch 1026, loss_perior 33.75109100341797, loss_series 33.75109100341797
epoch 1027, loss_perior 33.749149322509766, loss_series 33.749149322509766
epoch 1028, loss_perior 33.77629852294922, loss_series 33.77629852294922
epoch 1029, loss_perior 33.79078674316406, loss_series 33.79078674316406
epoch 1030, loss_perior 33.749839782714844, loss_series 33.749839782714844
epoch 1031, loss_perior 33.77437973022461, loss_series 33.77437973022461
epoch 1032, loss_perior 33.81452941894531, loss_series 33.81452941894531
epoch 1033, loss_perior 33.80073547363281, loss_series 33.80073547363281
epoch 1034, loss_perior 33.77342987060547, loss_series 33.77342987060547
epoch 1035, loss_perior 33.75432586669922, loss_series 33.75432586669922
epoch 1036, loss_perior 33.750701904296875, loss_series 33.750701904296875
epoch 1037, loss_perior 33.7857666015625, loss_series 33.7857666015625
epoch 1038, loss_perior 33.80628967285156, loss_series 33.80628967285156
epoch 1039, loss_perior 33.7808723449707, loss_series 33.7808723449707
epoch 1040, loss_perior 33.747772216796875, loss_series 33.747772216796875
epoch 1041, loss_perior 33.704345703125, loss_series 33.704345703125
epoch 1042, loss_perior 33.74967956542969, loss_series 33.74967956542969
epoch 1043, loss_perior 33.77696228027344, loss_series 33.77696228027344
epoch 1044, loss_perior 33.754966735839844, loss_series 33.754966735839844
epoch 1045, loss_perior 33.77732849121094, loss_series 33.77732849121094
epoch 1046, loss_perior 33.75088119506836, loss_series 33.75088119506836
epoch 1047, loss_perior 33.773136138916016, loss_series 33.773136138916016
epoch 1048, loss_perior 33.73039627075195, loss_series 33.73039627075195
epoch 1049, loss_perior 33.7413330078125, loss_series 33.7413330078125
epoch 1050, loss_perior 33.78214645385742, loss_series 33.78214645385742
epoch 1051, loss_perior 33.77569580078125, loss_series 33.77569580078125
epoch 1052, loss_perior 33.80168151855469, loss_series 33.80168151855469
epoch 1053, loss_perior 33.804039001464844, loss_series 33.804039001464844
epoch 1054, loss_perior 33.72728729248047, loss_series 33.72728729248047
epoch 1055, loss_perior 33.7951545715332, loss_series 33.7951545715332
epoch 1056, loss_perior 33.77070617675781, loss_series 33.77070617675781
epoch 1057, loss_perior 33.787071228027344, loss_series 33.787071228027344
epoch 1058, loss_perior 33.75827407836914, loss_series 33.75827407836914
epoch 1059, loss_perior 33.78116989135742, loss_series 33.78116989135742
epoch 1060, loss_perior 33.80247497558594, loss_series 33.80247497558594
epoch 1061, loss_perior 33.7381591796875, loss_series 33.7381591796875
epoch 1062, loss_perior 33.745296478271484, loss_series 33.745296478271484
epoch 1063, loss_perior 33.79682159423828, loss_series 33.79682159423828
epoch 1064, loss_perior 33.791259765625, loss_series 33.791259765625
epoch 1065, loss_perior 33.73674011230469, loss_series 33.73674011230469
epoch 1066, loss_perior 33.77785873413086, loss_series 33.77785873413086
epoch 1067, loss_perior 33.787437438964844, loss_series 33.787437438964844
epoch 1068, loss_perior 33.79118347167969, loss_series 33.79118347167969
epoch 1069, loss_perior 33.746543884277344, loss_series 33.746543884277344
epoch 1070, loss_perior 33.70556640625, loss_series 33.70556640625
epoch 1071, loss_perior 33.8033447265625, loss_series 33.8033447265625
epoch 1072, loss_perior 33.760581970214844, loss_series 33.760581970214844
epoch 1073, loss_perior 33.77717590332031, loss_series 33.77717590332031
epoch 1074, loss_perior 33.76033020019531, loss_series 33.76033020019531
epoch 1075, loss_perior 33.748130798339844, loss_series 33.748130798339844
epoch 1076, loss_perior 33.76469421386719, loss_series 33.76469421386719
epoch 1077, loss_perior 33.82115936279297, loss_series 33.82115936279297
epoch 1078, loss_perior 33.76402282714844, loss_series 33.76402282714844
epoch 1079, loss_perior 33.80552673339844, loss_series 33.80552673339844
epoch 1080, loss_perior 33.78089904785156, loss_series 33.78089904785156
epoch 1081, loss_perior 33.75537109375, loss_series 33.75537109375
epoch 1082, loss_perior 33.7794189453125, loss_series 33.7794189453125
epoch 1083, loss_perior 33.777462005615234, loss_series 33.777462005615234
epoch 1084, loss_perior 33.778717041015625, loss_series 33.778717041015625
epoch 1085, loss_perior 33.77989196777344, loss_series 33.77989196777344
epoch 1086, loss_perior 33.785858154296875, loss_series 33.785858154296875
epoch 1087, loss_perior 33.79743194580078, loss_series 33.79743194580078
epoch 1088, loss_perior 33.7652702331543, loss_series 33.7652702331543
epoch 1089, loss_perior 33.78840255737305, loss_series 33.78840255737305
epoch 1090, loss_perior 33.817928314208984, loss_series 33.817928314208984
epoch 1091, loss_perior 33.76439666748047, loss_series 33.76439666748047
epoch 1092, loss_perior 33.72728729248047, loss_series 33.72728729248047
epoch 1093, loss_perior 33.76485824584961, loss_series 33.76485824584961
epoch 1094, loss_perior 33.754051208496094, loss_series 33.754051208496094
epoch 1095, loss_perior 33.77635192871094, loss_series 33.77635192871094
epoch 1096, loss_perior 33.782569885253906, loss_series 33.782569885253906
epoch 1097, loss_perior 33.798606872558594, loss_series 33.798606872558594
epoch 1098, loss_perior 33.73836135864258, loss_series 33.73836135864258
	speed: 0.4013s/iter; left time: 1223.8614s
epoch 1099, loss_perior 33.774391174316406, loss_series 33.774391174316406
epoch 1100, loss_perior 33.77761459350586, loss_series 33.77761459350586
epoch 1101, loss_perior 33.781837463378906, loss_series 33.781837463378906
epoch 1102, loss_perior 33.76291275024414, loss_series 33.76291275024414
epoch 1103, loss_perior 33.759212493896484, loss_series 33.759212493896484
epoch 1104, loss_perior 33.722591400146484, loss_series 33.722591400146484
epoch 1105, loss_perior 33.767478942871094, loss_series 33.767478942871094
epoch 1106, loss_perior 33.80500793457031, loss_series 33.80500793457031
epoch 1107, loss_perior 33.78894805908203, loss_series 33.78894805908203
epoch 1108, loss_perior 33.75126266479492, loss_series 33.75126266479492
epoch 1109, loss_perior 33.74466323852539, loss_series 33.74466323852539
epoch 1110, loss_perior 33.746368408203125, loss_series 33.746368408203125
epoch 1111, loss_perior 33.77287292480469, loss_series 33.77287292480469
epoch 1112, loss_perior 33.73108673095703, loss_series 33.73108673095703
epoch 1113, loss_perior 33.75023651123047, loss_series 33.75023651123047
epoch 1114, loss_perior 33.80126190185547, loss_series 33.80126190185547
epoch 1115, loss_perior 33.77982711791992, loss_series 33.77982711791992
epoch 1116, loss_perior 33.742645263671875, loss_series 33.742645263671875
epoch 1117, loss_perior 33.714881896972656, loss_series 33.714881896972656
epoch 1118, loss_perior 33.73895263671875, loss_series 33.73895263671875
epoch 1119, loss_perior 33.751922607421875, loss_series 33.751922607421875
epoch 1120, loss_perior 33.7618408203125, loss_series 33.7618408203125
epoch 1121, loss_perior 33.785491943359375, loss_series 33.785491943359375
epoch 1122, loss_perior 33.7606315612793, loss_series 33.7606315612793
epoch 1123, loss_perior 33.76606750488281, loss_series 33.76606750488281
epoch 1124, loss_perior 33.755409240722656, loss_series 33.755409240722656
epoch 1125, loss_perior 33.78611755371094, loss_series 33.78611755371094
epoch 1126, loss_perior 33.77356719970703, loss_series 33.77356719970703
epoch 1127, loss_perior 33.76249694824219, loss_series 33.76249694824219
epoch 1128, loss_perior 33.77312469482422, loss_series 33.77312469482422
epoch 1129, loss_perior 33.748931884765625, loss_series 33.748931884765625
epoch 1130, loss_perior 33.82823181152344, loss_series 33.82823181152344
epoch 1131, loss_perior 33.751686096191406, loss_series 33.751686096191406
epoch 1132, loss_perior 33.7325439453125, loss_series 33.7325439453125
epoch 1133, loss_perior 33.80265808105469, loss_series 33.80265808105469
epoch 1134, loss_perior 33.795814514160156, loss_series 33.795814514160156
epoch 1135, loss_perior 33.762271881103516, loss_series 33.762271881103516
epoch 1136, loss_perior 33.77311325073242, loss_series 33.77311325073242
epoch 1137, loss_perior 33.71978759765625, loss_series 33.71978759765625
epoch 1138, loss_perior 33.73714065551758, loss_series 33.73714065551758
epoch 1139, loss_perior 33.78032684326172, loss_series 33.78032684326172
epoch 1140, loss_perior 33.78356170654297, loss_series 33.78356170654297
epoch 1141, loss_perior 33.73216247558594, loss_series 33.73216247558594
epoch 1142, loss_perior 33.76628875732422, loss_series 33.76628875732422
epoch 1143, loss_perior 33.73289489746094, loss_series 33.73289489746094
epoch 1144, loss_perior 33.75236129760742, loss_series 33.75236129760742
epoch 1145, loss_perior 33.748504638671875, loss_series 33.748504638671875
epoch 1146, loss_perior 33.76185607910156, loss_series 33.76185607910156
epoch 1147, loss_perior 33.77458190917969, loss_series 33.77458190917969
epoch 1148, loss_perior 33.80621337890625, loss_series 33.80621337890625
epoch 1149, loss_perior 33.768341064453125, loss_series 33.768341064453125
epoch 1150, loss_perior 33.76145935058594, loss_series 33.76145935058594
epoch 1151, loss_perior 33.755455017089844, loss_series 33.755455017089844
epoch 1152, loss_perior 33.7764892578125, loss_series 33.7764892578125
epoch 1153, loss_perior 33.76084518432617, loss_series 33.76084518432617
epoch 1154, loss_perior 33.74470901489258, loss_series 33.74470901489258
epoch 1155, loss_perior 33.81792449951172, loss_series 33.81792449951172
epoch 1156, loss_perior 33.78167724609375, loss_series 33.78167724609375
epoch 1157, loss_perior 33.7684440612793, loss_series 33.7684440612793
epoch 1158, loss_perior 33.74663543701172, loss_series 33.74663543701172
epoch 1159, loss_perior 33.75556182861328, loss_series 33.75556182861328
epoch 1160, loss_perior 33.75852966308594, loss_series 33.75852966308594
epoch 1161, loss_perior 33.76006317138672, loss_series 33.76006317138672
epoch 1162, loss_perior 33.80329132080078, loss_series 33.80329132080078
epoch 1163, loss_perior 33.75679016113281, loss_series 33.75679016113281
epoch 1164, loss_perior 33.7711067199707, loss_series 33.7711067199707
epoch 1165, loss_perior 33.717079162597656, loss_series 33.717079162597656
epoch 1166, loss_perior 33.697349548339844, loss_series 33.697349548339844
epoch 1167, loss_perior 33.759185791015625, loss_series 33.759185791015625
epoch 1168, loss_perior 33.79115295410156, loss_series 33.79115295410156
epoch 1169, loss_perior 33.74766540527344, loss_series 33.74766540527344
epoch 1170, loss_perior 33.74872970581055, loss_series 33.74872970581055
epoch 1171, loss_perior 33.74031066894531, loss_series 33.74031066894531
epoch 1172, loss_perior 33.708431243896484, loss_series 33.708431243896484
epoch 1173, loss_perior 33.752113342285156, loss_series 33.752113342285156
epoch 1174, loss_perior 33.731163024902344, loss_series 33.731163024902344
epoch 1175, loss_perior 33.72205352783203, loss_series 33.72205352783203
epoch 1176, loss_perior 33.7225341796875, loss_series 33.7225341796875
epoch 1177, loss_perior 33.715614318847656, loss_series 33.715614318847656
epoch 1178, loss_perior 33.72783279418945, loss_series 33.72783279418945
epoch 1179, loss_perior 33.74029541015625, loss_series 33.74029541015625
epoch 1180, loss_perior 33.76539611816406, loss_series 33.76539611816406
epoch 1181, loss_perior 33.76353073120117, loss_series 33.76353073120117
epoch 1182, loss_perior 33.74432373046875, loss_series 33.74432373046875
epoch 1183, loss_perior 33.772743225097656, loss_series 33.772743225097656
epoch 1184, loss_perior 33.80235290527344, loss_series 33.80235290527344
epoch 1185, loss_perior 33.759071350097656, loss_series 33.759071350097656
epoch 1186, loss_perior 33.7969970703125, loss_series 33.7969970703125
epoch 1187, loss_perior 33.80152893066406, loss_series 33.80152893066406
epoch 1188, loss_perior 33.76528549194336, loss_series 33.76528549194336
epoch 1189, loss_perior 33.750911712646484, loss_series 33.750911712646484
epoch 1190, loss_perior 33.766807556152344, loss_series 33.766807556152344
epoch 1191, loss_perior 33.739192962646484, loss_series 33.739192962646484
epoch 1192, loss_perior 33.7760009765625, loss_series 33.7760009765625
epoch 1193, loss_perior 33.743770599365234, loss_series 33.743770599365234
epoch 1194, loss_perior 33.796146392822266, loss_series 33.796146392822266
epoch 1195, loss_perior 33.79933166503906, loss_series 33.79933166503906
epoch 1196, loss_perior 33.75724411010742, loss_series 33.75724411010742
epoch 1197, loss_perior 33.7857551574707, loss_series 33.7857551574707
epoch 1198, loss_perior 33.719078063964844, loss_series 33.719078063964844
	speed: 0.4011s/iter; left time: 1183.1083s
epoch 1199, loss_perior 33.765235900878906, loss_series 33.765235900878906
epoch 1200, loss_perior 33.80534362792969, loss_series 33.80534362792969
epoch 1201, loss_perior 33.769283294677734, loss_series 33.769283294677734
epoch 1202, loss_perior 33.78780746459961, loss_series 33.78780746459961
epoch 1203, loss_perior 33.74982452392578, loss_series 33.74982452392578
epoch 1204, loss_perior 33.78374099731445, loss_series 33.78374099731445
epoch 1205, loss_perior 33.71140670776367, loss_series 33.71140670776367
epoch 1206, loss_perior 33.74419403076172, loss_series 33.74419403076172
epoch 1207, loss_perior 33.78440856933594, loss_series 33.78440856933594
epoch 1208, loss_perior 33.79655075073242, loss_series 33.79655075073242
epoch 1209, loss_perior 33.71753692626953, loss_series 33.71753692626953
epoch 1210, loss_perior 33.77529525756836, loss_series 33.77529525756836
epoch 1211, loss_perior 33.771697998046875, loss_series 33.771697998046875
epoch 1212, loss_perior 33.751285552978516, loss_series 33.751285552978516
epoch 1213, loss_perior 33.77024459838867, loss_series 33.77024459838867
epoch 1214, loss_perior 33.767723083496094, loss_series 33.767723083496094
epoch 1215, loss_perior 33.71991729736328, loss_series 33.71991729736328
epoch 1216, loss_perior 33.706668853759766, loss_series 33.706668853759766
epoch 1217, loss_perior 33.762943267822266, loss_series 33.762943267822266
epoch 1218, loss_perior 33.67840576171875, loss_series 33.67840576171875
epoch 1219, loss_perior 33.73502731323242, loss_series 33.73502731323242
epoch 1220, loss_perior 33.73738098144531, loss_series 33.73738098144531
epoch 1221, loss_perior 33.750648498535156, loss_series 33.750648498535156
epoch 1222, loss_perior 33.774166107177734, loss_series 33.774166107177734
epoch 1223, loss_perior 33.785221099853516, loss_series 33.785221099853516
epoch 1224, loss_perior 33.743953704833984, loss_series 33.743953704833984
epoch 1225, loss_perior 33.755584716796875, loss_series 33.755584716796875
epoch 1226, loss_perior 33.75362777709961, loss_series 33.75362777709961
epoch 1227, loss_perior 33.7493896484375, loss_series 33.7493896484375
epoch 1228, loss_perior 33.74958419799805, loss_series 33.74958419799805
epoch 1229, loss_perior 33.77589416503906, loss_series 33.77589416503906
epoch 1230, loss_perior 33.74519348144531, loss_series 33.74519348144531
epoch 1231, loss_perior 33.72869110107422, loss_series 33.72869110107422
epoch 1232, loss_perior 33.7790412902832, loss_series 33.7790412902832
epoch 1233, loss_perior 33.749977111816406, loss_series 33.749977111816406
epoch 1234, loss_perior 33.76469421386719, loss_series 33.76469421386719
epoch 1235, loss_perior 33.70702362060547, loss_series 33.70702362060547
epoch 1236, loss_perior 33.75847625732422, loss_series 33.75847625732422
epoch 1237, loss_perior 33.73773193359375, loss_series 33.73773193359375
epoch 1238, loss_perior 33.74717712402344, loss_series 33.74717712402344
epoch 1239, loss_perior 33.74932861328125, loss_series 33.74932861328125
epoch 1240, loss_perior 33.79966354370117, loss_series 33.79966354370117
epoch 1241, loss_perior 33.73674774169922, loss_series 33.73674774169922
epoch 1242, loss_perior 33.72266387939453, loss_series 33.72266387939453
epoch 1243, loss_perior 33.75850296020508, loss_series 33.75850296020508
epoch 1244, loss_perior 33.7354736328125, loss_series 33.7354736328125
epoch 1245, loss_perior 33.77122497558594, loss_series 33.77122497558594
epoch 1246, loss_perior 33.73574447631836, loss_series 33.73574447631836
epoch 1247, loss_perior 33.792152404785156, loss_series 33.792152404785156
epoch 1248, loss_perior 33.77560806274414, loss_series 33.77560806274414
epoch 1249, loss_perior 33.78171920776367, loss_series 33.78171920776367
epoch 1250, loss_perior 33.76032257080078, loss_series 33.76032257080078
epoch 1251, loss_perior 33.755558013916016, loss_series 33.755558013916016
epoch 1252, loss_perior 33.76426696777344, loss_series 33.76426696777344
epoch 1253, loss_perior 33.707244873046875, loss_series 33.707244873046875
epoch 1254, loss_perior 33.725948333740234, loss_series 33.725948333740234
epoch 1255, loss_perior 33.74607467651367, loss_series 33.74607467651367
epoch 1256, loss_perior 33.75672149658203, loss_series 33.75672149658203
epoch 1257, loss_perior 33.750728607177734, loss_series 33.750728607177734
epoch 1258, loss_perior 33.74943542480469, loss_series 33.74943542480469
epoch 1259, loss_perior 33.722198486328125, loss_series 33.722198486328125
epoch 1260, loss_perior 33.74456787109375, loss_series 33.74456787109375
epoch 1261, loss_perior 33.737548828125, loss_series 33.737548828125
epoch 1262, loss_perior 33.71442794799805, loss_series 33.71442794799805
epoch 1263, loss_perior 33.75907516479492, loss_series 33.75907516479492
epoch 1264, loss_perior 33.76543426513672, loss_series 33.76543426513672
epoch 1265, loss_perior 33.737789154052734, loss_series 33.737789154052734
epoch 1266, loss_perior 33.697731018066406, loss_series 33.697731018066406
epoch 1267, loss_perior 33.75127410888672, loss_series 33.75127410888672
epoch 1268, loss_perior 33.76013946533203, loss_series 33.76013946533203
epoch 1269, loss_perior 33.746864318847656, loss_series 33.746864318847656
epoch 1270, loss_perior 33.81622314453125, loss_series 33.81622314453125
epoch 1271, loss_perior 33.79753875732422, loss_series 33.79753875732422
epoch 1272, loss_perior 33.768592834472656, loss_series 33.768592834472656
epoch 1273, loss_perior 33.7651481628418, loss_series 33.7651481628418
epoch 1274, loss_perior 33.73321533203125, loss_series 33.73321533203125
epoch 1275, loss_perior 33.760555267333984, loss_series 33.760555267333984
epoch 1276, loss_perior 33.72955322265625, loss_series 33.72955322265625
epoch 1277, loss_perior 33.74418640136719, loss_series 33.74418640136719
epoch 1278, loss_perior 33.743492126464844, loss_series 33.743492126464844
epoch 1279, loss_perior 33.72700119018555, loss_series 33.72700119018555
epoch 1280, loss_perior 33.727760314941406, loss_series 33.727760314941406
epoch 1281, loss_perior 33.72343444824219, loss_series 33.72343444824219
epoch 1282, loss_perior 33.72271728515625, loss_series 33.72271728515625
epoch 1283, loss_perior 33.787376403808594, loss_series 33.787376403808594
epoch 1284, loss_perior 33.734046936035156, loss_series 33.734046936035156
epoch 1285, loss_perior 33.79949951171875, loss_series 33.79949951171875
epoch 1286, loss_perior 33.733642578125, loss_series 33.733642578125
epoch 1287, loss_perior 33.77073287963867, loss_series 33.77073287963867
epoch 1288, loss_perior 33.781978607177734, loss_series 33.781978607177734
epoch 1289, loss_perior 33.79184341430664, loss_series 33.79184341430664
epoch 1290, loss_perior 33.77855682373047, loss_series 33.77855682373047
epoch 1291, loss_perior 33.7097282409668, loss_series 33.7097282409668
epoch 1292, loss_perior 33.712318420410156, loss_series 33.712318420410156
epoch 1293, loss_perior 33.73860168457031, loss_series 33.73860168457031
epoch 1294, loss_perior 33.724971771240234, loss_series 33.724971771240234
epoch 1295, loss_perior 33.721527099609375, loss_series 33.721527099609375
epoch 1296, loss_perior 33.74286651611328, loss_series 33.74286651611328
epoch 1297, loss_perior 33.78984451293945, loss_series 33.78984451293945
epoch 1298, loss_perior 33.76108932495117, loss_series 33.76108932495117
	speed: 0.4012s/iter; left time: 1143.4435s
epoch 1299, loss_perior 33.72905731201172, loss_series 33.72905731201172
epoch 1300, loss_perior 33.80957794189453, loss_series 33.80957794189453
epoch 1301, loss_perior 33.75871276855469, loss_series 33.75871276855469
epoch 1302, loss_perior 33.76139450073242, loss_series 33.76139450073242
epoch 1303, loss_perior 33.77574157714844, loss_series 33.77574157714844
epoch 1304, loss_perior 33.670223236083984, loss_series 33.670223236083984
epoch 1305, loss_perior 33.78807830810547, loss_series 33.78807830810547
epoch 1306, loss_perior 33.742122650146484, loss_series 33.742122650146484
epoch 1307, loss_perior 33.78496551513672, loss_series 33.78496551513672
epoch 1308, loss_perior 33.79140090942383, loss_series 33.79140090942383
epoch 1309, loss_perior 33.7755012512207, loss_series 33.7755012512207
epoch 1310, loss_perior 33.753211975097656, loss_series 33.753211975097656
epoch 1311, loss_perior 33.7553825378418, loss_series 33.7553825378418
epoch 1312, loss_perior 33.763397216796875, loss_series 33.763397216796875
epoch 1313, loss_perior 33.748619079589844, loss_series 33.748619079589844
epoch 1314, loss_perior 33.842647552490234, loss_series 33.842647552490234
epoch 1315, loss_perior 33.75006103515625, loss_series 33.75006103515625
epoch 1316, loss_perior 33.77250671386719, loss_series 33.77250671386719
epoch 1317, loss_perior 33.70813751220703, loss_series 33.70813751220703
epoch 1318, loss_perior 33.753684997558594, loss_series 33.753684997558594
epoch 1319, loss_perior 33.743797302246094, loss_series 33.743797302246094
epoch 1320, loss_perior 33.730743408203125, loss_series 33.730743408203125
epoch 1321, loss_perior 33.71308517456055, loss_series 33.71308517456055
epoch 1322, loss_perior 33.75458908081055, loss_series 33.75458908081055
epoch 1323, loss_perior 33.795249938964844, loss_series 33.795249938964844
epoch 1324, loss_perior 33.78078079223633, loss_series 33.78078079223633
epoch 1325, loss_perior 33.735347747802734, loss_series 33.735347747802734
epoch 1326, loss_perior 33.747398376464844, loss_series 33.747398376464844
epoch 1327, loss_perior 33.73555374145508, loss_series 33.73555374145508
epoch 1328, loss_perior 33.752864837646484, loss_series 33.752864837646484
epoch 1329, loss_perior 33.783775329589844, loss_series 33.783775329589844
epoch 1330, loss_perior 33.7653923034668, loss_series 33.7653923034668
epoch 1331, loss_perior 33.73162841796875, loss_series 33.73162841796875
epoch 1332, loss_perior 33.71532440185547, loss_series 33.71532440185547
epoch 1333, loss_perior 33.74314880371094, loss_series 33.74314880371094
epoch 1334, loss_perior 33.748985290527344, loss_series 33.748985290527344
epoch 1335, loss_perior 33.72142028808594, loss_series 33.72142028808594
epoch 1336, loss_perior 33.726226806640625, loss_series 33.726226806640625
epoch 1337, loss_perior 33.71971130371094, loss_series 33.71971130371094
epoch 1338, loss_perior 33.768375396728516, loss_series 33.768375396728516
epoch 1339, loss_perior 33.74839401245117, loss_series 33.74839401245117
epoch 1340, loss_perior 33.74542236328125, loss_series 33.74542236328125
epoch 1341, loss_perior 33.7508430480957, loss_series 33.7508430480957
epoch 1342, loss_perior 33.72477340698242, loss_series 33.72477340698242
epoch 1343, loss_perior 33.73114776611328, loss_series 33.73114776611328
epoch 1344, loss_perior 33.750831604003906, loss_series 33.750831604003906
epoch 1345, loss_perior 33.70159149169922, loss_series 33.70159149169922
epoch 1346, loss_perior 33.7542839050293, loss_series 33.7542839050293
epoch 1347, loss_perior 33.75676727294922, loss_series 33.75676727294922
epoch 1348, loss_perior 33.725162506103516, loss_series 33.725162506103516
epoch 1349, loss_perior 33.770042419433594, loss_series 33.770042419433594
epoch 1350, loss_perior 33.71609115600586, loss_series 33.71609115600586
epoch 1351, loss_perior 33.76112365722656, loss_series 33.76112365722656
epoch 1352, loss_perior 33.72632598876953, loss_series 33.72632598876953
epoch 1353, loss_perior 33.791412353515625, loss_series 33.791412353515625
epoch 1354, loss_perior 33.73753356933594, loss_series 33.73753356933594
epoch 1355, loss_perior 33.75205993652344, loss_series 33.75205993652344
epoch 1356, loss_perior 33.7859001159668, loss_series 33.7859001159668
epoch 1357, loss_perior 33.74626541137695, loss_series 33.74626541137695
epoch 1358, loss_perior 33.75745391845703, loss_series 33.75745391845703
epoch 1359, loss_perior 33.72239303588867, loss_series 33.72239303588867
epoch 1360, loss_perior 33.68775939941406, loss_series 33.68775939941406
epoch 1361, loss_perior 33.756591796875, loss_series 33.756591796875
epoch 1362, loss_perior 33.79362487792969, loss_series 33.79362487792969
epoch 1363, loss_perior 33.737281799316406, loss_series 33.737281799316406
epoch 1364, loss_perior 33.77329635620117, loss_series 33.77329635620117
epoch 1365, loss_perior 33.737510681152344, loss_series 33.737510681152344
epoch 1366, loss_perior 33.728431701660156, loss_series 33.728431701660156
epoch 1367, loss_perior 33.735355377197266, loss_series 33.735355377197266
epoch 1368, loss_perior 33.76518630981445, loss_series 33.76518630981445
epoch 1369, loss_perior 33.77485656738281, loss_series 33.77485656738281
epoch 1370, loss_perior 33.721656799316406, loss_series 33.721656799316406
epoch 1371, loss_perior 33.75106430053711, loss_series 33.75106430053711
epoch 1372, loss_perior 33.73943328857422, loss_series 33.73943328857422
epoch 1373, loss_perior 33.72027587890625, loss_series 33.72027587890625
epoch 1374, loss_perior 33.76588439941406, loss_series 33.76588439941406
epoch 1375, loss_perior 33.72652816772461, loss_series 33.72652816772461
epoch 1376, loss_perior 33.753379821777344, loss_series 33.753379821777344
epoch 1377, loss_perior 33.70866394042969, loss_series 33.70866394042969
epoch 1378, loss_perior 33.69486999511719, loss_series 33.69486999511719
epoch 1379, loss_perior 33.739662170410156, loss_series 33.739662170410156
epoch 1380, loss_perior 33.7659912109375, loss_series 33.7659912109375
epoch 1381, loss_perior 33.736228942871094, loss_series 33.736228942871094
epoch 1382, loss_perior 33.84002685546875, loss_series 33.84002685546875
Epoch: 1, Cost time: 776.688s 
epoch 1383, loss_perior 33.70463943481445, loss_series 33.70463943481445
epoch 1384, loss_perior 33.733985900878906, loss_series 33.733985900878906
epoch 1385, loss_perior 33.747718811035156, loss_series 33.747718811035156
epoch 1386, loss_perior 33.76658630371094, loss_series 33.76658630371094
epoch 1387, loss_perior 33.776893615722656, loss_series 33.776893615722656
epoch 1388, loss_perior 33.74589157104492, loss_series 33.74589157104492
epoch 1389, loss_perior 33.78254699707031, loss_series 33.78254699707031
epoch 1390, loss_perior 33.76195526123047, loss_series 33.76195526123047
epoch 1391, loss_perior 33.758609771728516, loss_series 33.758609771728516
epoch 1392, loss_perior 33.82099914550781, loss_series 33.82099914550781
epoch 1393, loss_perior 33.797176361083984, loss_series 33.797176361083984
epoch 1394, loss_perior 33.7701530456543, loss_series 33.7701530456543
epoch 1395, loss_perior 33.716793060302734, loss_series 33.716793060302734
epoch 1396, loss_perior 33.71406173706055, loss_series 33.71406173706055
epoch 1397, loss_perior 33.710914611816406, loss_series 33.710914611816406
epoch 1398, loss_perior 33.763153076171875, loss_series 33.763153076171875
epoch 1399, loss_perior 33.755287170410156, loss_series 33.755287170410156
epoch 1400, loss_perior 33.76396560668945, loss_series 33.76396560668945
epoch 1401, loss_perior 33.75025177001953, loss_series 33.75025177001953
epoch 1402, loss_perior 33.731910705566406, loss_series 33.731910705566406
epoch 1403, loss_perior 33.71157455444336, loss_series 33.71157455444336
epoch 1404, loss_perior 33.73826599121094, loss_series 33.73826599121094
epoch 1405, loss_perior 33.687164306640625, loss_series 33.687164306640625
epoch 1406, loss_perior 33.73448944091797, loss_series 33.73448944091797
epoch 1407, loss_perior 33.78339385986328, loss_series 33.78339385986328
epoch 1408, loss_perior 33.77171325683594, loss_series 33.77171325683594
epoch 1409, loss_perior 33.75947952270508, loss_series 33.75947952270508
epoch 1410, loss_perior 33.744651794433594, loss_series 33.744651794433594
epoch 1411, loss_perior 33.7520866394043, loss_series 33.7520866394043
epoch 1412, loss_perior 33.77095031738281, loss_series 33.77095031738281
epoch 1413, loss_perior 33.76951599121094, loss_series 33.76951599121094
epoch 1414, loss_perior 33.7313232421875, loss_series 33.7313232421875
epoch 1415, loss_perior 33.762451171875, loss_series 33.762451171875
epoch 1416, loss_perior 33.74424362182617, loss_series 33.74424362182617
epoch 1417, loss_perior 33.75330352783203, loss_series 33.75330352783203
epoch 1418, loss_perior 33.76209259033203, loss_series 33.76209259033203
epoch 1419, loss_perior 33.73448944091797, loss_series 33.73448944091797
epoch 1420, loss_perior 33.73184585571289, loss_series 33.73184585571289
epoch 1421, loss_perior 33.73678207397461, loss_series 33.73678207397461
epoch 1422, loss_perior 33.73246765136719, loss_series 33.73246765136719
epoch 1423, loss_perior 33.7679443359375, loss_series 33.7679443359375
epoch 1424, loss_perior 33.76995086669922, loss_series 33.76995086669922
epoch 1425, loss_perior 33.72686004638672, loss_series 33.72686004638672
epoch 1426, loss_perior 33.7473030090332, loss_series 33.7473030090332
epoch 1427, loss_perior 33.78350067138672, loss_series 33.78350067138672
epoch 1428, loss_perior 33.80129623413086, loss_series 33.80129623413086
epoch 1429, loss_perior 33.78338623046875, loss_series 33.78338623046875
epoch 1430, loss_perior 33.76471710205078, loss_series 33.76471710205078
epoch 1431, loss_perior 33.75837326049805, loss_series 33.75837326049805
epoch 1432, loss_perior 33.7833251953125, loss_series 33.7833251953125
epoch 1433, loss_perior 33.720359802246094, loss_series 33.720359802246094
epoch 1434, loss_perior 33.69932174682617, loss_series 33.69932174682617
epoch 1435, loss_perior 33.75920104980469, loss_series 33.75920104980469
epoch 1436, loss_perior 33.78546905517578, loss_series 33.78546905517578
epoch 1437, loss_perior 33.748661041259766, loss_series 33.748661041259766
epoch 1438, loss_perior 33.74086380004883, loss_series 33.74086380004883
epoch 1439, loss_perior 33.707611083984375, loss_series 33.707611083984375
epoch 1440, loss_perior 33.75497055053711, loss_series 33.75497055053711
epoch 1441, loss_perior 33.749656677246094, loss_series 33.749656677246094
epoch 1442, loss_perior 33.730201721191406, loss_series 33.730201721191406
epoch 1443, loss_perior 33.758907318115234, loss_series 33.758907318115234
epoch 1444, loss_perior 33.7306022644043, loss_series 33.7306022644043
epoch 1445, loss_perior 33.694908142089844, loss_series 33.694908142089844
epoch 1446, loss_perior 33.77315902709961, loss_series 33.77315902709961
epoch 1447, loss_perior 33.72517013549805, loss_series 33.72517013549805
epoch 1448, loss_perior 33.71284103393555, loss_series 33.71284103393555
epoch 1449, loss_perior 33.79595947265625, loss_series 33.79595947265625
epoch 1450, loss_perior 33.779293060302734, loss_series 33.779293060302734
epoch 1451, loss_perior 33.732059478759766, loss_series 33.732059478759766
epoch 1452, loss_perior 33.711875915527344, loss_series 33.711875915527344
epoch 1453, loss_perior 33.76715087890625, loss_series 33.76715087890625
epoch 1454, loss_perior 33.72893524169922, loss_series 33.72893524169922
epoch 1455, loss_perior 33.75621032714844, loss_series 33.75621032714844
epoch 1456, loss_perior 33.72356414794922, loss_series 33.72356414794922
epoch 1457, loss_perior 33.77657699584961, loss_series 33.77657699584961
epoch 1458, loss_perior 33.777130126953125, loss_series 33.777130126953125
epoch 1459, loss_perior 33.71298599243164, loss_series 33.71298599243164
epoch 1460, loss_perior 33.73345184326172, loss_series 33.73345184326172
epoch 1461, loss_perior 33.74169921875, loss_series 33.74169921875
epoch 1462, loss_perior 33.7435302734375, loss_series 33.7435302734375
epoch 1463, loss_perior 33.730674743652344, loss_series 33.730674743652344
epoch 1464, loss_perior 33.73150634765625, loss_series 33.73150634765625
epoch 1465, loss_perior 33.76686096191406, loss_series 33.76686096191406
epoch 1466, loss_perior 33.76366424560547, loss_series 33.76366424560547
epoch 1467, loss_perior 33.74110412597656, loss_series 33.74110412597656
epoch 1468, loss_perior 33.72340393066406, loss_series 33.72340393066406
epoch 1469, loss_perior 33.763519287109375, loss_series 33.763519287109375
epoch 1470, loss_perior 33.71215057373047, loss_series 33.71215057373047
epoch 1471, loss_perior 33.72136688232422, loss_series 33.72136688232422
epoch 1472, loss_perior 33.762718200683594, loss_series 33.762718200683594
epoch 1473, loss_perior 33.78119659423828, loss_series 33.78119659423828
epoch 1474, loss_perior 33.72638702392578, loss_series 33.72638702392578
epoch 1475, loss_perior 33.739471435546875, loss_series 33.739471435546875
epoch 1476, loss_perior 33.69045639038086, loss_series 33.69045639038086
epoch 1477, loss_perior 33.77983093261719, loss_series 33.77983093261719
epoch 1478, loss_perior 33.71981430053711, loss_series 33.71981430053711
epoch 1479, loss_perior 33.727806091308594, loss_series 33.727806091308594
epoch 1480, loss_perior 33.69138717651367, loss_series 33.69138717651367
epoch 1481, loss_perior 33.737789154052734, loss_series 33.737789154052734
	speed: 2.9470s/iter; left time: 7859.6616s
epoch 1482, loss_perior 33.73747253417969, loss_series 33.73747253417969
epoch 1483, loss_perior 33.764671325683594, loss_series 33.764671325683594
epoch 1484, loss_perior 33.73462677001953, loss_series 33.73462677001953
epoch 1485, loss_perior 33.749610900878906, loss_series 33.749610900878906
epoch 1486, loss_perior 33.7630615234375, loss_series 33.7630615234375
epoch 1487, loss_perior 33.739112854003906, loss_series 33.739112854003906
epoch 1488, loss_perior 33.70811462402344, loss_series 33.70811462402344
epoch 1489, loss_perior 33.731849670410156, loss_series 33.731849670410156
epoch 1490, loss_perior 33.7764892578125, loss_series 33.7764892578125
epoch 1491, loss_perior 33.748043060302734, loss_series 33.748043060302734
epoch 1492, loss_perior 33.728485107421875, loss_series 33.728485107421875
epoch 1493, loss_perior 33.73399353027344, loss_series 33.73399353027344
epoch 1494, loss_perior 33.761810302734375, loss_series 33.761810302734375
epoch 1495, loss_perior 33.74535369873047, loss_series 33.74535369873047
epoch 1496, loss_perior 33.751319885253906, loss_series 33.751319885253906
epoch 1497, loss_perior 33.73603820800781, loss_series 33.73603820800781
epoch 1498, loss_perior 33.692626953125, loss_series 33.692626953125
epoch 1499, loss_perior 33.762184143066406, loss_series 33.762184143066406
epoch 1500, loss_perior 33.75782012939453, loss_series 33.75782012939453
epoch 1501, loss_perior 33.724449157714844, loss_series 33.724449157714844
epoch 1502, loss_perior 33.741512298583984, loss_series 33.741512298583984
epoch 1503, loss_perior 33.71013641357422, loss_series 33.71013641357422
epoch 1504, loss_perior 33.73724365234375, loss_series 33.73724365234375
epoch 1505, loss_perior 33.752994537353516, loss_series 33.752994537353516
epoch 1506, loss_perior 33.74105453491211, loss_series 33.74105453491211
epoch 1507, loss_perior 33.74213790893555, loss_series 33.74213790893555
epoch 1508, loss_perior 33.674232482910156, loss_series 33.674232482910156
epoch 1509, loss_perior 33.70112228393555, loss_series 33.70112228393555
epoch 1510, loss_perior 33.74201965332031, loss_series 33.74201965332031
epoch 1511, loss_perior 33.75250244140625, loss_series 33.75250244140625
epoch 1512, loss_perior 33.739013671875, loss_series 33.739013671875
epoch 1513, loss_perior 33.73571014404297, loss_series 33.73571014404297
epoch 1514, loss_perior 33.687095642089844, loss_series 33.687095642089844
epoch 1515, loss_perior 33.76737976074219, loss_series 33.76737976074219
epoch 1516, loss_perior 33.71289825439453, loss_series 33.71289825439453
epoch 1517, loss_perior 33.77622985839844, loss_series 33.77622985839844
epoch 1518, loss_perior 33.712406158447266, loss_series 33.712406158447266
epoch 1519, loss_perior 33.76039123535156, loss_series 33.76039123535156
epoch 1520, loss_perior 33.71200180053711, loss_series 33.71200180053711
epoch 1521, loss_perior 33.81016540527344, loss_series 33.81016540527344
epoch 1522, loss_perior 33.7235107421875, loss_series 33.7235107421875
epoch 1523, loss_perior 33.722755432128906, loss_series 33.722755432128906
epoch 1524, loss_perior 33.7473258972168, loss_series 33.7473258972168
epoch 1525, loss_perior 33.679195404052734, loss_series 33.679195404052734
epoch 1526, loss_perior 33.767215728759766, loss_series 33.767215728759766
epoch 1527, loss_perior 33.76971435546875, loss_series 33.76971435546875
epoch 1528, loss_perior 33.73949432373047, loss_series 33.73949432373047
epoch 1529, loss_perior 33.75468826293945, loss_series 33.75468826293945
epoch 1530, loss_perior 33.75443649291992, loss_series 33.75443649291992
epoch 1531, loss_perior 33.779605865478516, loss_series 33.779605865478516
epoch 1532, loss_perior 33.761112213134766, loss_series 33.761112213134766
epoch 1533, loss_perior 33.75049591064453, loss_series 33.75049591064453
epoch 1534, loss_perior 33.746803283691406, loss_series 33.746803283691406
epoch 1535, loss_perior 33.712520599365234, loss_series 33.712520599365234
epoch 1536, loss_perior 33.686405181884766, loss_series 33.686405181884766
epoch 1537, loss_perior 33.754276275634766, loss_series 33.754276275634766
epoch 1538, loss_perior 33.74980926513672, loss_series 33.74980926513672
epoch 1539, loss_perior 33.756072998046875, loss_series 33.756072998046875
epoch 1540, loss_perior 33.765769958496094, loss_series 33.765769958496094
epoch 1541, loss_perior 33.69620895385742, loss_series 33.69620895385742
epoch 1542, loss_perior 33.71853256225586, loss_series 33.71853256225586
epoch 1543, loss_perior 33.76276779174805, loss_series 33.76276779174805
epoch 1544, loss_perior 33.72686767578125, loss_series 33.72686767578125
epoch 1545, loss_perior 33.73290252685547, loss_series 33.73290252685547
epoch 1546, loss_perior 33.721946716308594, loss_series 33.721946716308594
epoch 1547, loss_perior 33.70613098144531, loss_series 33.70613098144531
epoch 1548, loss_perior 33.773101806640625, loss_series 33.773101806640625
epoch 1549, loss_perior 33.73291778564453, loss_series 33.73291778564453
epoch 1550, loss_perior 33.75181198120117, loss_series 33.75181198120117
epoch 1551, loss_perior 33.74925994873047, loss_series 33.74925994873047
epoch 1552, loss_perior 33.750450134277344, loss_series 33.750450134277344
epoch 1553, loss_perior 33.72661590576172, loss_series 33.72661590576172
epoch 1554, loss_perior 33.71227264404297, loss_series 33.71227264404297
epoch 1555, loss_perior 33.76036834716797, loss_series 33.76036834716797
epoch 1556, loss_perior 33.74043273925781, loss_series 33.74043273925781
epoch 1557, loss_perior 33.711158752441406, loss_series 33.711158752441406
epoch 1558, loss_perior 33.73505401611328, loss_series 33.73505401611328
epoch 1559, loss_perior 33.677921295166016, loss_series 33.677921295166016
epoch 1560, loss_perior 33.71137237548828, loss_series 33.71137237548828
epoch 1561, loss_perior 33.707542419433594, loss_series 33.707542419433594
epoch 1562, loss_perior 33.718841552734375, loss_series 33.718841552734375
epoch 1563, loss_perior 33.68588638305664, loss_series 33.68588638305664
epoch 1564, loss_perior 33.717613220214844, loss_series 33.717613220214844
epoch 1565, loss_perior 33.72957992553711, loss_series 33.72957992553711
epoch 1566, loss_perior 33.75297546386719, loss_series 33.75297546386719
epoch 1567, loss_perior 33.689964294433594, loss_series 33.689964294433594
epoch 1568, loss_perior 33.717018127441406, loss_series 33.717018127441406
epoch 1569, loss_perior 33.732032775878906, loss_series 33.732032775878906
epoch 1570, loss_perior 33.70915222167969, loss_series 33.70915222167969
epoch 1571, loss_perior 33.71204376220703, loss_series 33.71204376220703
epoch 1572, loss_perior 33.7376708984375, loss_series 33.7376708984375
epoch 1573, loss_perior 33.71577835083008, loss_series 33.71577835083008
epoch 1574, loss_perior 33.76318359375, loss_series 33.76318359375
epoch 1575, loss_perior 33.73465347290039, loss_series 33.73465347290039
epoch 1576, loss_perior 33.73295211791992, loss_series 33.73295211791992
epoch 1577, loss_perior 33.74047088623047, loss_series 33.74047088623047
epoch 1578, loss_perior 33.720977783203125, loss_series 33.720977783203125
epoch 1579, loss_perior 33.72455596923828, loss_series 33.72455596923828
epoch 1580, loss_perior 33.7370491027832, loss_series 33.7370491027832
epoch 1581, loss_perior 33.75212097167969, loss_series 33.75212097167969
	speed: 0.4015s/iter; left time: 1030.6441s
epoch 1582, loss_perior 33.742332458496094, loss_series 33.742332458496094
epoch 1583, loss_perior 33.70189666748047, loss_series 33.70189666748047
epoch 1584, loss_perior 33.72850799560547, loss_series 33.72850799560547
epoch 1585, loss_perior 33.69237518310547, loss_series 33.69237518310547
epoch 1586, loss_perior 33.700504302978516, loss_series 33.700504302978516
epoch 1587, loss_perior 33.76007843017578, loss_series 33.76007843017578
epoch 1588, loss_perior 33.747833251953125, loss_series 33.747833251953125
epoch 1589, loss_perior 33.713714599609375, loss_series 33.713714599609375
epoch 1590, loss_perior 33.7279167175293, loss_series 33.7279167175293
epoch 1591, loss_perior 33.740821838378906, loss_series 33.740821838378906
epoch 1592, loss_perior 33.779014587402344, loss_series 33.779014587402344
epoch 1593, loss_perior 33.7034912109375, loss_series 33.7034912109375
epoch 1594, loss_perior 33.73661422729492, loss_series 33.73661422729492
epoch 1595, loss_perior 33.71430587768555, loss_series 33.71430587768555
epoch 1596, loss_perior 33.747249603271484, loss_series 33.747249603271484
epoch 1597, loss_perior 33.733116149902344, loss_series 33.733116149902344
epoch 1598, loss_perior 33.73560333251953, loss_series 33.73560333251953
epoch 1599, loss_perior 33.71818161010742, loss_series 33.71818161010742
epoch 1600, loss_perior 33.747901916503906, loss_series 33.747901916503906
epoch 1601, loss_perior 33.76192855834961, loss_series 33.76192855834961
epoch 1602, loss_perior 33.6844482421875, loss_series 33.6844482421875
epoch 1603, loss_perior 33.74806213378906, loss_series 33.74806213378906
epoch 1604, loss_perior 33.775299072265625, loss_series 33.775299072265625
epoch 1605, loss_perior 33.76945495605469, loss_series 33.76945495605469
epoch 1606, loss_perior 33.67880630493164, loss_series 33.67880630493164
epoch 1607, loss_perior 33.769073486328125, loss_series 33.769073486328125
epoch 1608, loss_perior 33.76994323730469, loss_series 33.76994323730469
epoch 1609, loss_perior 33.70256805419922, loss_series 33.70256805419922
epoch 1610, loss_perior 33.708251953125, loss_series 33.708251953125
epoch 1611, loss_perior 33.69908905029297, loss_series 33.69908905029297
epoch 1612, loss_perior 33.71636962890625, loss_series 33.71636962890625
epoch 1613, loss_perior 33.7052001953125, loss_series 33.7052001953125
epoch 1614, loss_perior 33.692317962646484, loss_series 33.692317962646484
epoch 1615, loss_perior 33.67536163330078, loss_series 33.67536163330078
epoch 1616, loss_perior 33.74657440185547, loss_series 33.74657440185547
epoch 1617, loss_perior 33.73016357421875, loss_series 33.73016357421875
epoch 1618, loss_perior 33.74716567993164, loss_series 33.74716567993164
epoch 1619, loss_perior 33.70877456665039, loss_series 33.70877456665039
epoch 1620, loss_perior 33.73563766479492, loss_series 33.73563766479492
epoch 1621, loss_perior 33.69538116455078, loss_series 33.69538116455078
epoch 1622, loss_perior 33.70292663574219, loss_series 33.70292663574219
epoch 1623, loss_perior 33.715885162353516, loss_series 33.715885162353516
epoch 1624, loss_perior 33.768531799316406, loss_series 33.768531799316406
epoch 1625, loss_perior 33.733917236328125, loss_series 33.733917236328125
epoch 1626, loss_perior 33.71247863769531, loss_series 33.71247863769531
epoch 1627, loss_perior 33.74311828613281, loss_series 33.74311828613281
epoch 1628, loss_perior 33.729530334472656, loss_series 33.729530334472656
epoch 1629, loss_perior 33.73481750488281, loss_series 33.73481750488281
epoch 1630, loss_perior 33.74870300292969, loss_series 33.74870300292969
epoch 1631, loss_perior 33.72224426269531, loss_series 33.72224426269531
epoch 1632, loss_perior 33.7198486328125, loss_series 33.7198486328125
epoch 1633, loss_perior 33.721431732177734, loss_series 33.721431732177734
epoch 1634, loss_perior 33.78005599975586, loss_series 33.78005599975586
epoch 1635, loss_perior 33.67795944213867, loss_series 33.67795944213867
epoch 1636, loss_perior 33.72171401977539, loss_series 33.72171401977539
epoch 1637, loss_perior 33.723846435546875, loss_series 33.723846435546875
epoch 1638, loss_perior 33.72499084472656, loss_series 33.72499084472656
epoch 1639, loss_perior 33.74146270751953, loss_series 33.74146270751953
epoch 1640, loss_perior 33.690818786621094, loss_series 33.690818786621094
epoch 1641, loss_perior 33.712886810302734, loss_series 33.712886810302734
epoch 1642, loss_perior 33.762855529785156, loss_series 33.762855529785156
epoch 1643, loss_perior 33.74208068847656, loss_series 33.74208068847656
epoch 1644, loss_perior 33.737571716308594, loss_series 33.737571716308594
epoch 1645, loss_perior 33.67093276977539, loss_series 33.67093276977539
epoch 1646, loss_perior 33.728721618652344, loss_series 33.728721618652344
epoch 1647, loss_perior 33.734130859375, loss_series 33.734130859375
epoch 1648, loss_perior 33.733028411865234, loss_series 33.733028411865234
epoch 1649, loss_perior 33.72465515136719, loss_series 33.72465515136719
epoch 1650, loss_perior 33.70928192138672, loss_series 33.70928192138672
epoch 1651, loss_perior 33.67914581298828, loss_series 33.67914581298828
epoch 1652, loss_perior 33.725616455078125, loss_series 33.725616455078125
epoch 1653, loss_perior 33.74494171142578, loss_series 33.74494171142578
epoch 1654, loss_perior 33.675743103027344, loss_series 33.675743103027344
epoch 1655, loss_perior 33.7461051940918, loss_series 33.7461051940918
epoch 1656, loss_perior 33.73486328125, loss_series 33.73486328125
epoch 1657, loss_perior 33.712066650390625, loss_series 33.712066650390625
epoch 1658, loss_perior 33.69657897949219, loss_series 33.69657897949219
epoch 1659, loss_perior 33.69922637939453, loss_series 33.69922637939453
epoch 1660, loss_perior 33.71757507324219, loss_series 33.71757507324219
epoch 1661, loss_perior 33.67817687988281, loss_series 33.67817687988281
epoch 1662, loss_perior 33.730281829833984, loss_series 33.730281829833984
epoch 1663, loss_perior 33.689178466796875, loss_series 33.689178466796875
epoch 1664, loss_perior 33.74602127075195, loss_series 33.74602127075195
epoch 1665, loss_perior 33.75627899169922, loss_series 33.75627899169922
epoch 1666, loss_perior 33.705238342285156, loss_series 33.705238342285156
epoch 1667, loss_perior 33.736812591552734, loss_series 33.736812591552734
epoch 1668, loss_perior 33.73500061035156, loss_series 33.73500061035156
epoch 1669, loss_perior 33.712032318115234, loss_series 33.712032318115234
epoch 1670, loss_perior 33.71614074707031, loss_series 33.71614074707031
epoch 1671, loss_perior 33.68313980102539, loss_series 33.68313980102539
epoch 1672, loss_perior 33.73968505859375, loss_series 33.73968505859375
epoch 1673, loss_perior 33.69236755371094, loss_series 33.69236755371094
epoch 1674, loss_perior 33.67212677001953, loss_series 33.67212677001953
epoch 1675, loss_perior 33.70550537109375, loss_series 33.70550537109375
epoch 1676, loss_perior 33.694541931152344, loss_series 33.694541931152344
epoch 1677, loss_perior 33.68233108520508, loss_series 33.68233108520508
epoch 1678, loss_perior 33.69945526123047, loss_series 33.69945526123047
epoch 1679, loss_perior 33.739566802978516, loss_series 33.739566802978516
epoch 1680, loss_perior 33.7293701171875, loss_series 33.7293701171875
epoch 1681, loss_perior 33.77178192138672, loss_series 33.77178192138672
	speed: 0.4017s/iter; left time: 991.0577s
epoch 1682, loss_perior 33.72103500366211, loss_series 33.72103500366211
epoch 1683, loss_perior 33.73382568359375, loss_series 33.73382568359375
epoch 1684, loss_perior 33.730567932128906, loss_series 33.730567932128906
epoch 1685, loss_perior 33.73922348022461, loss_series 33.73922348022461
epoch 1686, loss_perior 33.78166198730469, loss_series 33.78166198730469
epoch 1687, loss_perior 33.70747375488281, loss_series 33.70747375488281
epoch 1688, loss_perior 33.69947814941406, loss_series 33.69947814941406
epoch 1689, loss_perior 33.75094223022461, loss_series 33.75094223022461
epoch 1690, loss_perior 33.70060348510742, loss_series 33.70060348510742
epoch 1691, loss_perior 33.73404312133789, loss_series 33.73404312133789
epoch 1692, loss_perior 33.73651123046875, loss_series 33.73651123046875
epoch 1693, loss_perior 33.72623825073242, loss_series 33.72623825073242
epoch 1694, loss_perior 33.776771545410156, loss_series 33.776771545410156
epoch 1695, loss_perior 33.70270538330078, loss_series 33.70270538330078
epoch 1696, loss_perior 33.69944763183594, loss_series 33.69944763183594
epoch 1697, loss_perior 33.71449279785156, loss_series 33.71449279785156
epoch 1698, loss_perior 33.703285217285156, loss_series 33.703285217285156
epoch 1699, loss_perior 33.732139587402344, loss_series 33.732139587402344
epoch 1700, loss_perior 33.70693588256836, loss_series 33.70693588256836
epoch 1701, loss_perior 33.744476318359375, loss_series 33.744476318359375
epoch 1702, loss_perior 33.70708465576172, loss_series 33.70708465576172
epoch 1703, loss_perior 33.70800018310547, loss_series 33.70800018310547
epoch 1704, loss_perior 33.709869384765625, loss_series 33.709869384765625
epoch 1705, loss_perior 33.678863525390625, loss_series 33.678863525390625
epoch 1706, loss_perior 33.723106384277344, loss_series 33.723106384277344
epoch 1707, loss_perior 33.73176574707031, loss_series 33.73176574707031
epoch 1708, loss_perior 33.72578430175781, loss_series 33.72578430175781
epoch 1709, loss_perior 33.73344039916992, loss_series 33.73344039916992
epoch 1710, loss_perior 33.72842025756836, loss_series 33.72842025756836
epoch 1711, loss_perior 33.740474700927734, loss_series 33.740474700927734
epoch 1712, loss_perior 33.73914337158203, loss_series 33.73914337158203
epoch 1713, loss_perior 33.75034713745117, loss_series 33.75034713745117
epoch 1714, loss_perior 33.73899841308594, loss_series 33.73899841308594
epoch 1715, loss_perior 33.759986877441406, loss_series 33.759986877441406
epoch 1716, loss_perior 33.72178649902344, loss_series 33.72178649902344
epoch 1717, loss_perior 33.73638916015625, loss_series 33.73638916015625
epoch 1718, loss_perior 33.708133697509766, loss_series 33.708133697509766
epoch 1719, loss_perior 33.67145919799805, loss_series 33.67145919799805
epoch 1720, loss_perior 33.71742248535156, loss_series 33.71742248535156
epoch 1721, loss_perior 33.77262878417969, loss_series 33.77262878417969
epoch 1722, loss_perior 33.73431396484375, loss_series 33.73431396484375
epoch 1723, loss_perior 33.69889450073242, loss_series 33.69889450073242
epoch 1724, loss_perior 33.718528747558594, loss_series 33.718528747558594
epoch 1725, loss_perior 33.71293640136719, loss_series 33.71293640136719
epoch 1726, loss_perior 33.73339080810547, loss_series 33.73339080810547
epoch 1727, loss_perior 33.71061706542969, loss_series 33.71061706542969
epoch 1728, loss_perior 33.705867767333984, loss_series 33.705867767333984
epoch 1729, loss_perior 33.71812438964844, loss_series 33.71812438964844
epoch 1730, loss_perior 33.708099365234375, loss_series 33.708099365234375
epoch 1731, loss_perior 33.698455810546875, loss_series 33.698455810546875
epoch 1732, loss_perior 33.66682815551758, loss_series 33.66682815551758
epoch 1733, loss_perior 33.68558120727539, loss_series 33.68558120727539
epoch 1734, loss_perior 33.75675964355469, loss_series 33.75675964355469
epoch 1735, loss_perior 33.644287109375, loss_series 33.644287109375
epoch 1736, loss_perior 33.69065475463867, loss_series 33.69065475463867
epoch 1737, loss_perior 33.69268035888672, loss_series 33.69268035888672
epoch 1738, loss_perior 33.677425384521484, loss_series 33.677425384521484
epoch 1739, loss_perior 33.70397186279297, loss_series 33.70397186279297
epoch 1740, loss_perior 33.705833435058594, loss_series 33.705833435058594
epoch 1741, loss_perior 33.741615295410156, loss_series 33.741615295410156
epoch 1742, loss_perior 33.71929931640625, loss_series 33.71929931640625
epoch 1743, loss_perior 33.74563217163086, loss_series 33.74563217163086
epoch 1744, loss_perior 33.682498931884766, loss_series 33.682498931884766
epoch 1745, loss_perior 33.73830032348633, loss_series 33.73830032348633
epoch 1746, loss_perior 33.71458435058594, loss_series 33.71458435058594
epoch 1747, loss_perior 33.74699020385742, loss_series 33.74699020385742
epoch 1748, loss_perior 33.7611198425293, loss_series 33.7611198425293
epoch 1749, loss_perior 33.79054641723633, loss_series 33.79054641723633
epoch 1750, loss_perior 33.704124450683594, loss_series 33.704124450683594
epoch 1751, loss_perior 33.76453399658203, loss_series 33.76453399658203
epoch 1752, loss_perior 33.71725845336914, loss_series 33.71725845336914
epoch 1753, loss_perior 33.750946044921875, loss_series 33.750946044921875
epoch 1754, loss_perior 33.748207092285156, loss_series 33.748207092285156
epoch 1755, loss_perior 33.76289749145508, loss_series 33.76289749145508
epoch 1756, loss_perior 33.72105407714844, loss_series 33.72105407714844
epoch 1757, loss_perior 33.68085861206055, loss_series 33.68085861206055
epoch 1758, loss_perior 33.72370147705078, loss_series 33.72370147705078
epoch 1759, loss_perior 33.70597839355469, loss_series 33.70597839355469
epoch 1760, loss_perior 33.68352508544922, loss_series 33.68352508544922
epoch 1761, loss_perior 33.73289489746094, loss_series 33.73289489746094
epoch 1762, loss_perior 33.71757888793945, loss_series 33.71757888793945
epoch 1763, loss_perior 33.74647903442383, loss_series 33.74647903442383
epoch 1764, loss_perior 33.716148376464844, loss_series 33.716148376464844
epoch 1765, loss_perior 33.732444763183594, loss_series 33.732444763183594
epoch 1766, loss_perior 33.727996826171875, loss_series 33.727996826171875
epoch 1767, loss_perior 33.72466278076172, loss_series 33.72466278076172
epoch 1768, loss_perior 33.73444747924805, loss_series 33.73444747924805
epoch 1769, loss_perior 33.705162048339844, loss_series 33.705162048339844
epoch 1770, loss_perior 33.667640686035156, loss_series 33.667640686035156
epoch 1771, loss_perior 33.73065948486328, loss_series 33.73065948486328
epoch 1772, loss_perior 33.67034912109375, loss_series 33.67034912109375
epoch 1773, loss_perior 33.6798095703125, loss_series 33.6798095703125
epoch 1774, loss_perior 33.66920471191406, loss_series 33.66920471191406
epoch 1775, loss_perior 33.72999572753906, loss_series 33.72999572753906
epoch 1776, loss_perior 33.746803283691406, loss_series 33.746803283691406
epoch 1777, loss_perior 33.75785446166992, loss_series 33.75785446166992
epoch 1778, loss_perior 33.72626495361328, loss_series 33.72626495361328
epoch 1779, loss_perior 33.751609802246094, loss_series 33.751609802246094
epoch 1780, loss_perior 33.70453643798828, loss_series 33.70453643798828
epoch 1781, loss_perior 33.74414825439453, loss_series 33.74414825439453
	speed: 0.4018s/iter; left time: 951.1622s
epoch 1782, loss_perior 33.72980499267578, loss_series 33.72980499267578
epoch 1783, loss_perior 33.70134353637695, loss_series 33.70134353637695
epoch 1784, loss_perior 33.715904235839844, loss_series 33.715904235839844
epoch 1785, loss_perior 33.70661163330078, loss_series 33.70661163330078
epoch 1786, loss_perior 33.66965866088867, loss_series 33.66965866088867
epoch 1787, loss_perior 33.67397689819336, loss_series 33.67397689819336
epoch 1788, loss_perior 33.776710510253906, loss_series 33.776710510253906
epoch 1789, loss_perior 33.765052795410156, loss_series 33.765052795410156
epoch 1790, loss_perior 33.714229583740234, loss_series 33.714229583740234
epoch 1791, loss_perior 33.7489013671875, loss_series 33.7489013671875
epoch 1792, loss_perior 33.73420333862305, loss_series 33.73420333862305
epoch 1793, loss_perior 33.7143669128418, loss_series 33.7143669128418
epoch 1794, loss_perior 33.71377182006836, loss_series 33.71377182006836
epoch 1795, loss_perior 33.69532012939453, loss_series 33.69532012939453
epoch 1796, loss_perior 33.701026916503906, loss_series 33.701026916503906
epoch 1797, loss_perior 33.68595504760742, loss_series 33.68595504760742
epoch 1798, loss_perior 33.750999450683594, loss_series 33.750999450683594
epoch 1799, loss_perior 33.71342086791992, loss_series 33.71342086791992
epoch 1800, loss_perior 33.72247314453125, loss_series 33.72247314453125
epoch 1801, loss_perior 33.70317840576172, loss_series 33.70317840576172
epoch 1802, loss_perior 33.677146911621094, loss_series 33.677146911621094
epoch 1803, loss_perior 33.700408935546875, loss_series 33.700408935546875
epoch 1804, loss_perior 33.697181701660156, loss_series 33.697181701660156
epoch 1805, loss_perior 33.693050384521484, loss_series 33.693050384521484
epoch 1806, loss_perior 33.73084259033203, loss_series 33.73084259033203
epoch 1807, loss_perior 33.78340148925781, loss_series 33.78340148925781
epoch 1808, loss_perior 33.699954986572266, loss_series 33.699954986572266
epoch 1809, loss_perior 33.741607666015625, loss_series 33.741607666015625
epoch 1810, loss_perior 33.743499755859375, loss_series 33.743499755859375
epoch 1811, loss_perior 33.665130615234375, loss_series 33.665130615234375
epoch 1812, loss_perior 33.70787811279297, loss_series 33.70787811279297
epoch 1813, loss_perior 33.69539260864258, loss_series 33.69539260864258
epoch 1814, loss_perior 33.69005584716797, loss_series 33.69005584716797
epoch 1815, loss_perior 33.72064208984375, loss_series 33.72064208984375
epoch 1816, loss_perior 33.7076530456543, loss_series 33.7076530456543
epoch 1817, loss_perior 33.718074798583984, loss_series 33.718074798583984
epoch 1818, loss_perior 33.71503448486328, loss_series 33.71503448486328
epoch 1819, loss_perior 33.77661895751953, loss_series 33.77661895751953
epoch 1820, loss_perior 33.69656753540039, loss_series 33.69656753540039
epoch 1821, loss_perior 33.71889877319336, loss_series 33.71889877319336
epoch 1822, loss_perior 33.75975036621094, loss_series 33.75975036621094
epoch 1823, loss_perior 33.68891143798828, loss_series 33.68891143798828
epoch 1824, loss_perior 33.685508728027344, loss_series 33.685508728027344
epoch 1825, loss_perior 33.70268630981445, loss_series 33.70268630981445
epoch 1826, loss_perior 33.725425720214844, loss_series 33.725425720214844
epoch 1827, loss_perior 33.714786529541016, loss_series 33.714786529541016
epoch 1828, loss_perior 33.736610412597656, loss_series 33.736610412597656
epoch 1829, loss_perior 33.710227966308594, loss_series 33.710227966308594
epoch 1830, loss_perior 33.76059341430664, loss_series 33.76059341430664
epoch 1831, loss_perior 33.691741943359375, loss_series 33.691741943359375
epoch 1832, loss_perior 33.73240661621094, loss_series 33.73240661621094
epoch 1833, loss_perior 33.71959686279297, loss_series 33.71959686279297
epoch 1834, loss_perior 33.744720458984375, loss_series 33.744720458984375
epoch 1835, loss_perior 33.714027404785156, loss_series 33.714027404785156
epoch 1836, loss_perior 33.691993713378906, loss_series 33.691993713378906
epoch 1837, loss_perior 33.729881286621094, loss_series 33.729881286621094
epoch 1838, loss_perior 33.72148132324219, loss_series 33.72148132324219
epoch 1839, loss_perior 33.74121856689453, loss_series 33.74121856689453
epoch 1840, loss_perior 33.70212173461914, loss_series 33.70212173461914
epoch 1841, loss_perior 33.69367218017578, loss_series 33.69367218017578
epoch 1842, loss_perior 33.726402282714844, loss_series 33.726402282714844
epoch 1843, loss_perior 33.73833465576172, loss_series 33.73833465576172
epoch 1844, loss_perior 33.70692825317383, loss_series 33.70692825317383
epoch 1845, loss_perior 33.729148864746094, loss_series 33.729148864746094
epoch 1846, loss_perior 33.72654724121094, loss_series 33.72654724121094
epoch 1847, loss_perior 33.70606231689453, loss_series 33.70606231689453
epoch 1848, loss_perior 33.67399978637695, loss_series 33.67399978637695
epoch 1849, loss_perior 33.74766540527344, loss_series 33.74766540527344
epoch 1850, loss_perior 33.749786376953125, loss_series 33.749786376953125
epoch 1851, loss_perior 33.68681335449219, loss_series 33.68681335449219
epoch 1852, loss_perior 33.70957946777344, loss_series 33.70957946777344
epoch 1853, loss_perior 33.709983825683594, loss_series 33.709983825683594
epoch 1854, loss_perior 33.73262023925781, loss_series 33.73262023925781
epoch 1855, loss_perior 33.71916198730469, loss_series 33.71916198730469
epoch 1856, loss_perior 33.710872650146484, loss_series 33.710872650146484
epoch 1857, loss_perior 33.7103271484375, loss_series 33.7103271484375
epoch 1858, loss_perior 33.708534240722656, loss_series 33.708534240722656
epoch 1859, loss_perior 33.701866149902344, loss_series 33.701866149902344
epoch 1860, loss_perior 33.770442962646484, loss_series 33.770442962646484
epoch 1861, loss_perior 33.733741760253906, loss_series 33.733741760253906
epoch 1862, loss_perior 33.72566604614258, loss_series 33.72566604614258
epoch 1863, loss_perior 33.68785858154297, loss_series 33.68785858154297
epoch 1864, loss_perior 33.71416091918945, loss_series 33.71416091918945
epoch 1865, loss_perior 33.67115783691406, loss_series 33.67115783691406
epoch 1866, loss_perior 33.72943878173828, loss_series 33.72943878173828
epoch 1867, loss_perior 33.69355773925781, loss_series 33.69355773925781
epoch 1868, loss_perior 33.702972412109375, loss_series 33.702972412109375
epoch 1869, loss_perior 33.745689392089844, loss_series 33.745689392089844
epoch 1870, loss_perior 33.701515197753906, loss_series 33.701515197753906
epoch 1871, loss_perior 33.73416519165039, loss_series 33.73416519165039
epoch 1872, loss_perior 33.70053482055664, loss_series 33.70053482055664
epoch 1873, loss_perior 33.721412658691406, loss_series 33.721412658691406
epoch 1874, loss_perior 33.74169158935547, loss_series 33.74169158935547
epoch 1875, loss_perior 33.7017707824707, loss_series 33.7017707824707
epoch 1876, loss_perior 33.69218444824219, loss_series 33.69218444824219
epoch 1877, loss_perior 33.703887939453125, loss_series 33.703887939453125
epoch 1878, loss_perior 33.71574783325195, loss_series 33.71574783325195
epoch 1879, loss_perior 33.70481872558594, loss_series 33.70481872558594
epoch 1880, loss_perior 33.68137741088867, loss_series 33.68137741088867
epoch 1881, loss_perior 33.694114685058594, loss_series 33.694114685058594
	speed: 0.4018s/iter; left time: 910.8121s
epoch 1882, loss_perior 33.735347747802734, loss_series 33.735347747802734
epoch 1883, loss_perior 33.71088790893555, loss_series 33.71088790893555
epoch 1884, loss_perior 33.7213020324707, loss_series 33.7213020324707
epoch 1885, loss_perior 33.77980422973633, loss_series 33.77980422973633
epoch 1886, loss_perior 33.714019775390625, loss_series 33.714019775390625
epoch 1887, loss_perior 33.716575622558594, loss_series 33.716575622558594
epoch 1888, loss_perior 33.70257568359375, loss_series 33.70257568359375
epoch 1889, loss_perior 33.6909294128418, loss_series 33.6909294128418
epoch 1890, loss_perior 33.67071533203125, loss_series 33.67071533203125
epoch 1891, loss_perior 33.68040466308594, loss_series 33.68040466308594
epoch 1892, loss_perior 33.72553634643555, loss_series 33.72553634643555
epoch 1893, loss_perior 33.71025085449219, loss_series 33.71025085449219
epoch 1894, loss_perior 33.6802978515625, loss_series 33.6802978515625
epoch 1895, loss_perior 33.73161315917969, loss_series 33.73161315917969
epoch 1896, loss_perior 33.71628189086914, loss_series 33.71628189086914
epoch 1897, loss_perior 33.70890426635742, loss_series 33.70890426635742
epoch 1898, loss_perior 33.71857452392578, loss_series 33.71857452392578
epoch 1899, loss_perior 33.73435974121094, loss_series 33.73435974121094
epoch 1900, loss_perior 33.75627136230469, loss_series 33.75627136230469
epoch 1901, loss_perior 33.76206970214844, loss_series 33.76206970214844
epoch 1902, loss_perior 33.653133392333984, loss_series 33.653133392333984
epoch 1903, loss_perior 33.68865966796875, loss_series 33.68865966796875
epoch 1904, loss_perior 33.67892074584961, loss_series 33.67892074584961
epoch 1905, loss_perior 33.71692657470703, loss_series 33.71692657470703
epoch 1906, loss_perior 33.696773529052734, loss_series 33.696773529052734
epoch 1907, loss_perior 33.72063446044922, loss_series 33.72063446044922
epoch 1908, loss_perior 33.75462341308594, loss_series 33.75462341308594
epoch 1909, loss_perior 33.694679260253906, loss_series 33.694679260253906
epoch 1910, loss_perior 33.7221794128418, loss_series 33.7221794128418
epoch 1911, loss_perior 33.72465896606445, loss_series 33.72465896606445
epoch 1912, loss_perior 33.712581634521484, loss_series 33.712581634521484
epoch 1913, loss_perior 33.73082733154297, loss_series 33.73082733154297
epoch 1914, loss_perior 33.77107238769531, loss_series 33.77107238769531
epoch 1915, loss_perior 33.70793151855469, loss_series 33.70793151855469
epoch 1916, loss_perior 33.71337890625, loss_series 33.71337890625
epoch 1917, loss_perior 33.73974609375, loss_series 33.73974609375
epoch 1918, loss_perior 33.7260627746582, loss_series 33.7260627746582
epoch 1919, loss_perior 33.67783737182617, loss_series 33.67783737182617
epoch 1920, loss_perior 33.744232177734375, loss_series 33.744232177734375
epoch 1921, loss_perior 33.69369125366211, loss_series 33.69369125366211
epoch 1922, loss_perior 33.66802978515625, loss_series 33.66802978515625
epoch 1923, loss_perior 33.70060729980469, loss_series 33.70060729980469
epoch 1924, loss_perior 33.68940734863281, loss_series 33.68940734863281
epoch 1925, loss_perior 33.67499923706055, loss_series 33.67499923706055
epoch 1926, loss_perior 33.710933685302734, loss_series 33.710933685302734
epoch 1927, loss_perior 33.75410842895508, loss_series 33.75410842895508
epoch 1928, loss_perior 33.701934814453125, loss_series 33.701934814453125
epoch 1929, loss_perior 33.7169189453125, loss_series 33.7169189453125
epoch 1930, loss_perior 33.665157318115234, loss_series 33.665157318115234
epoch 1931, loss_perior 33.708492279052734, loss_series 33.708492279052734
epoch 1932, loss_perior 33.71436309814453, loss_series 33.71436309814453
epoch 1933, loss_perior 33.72308349609375, loss_series 33.72308349609375
epoch 1934, loss_perior 33.68366622924805, loss_series 33.68366622924805
epoch 1935, loss_perior 33.672115325927734, loss_series 33.672115325927734
epoch 1936, loss_perior 33.67739486694336, loss_series 33.67739486694336
epoch 1937, loss_perior 33.72392272949219, loss_series 33.72392272949219
epoch 1938, loss_perior 33.7314453125, loss_series 33.7314453125
epoch 1939, loss_perior 33.68842697143555, loss_series 33.68842697143555
epoch 1940, loss_perior 33.7559814453125, loss_series 33.7559814453125
epoch 1941, loss_perior 33.73221206665039, loss_series 33.73221206665039
epoch 1942, loss_perior 33.71360778808594, loss_series 33.71360778808594
epoch 1943, loss_perior 33.73215103149414, loss_series 33.73215103149414
epoch 1944, loss_perior 33.71352767944336, loss_series 33.71352767944336
epoch 1945, loss_perior 33.72997283935547, loss_series 33.72997283935547
epoch 1946, loss_perior 33.70295333862305, loss_series 33.70295333862305
epoch 1947, loss_perior 33.67366027832031, loss_series 33.67366027832031
epoch 1948, loss_perior 33.708885192871094, loss_series 33.708885192871094
epoch 1949, loss_perior 33.72362518310547, loss_series 33.72362518310547
epoch 1950, loss_perior 33.72712326049805, loss_series 33.72712326049805
epoch 1951, loss_perior 33.718467712402344, loss_series 33.718467712402344
epoch 1952, loss_perior 33.74451446533203, loss_series 33.74451446533203
epoch 1953, loss_perior 33.73366928100586, loss_series 33.73366928100586
epoch 1954, loss_perior 33.76226043701172, loss_series 33.76226043701172
epoch 1955, loss_perior 33.71715545654297, loss_series 33.71715545654297
epoch 1956, loss_perior 33.716156005859375, loss_series 33.716156005859375
epoch 1957, loss_perior 33.707435607910156, loss_series 33.707435607910156
epoch 1958, loss_perior 33.704071044921875, loss_series 33.704071044921875
epoch 1959, loss_perior 33.68641662597656, loss_series 33.68641662597656
epoch 1960, loss_perior 33.67295837402344, loss_series 33.67295837402344
epoch 1961, loss_perior 33.742454528808594, loss_series 33.742454528808594
epoch 1962, loss_perior 33.68645477294922, loss_series 33.68645477294922
epoch 1963, loss_perior 33.726314544677734, loss_series 33.726314544677734
epoch 1964, loss_perior 33.703529357910156, loss_series 33.703529357910156
epoch 1965, loss_perior 33.700782775878906, loss_series 33.700782775878906
epoch 1966, loss_perior 33.686763763427734, loss_series 33.686763763427734
epoch 1967, loss_perior 33.72830581665039, loss_series 33.72830581665039
epoch 1968, loss_perior 33.71593475341797, loss_series 33.71593475341797
epoch 1969, loss_perior 33.67908477783203, loss_series 33.67908477783203
epoch 1970, loss_perior 33.70608139038086, loss_series 33.70608139038086
epoch 1971, loss_perior 33.70800018310547, loss_series 33.70800018310547
epoch 1972, loss_perior 33.695892333984375, loss_series 33.695892333984375
epoch 1973, loss_perior 33.682342529296875, loss_series 33.682342529296875
epoch 1974, loss_perior 33.71308898925781, loss_series 33.71308898925781
epoch 1975, loss_perior 33.704925537109375, loss_series 33.704925537109375
epoch 1976, loss_perior 33.684844970703125, loss_series 33.684844970703125
epoch 1977, loss_perior 33.73368835449219, loss_series 33.73368835449219
epoch 1978, loss_perior 33.744361877441406, loss_series 33.744361877441406
epoch 1979, loss_perior 33.68824768066406, loss_series 33.68824768066406
epoch 1980, loss_perior 33.69200897216797, loss_series 33.69200897216797
epoch 1981, loss_perior 33.711021423339844, loss_series 33.711021423339844
	speed: 0.4018s/iter; left time: 870.6600s
epoch 1982, loss_perior 33.665374755859375, loss_series 33.665374755859375
epoch 1983, loss_perior 33.75615310668945, loss_series 33.75615310668945
epoch 1984, loss_perior 33.687652587890625, loss_series 33.687652587890625
epoch 1985, loss_perior 33.68332290649414, loss_series 33.68332290649414
epoch 1986, loss_perior 33.71028518676758, loss_series 33.71028518676758
epoch 1987, loss_perior 33.70960998535156, loss_series 33.70960998535156
epoch 1988, loss_perior 33.7026481628418, loss_series 33.7026481628418
epoch 1989, loss_perior 33.70587158203125, loss_series 33.70587158203125
epoch 1990, loss_perior 33.69630432128906, loss_series 33.69630432128906
epoch 1991, loss_perior 33.698822021484375, loss_series 33.698822021484375
epoch 1992, loss_perior 33.712493896484375, loss_series 33.712493896484375
epoch 1993, loss_perior 33.71247100830078, loss_series 33.71247100830078
epoch 1994, loss_perior 33.743221282958984, loss_series 33.743221282958984
epoch 1995, loss_perior 33.7395133972168, loss_series 33.7395133972168
epoch 1996, loss_perior 33.7021484375, loss_series 33.7021484375
epoch 1997, loss_perior 33.76311492919922, loss_series 33.76311492919922
epoch 1998, loss_perior 33.754913330078125, loss_series 33.754913330078125
epoch 1999, loss_perior 33.74180221557617, loss_series 33.74180221557617
epoch 2000, loss_perior 33.673011779785156, loss_series 33.673011779785156
epoch 2001, loss_perior 33.76032638549805, loss_series 33.76032638549805
epoch 2002, loss_perior 33.69378662109375, loss_series 33.69378662109375
epoch 2003, loss_perior 33.7297477722168, loss_series 33.7297477722168
epoch 2004, loss_perior 33.697303771972656, loss_series 33.697303771972656
epoch 2005, loss_perior 33.70351028442383, loss_series 33.70351028442383
epoch 2006, loss_perior 33.74294662475586, loss_series 33.74294662475586
epoch 2007, loss_perior 33.69921112060547, loss_series 33.69921112060547
epoch 2008, loss_perior 33.72294616699219, loss_series 33.72294616699219
epoch 2009, loss_perior 33.69694137573242, loss_series 33.69694137573242
epoch 2010, loss_perior 33.74430465698242, loss_series 33.74430465698242
epoch 2011, loss_perior 33.720497131347656, loss_series 33.720497131347656
epoch 2012, loss_perior 33.70307159423828, loss_series 33.70307159423828
epoch 2013, loss_perior 33.698280334472656, loss_series 33.698280334472656
epoch 2014, loss_perior 33.7011604309082, loss_series 33.7011604309082
epoch 2015, loss_perior 33.7128791809082, loss_series 33.7128791809082
epoch 2016, loss_perior 33.721351623535156, loss_series 33.721351623535156
epoch 2017, loss_perior 33.69032287597656, loss_series 33.69032287597656
epoch 2018, loss_perior 33.68503952026367, loss_series 33.68503952026367
epoch 2019, loss_perior 33.73262023925781, loss_series 33.73262023925781
epoch 2020, loss_perior 33.67793273925781, loss_series 33.67793273925781
epoch 2021, loss_perior 33.707420349121094, loss_series 33.707420349121094
epoch 2022, loss_perior 33.70252227783203, loss_series 33.70252227783203
epoch 2023, loss_perior 33.69765853881836, loss_series 33.69765853881836
epoch 2024, loss_perior 33.72461700439453, loss_series 33.72461700439453
epoch 2025, loss_perior 33.68265151977539, loss_series 33.68265151977539
epoch 2026, loss_perior 33.67194366455078, loss_series 33.67194366455078
epoch 2027, loss_perior 33.678855895996094, loss_series 33.678855895996094
epoch 2028, loss_perior 33.709205627441406, loss_series 33.709205627441406
epoch 2029, loss_perior 33.694862365722656, loss_series 33.694862365722656
epoch 2030, loss_perior 33.66250228881836, loss_series 33.66250228881836
epoch 2031, loss_perior 33.70790481567383, loss_series 33.70790481567383
epoch 2032, loss_perior 33.737892150878906, loss_series 33.737892150878906
epoch 2033, loss_perior 33.68898010253906, loss_series 33.68898010253906
epoch 2034, loss_perior 33.70815658569336, loss_series 33.70815658569336
epoch 2035, loss_perior 33.684288024902344, loss_series 33.684288024902344
epoch 2036, loss_perior 33.710693359375, loss_series 33.710693359375
epoch 2037, loss_perior 33.73499298095703, loss_series 33.73499298095703
epoch 2038, loss_perior 33.64852523803711, loss_series 33.64852523803711
epoch 2039, loss_perior 33.7125244140625, loss_series 33.7125244140625
epoch 2040, loss_perior 33.730018615722656, loss_series 33.730018615722656
epoch 2041, loss_perior 33.673240661621094, loss_series 33.673240661621094
epoch 2042, loss_perior 33.69684982299805, loss_series 33.69684982299805
epoch 2043, loss_perior 33.72394943237305, loss_series 33.72394943237305
epoch 2044, loss_perior 33.69598388671875, loss_series 33.69598388671875
epoch 2045, loss_perior 33.740928649902344, loss_series 33.740928649902344
epoch 2046, loss_perior 33.705352783203125, loss_series 33.705352783203125
epoch 2047, loss_perior 33.712974548339844, loss_series 33.712974548339844
epoch 2048, loss_perior 33.74712371826172, loss_series 33.74712371826172
epoch 2049, loss_perior 33.65739440917969, loss_series 33.65739440917969
epoch 2050, loss_perior 33.7534065246582, loss_series 33.7534065246582
epoch 2051, loss_perior 33.689491271972656, loss_series 33.689491271972656
epoch 2052, loss_perior 33.674041748046875, loss_series 33.674041748046875
epoch 2053, loss_perior 33.665916442871094, loss_series 33.665916442871094
epoch 2054, loss_perior 33.672908782958984, loss_series 33.672908782958984
epoch 2055, loss_perior 33.685184478759766, loss_series 33.685184478759766
epoch 2056, loss_perior 33.66907501220703, loss_series 33.66907501220703
epoch 2057, loss_perior 33.67143630981445, loss_series 33.67143630981445
epoch 2058, loss_perior 33.70187759399414, loss_series 33.70187759399414
epoch 2059, loss_perior 33.69716262817383, loss_series 33.69716262817383
epoch 2060, loss_perior 33.68644714355469, loss_series 33.68644714355469
epoch 2061, loss_perior 33.70455551147461, loss_series 33.70455551147461
epoch 2062, loss_perior 33.70217514038086, loss_series 33.70217514038086
epoch 2063, loss_perior 33.665130615234375, loss_series 33.665130615234375
epoch 2064, loss_perior 33.74773406982422, loss_series 33.74773406982422
epoch 2065, loss_perior 33.6822395324707, loss_series 33.6822395324707
epoch 2066, loss_perior 33.7348747253418, loss_series 33.7348747253418
epoch 2067, loss_perior 33.67662811279297, loss_series 33.67662811279297
epoch 2068, loss_perior 33.69744873046875, loss_series 33.69744873046875
epoch 2069, loss_perior 33.744232177734375, loss_series 33.744232177734375
epoch 2070, loss_perior 33.692054748535156, loss_series 33.692054748535156
epoch 2071, loss_perior 33.6878662109375, loss_series 33.6878662109375
epoch 2072, loss_perior 33.731597900390625, loss_series 33.731597900390625
epoch 2073, loss_perior 33.69164276123047, loss_series 33.69164276123047
epoch 2074, loss_perior 33.7156982421875, loss_series 33.7156982421875
epoch 2075, loss_perior 33.687068939208984, loss_series 33.687068939208984
epoch 2076, loss_perior 33.74924087524414, loss_series 33.74924087524414
epoch 2077, loss_perior 33.700340270996094, loss_series 33.700340270996094
epoch 2078, loss_perior 33.68075180053711, loss_series 33.68075180053711
epoch 2079, loss_perior 33.7274169921875, loss_series 33.7274169921875
epoch 2080, loss_perior 33.64642333984375, loss_series 33.64642333984375
epoch 2081, loss_perior 33.67737579345703, loss_series 33.67737579345703
	speed: 0.4019s/iter; left time: 830.6519s
epoch 2082, loss_perior 33.744163513183594, loss_series 33.744163513183594
epoch 2083, loss_perior 33.7138671875, loss_series 33.7138671875
epoch 2084, loss_perior 33.690486907958984, loss_series 33.690486907958984
epoch 2085, loss_perior 33.694313049316406, loss_series 33.694313049316406
epoch 2086, loss_perior 33.72673797607422, loss_series 33.72673797607422
epoch 2087, loss_perior 33.70752716064453, loss_series 33.70752716064453
epoch 2088, loss_perior 33.696868896484375, loss_series 33.696868896484375
epoch 2089, loss_perior 33.69492721557617, loss_series 33.69492721557617
epoch 2090, loss_perior 33.73728942871094, loss_series 33.73728942871094
epoch 2091, loss_perior 33.71331787109375, loss_series 33.71331787109375
epoch 2092, loss_perior 33.759307861328125, loss_series 33.759307861328125
epoch 2093, loss_perior 33.70848846435547, loss_series 33.70848846435547
epoch 2094, loss_perior 33.68733215332031, loss_series 33.68733215332031
epoch 2095, loss_perior 33.67611312866211, loss_series 33.67611312866211
epoch 2096, loss_perior 33.670372009277344, loss_series 33.670372009277344
epoch 2097, loss_perior 33.716064453125, loss_series 33.716064453125
epoch 2098, loss_perior 33.760292053222656, loss_series 33.760292053222656
epoch 2099, loss_perior 33.66788864135742, loss_series 33.66788864135742
epoch 2100, loss_perior 33.735137939453125, loss_series 33.735137939453125
epoch 2101, loss_perior 33.72178268432617, loss_series 33.72178268432617
epoch 2102, loss_perior 33.68443298339844, loss_series 33.68443298339844
epoch 2103, loss_perior 33.726104736328125, loss_series 33.726104736328125
epoch 2104, loss_perior 33.66389465332031, loss_series 33.66389465332031
epoch 2105, loss_perior 33.7567138671875, loss_series 33.7567138671875
epoch 2106, loss_perior 33.73999786376953, loss_series 33.73999786376953
epoch 2107, loss_perior 33.6906852722168, loss_series 33.6906852722168
epoch 2108, loss_perior 33.68699645996094, loss_series 33.68699645996094
epoch 2109, loss_perior 33.745941162109375, loss_series 33.745941162109375
epoch 2110, loss_perior 33.68569564819336, loss_series 33.68569564819336
epoch 2111, loss_perior 33.691497802734375, loss_series 33.691497802734375
epoch 2112, loss_perior 33.710567474365234, loss_series 33.710567474365234
epoch 2113, loss_perior 33.715599060058594, loss_series 33.715599060058594
epoch 2114, loss_perior 33.770484924316406, loss_series 33.770484924316406
epoch 2115, loss_perior 33.69633483886719, loss_series 33.69633483886719
epoch 2116, loss_perior 33.66866683959961, loss_series 33.66866683959961
epoch 2117, loss_perior 33.69822692871094, loss_series 33.69822692871094
epoch 2118, loss_perior 33.68397521972656, loss_series 33.68397521972656
epoch 2119, loss_perior 33.67821502685547, loss_series 33.67821502685547
epoch 2120, loss_perior 33.72030258178711, loss_series 33.72030258178711
epoch 2121, loss_perior 33.676513671875, loss_series 33.676513671875
epoch 2122, loss_perior 33.70539093017578, loss_series 33.70539093017578
epoch 2123, loss_perior 33.718353271484375, loss_series 33.718353271484375
epoch 2124, loss_perior 33.71184539794922, loss_series 33.71184539794922
epoch 2125, loss_perior 33.685302734375, loss_series 33.685302734375
epoch 2126, loss_perior 33.706443786621094, loss_series 33.706443786621094
epoch 2127, loss_perior 33.647666931152344, loss_series 33.647666931152344
epoch 2128, loss_perior 33.68560791015625, loss_series 33.68560791015625
epoch 2129, loss_perior 33.69398498535156, loss_series 33.69398498535156
epoch 2130, loss_perior 33.69343566894531, loss_series 33.69343566894531
epoch 2131, loss_perior 33.6867790222168, loss_series 33.6867790222168
epoch 2132, loss_perior 33.7129020690918, loss_series 33.7129020690918
epoch 2133, loss_perior 33.71537780761719, loss_series 33.71537780761719
epoch 2134, loss_perior 33.66933822631836, loss_series 33.66933822631836
epoch 2135, loss_perior 33.708003997802734, loss_series 33.708003997802734
epoch 2136, loss_perior 33.667640686035156, loss_series 33.667640686035156
epoch 2137, loss_perior 33.73333740234375, loss_series 33.73333740234375
epoch 2138, loss_perior 33.712894439697266, loss_series 33.712894439697266
epoch 2139, loss_perior 33.727046966552734, loss_series 33.727046966552734
epoch 2140, loss_perior 33.700016021728516, loss_series 33.700016021728516
epoch 2141, loss_perior 33.70374298095703, loss_series 33.70374298095703
epoch 2142, loss_perior 33.69266891479492, loss_series 33.69266891479492
epoch 2143, loss_perior 33.72248840332031, loss_series 33.72248840332031
epoch 2144, loss_perior 33.68144989013672, loss_series 33.68144989013672
epoch 2145, loss_perior 33.715843200683594, loss_series 33.715843200683594
epoch 2146, loss_perior 33.72666931152344, loss_series 33.72666931152344
epoch 2147, loss_perior 33.70491027832031, loss_series 33.70491027832031
epoch 2148, loss_perior 33.720516204833984, loss_series 33.720516204833984
epoch 2149, loss_perior 33.718868255615234, loss_series 33.718868255615234
epoch 2150, loss_perior 33.69855880737305, loss_series 33.69855880737305
epoch 2151, loss_perior 33.688716888427734, loss_series 33.688716888427734
epoch 2152, loss_perior 33.73740768432617, loss_series 33.73740768432617
epoch 2153, loss_perior 33.70160675048828, loss_series 33.70160675048828
epoch 2154, loss_perior 33.6865119934082, loss_series 33.6865119934082
epoch 2155, loss_perior 33.700862884521484, loss_series 33.700862884521484
epoch 2156, loss_perior 33.721336364746094, loss_series 33.721336364746094
epoch 2157, loss_perior 33.652732849121094, loss_series 33.652732849121094
epoch 2158, loss_perior 33.670928955078125, loss_series 33.670928955078125
epoch 2159, loss_perior 33.68775177001953, loss_series 33.68775177001953
epoch 2160, loss_perior 33.71783447265625, loss_series 33.71783447265625
epoch 2161, loss_perior 33.65196228027344, loss_series 33.65196228027344
epoch 2162, loss_perior 33.705955505371094, loss_series 33.705955505371094
epoch 2163, loss_perior 33.675537109375, loss_series 33.675537109375
epoch 2164, loss_perior 33.68386459350586, loss_series 33.68386459350586
epoch 2165, loss_perior 33.67197799682617, loss_series 33.67197799682617
epoch 2166, loss_perior 33.69154739379883, loss_series 33.69154739379883
epoch 2167, loss_perior 33.70903015136719, loss_series 33.70903015136719
epoch 2168, loss_perior 33.68910217285156, loss_series 33.68910217285156
epoch 2169, loss_perior 33.686004638671875, loss_series 33.686004638671875
epoch 2170, loss_perior 33.68675994873047, loss_series 33.68675994873047
epoch 2171, loss_perior 33.7428092956543, loss_series 33.7428092956543
epoch 2172, loss_perior 33.70122528076172, loss_series 33.70122528076172
epoch 2173, loss_perior 33.75519561767578, loss_series 33.75519561767578
epoch 2174, loss_perior 33.70453643798828, loss_series 33.70453643798828
epoch 2175, loss_perior 33.71611022949219, loss_series 33.71611022949219
epoch 2176, loss_perior 33.73387908935547, loss_series 33.73387908935547
epoch 2177, loss_perior 33.72015380859375, loss_series 33.72015380859375
epoch 2178, loss_perior 33.679443359375, loss_series 33.679443359375
epoch 2179, loss_perior 33.70966339111328, loss_series 33.70966339111328
epoch 2180, loss_perior 33.720462799072266, loss_series 33.720462799072266
epoch 2181, loss_perior 33.669883728027344, loss_series 33.669883728027344
	speed: 0.4018s/iter; left time: 790.3308s
epoch 2182, loss_perior 33.69526672363281, loss_series 33.69526672363281
epoch 2183, loss_perior 33.70703887939453, loss_series 33.70703887939453
epoch 2184, loss_perior 33.69852066040039, loss_series 33.69852066040039
epoch 2185, loss_perior 33.737430572509766, loss_series 33.737430572509766
epoch 2186, loss_perior 33.69476318359375, loss_series 33.69476318359375
epoch 2187, loss_perior 33.69488525390625, loss_series 33.69488525390625
epoch 2188, loss_perior 33.72544860839844, loss_series 33.72544860839844
epoch 2189, loss_perior 33.66957092285156, loss_series 33.66957092285156
epoch 2190, loss_perior 33.67719268798828, loss_series 33.67719268798828
epoch 2191, loss_perior 33.721282958984375, loss_series 33.721282958984375
epoch 2192, loss_perior 33.68194580078125, loss_series 33.68194580078125
epoch 2193, loss_perior 33.732025146484375, loss_series 33.732025146484375
epoch 2194, loss_perior 33.7053337097168, loss_series 33.7053337097168
epoch 2195, loss_perior 33.71693420410156, loss_series 33.71693420410156
epoch 2196, loss_perior 33.73338317871094, loss_series 33.73338317871094
epoch 2197, loss_perior 33.70347595214844, loss_series 33.70347595214844
epoch 2198, loss_perior 33.709251403808594, loss_series 33.709251403808594
epoch 2199, loss_perior 33.692623138427734, loss_series 33.692623138427734
epoch 2200, loss_perior 33.68754577636719, loss_series 33.68754577636719
epoch 2201, loss_perior 33.75008773803711, loss_series 33.75008773803711
epoch 2202, loss_perior 33.7088737487793, loss_series 33.7088737487793
epoch 2203, loss_perior 33.68643569946289, loss_series 33.68643569946289
epoch 2204, loss_perior 33.68987274169922, loss_series 33.68987274169922
epoch 2205, loss_perior 33.67230987548828, loss_series 33.67230987548828
epoch 2206, loss_perior 33.68921661376953, loss_series 33.68921661376953
epoch 2207, loss_perior 33.76594161987305, loss_series 33.76594161987305
epoch 2208, loss_perior 33.68241882324219, loss_series 33.68241882324219
epoch 2209, loss_perior 33.71828842163086, loss_series 33.71828842163086
epoch 2210, loss_perior 33.699554443359375, loss_series 33.699554443359375
epoch 2211, loss_perior 33.65372848510742, loss_series 33.65372848510742
epoch 2212, loss_perior 33.6898193359375, loss_series 33.6898193359375
epoch 2213, loss_perior 33.676387786865234, loss_series 33.676387786865234
epoch 2214, loss_perior 33.730079650878906, loss_series 33.730079650878906
epoch 2215, loss_perior 33.720420837402344, loss_series 33.720420837402344
epoch 2216, loss_perior 33.756256103515625, loss_series 33.756256103515625
epoch 2217, loss_perior 33.64602279663086, loss_series 33.64602279663086
epoch 2218, loss_perior 33.69317626953125, loss_series 33.69317626953125
epoch 2219, loss_perior 33.69805145263672, loss_series 33.69805145263672
epoch 2220, loss_perior 33.717708587646484, loss_series 33.717708587646484
epoch 2221, loss_perior 33.68003845214844, loss_series 33.68003845214844
epoch 2222, loss_perior 33.70350646972656, loss_series 33.70350646972656
epoch 2223, loss_perior 33.69997787475586, loss_series 33.69997787475586
epoch 2224, loss_perior 33.74390411376953, loss_series 33.74390411376953
epoch 2225, loss_perior 33.715457916259766, loss_series 33.715457916259766
epoch 2226, loss_perior 33.66902542114258, loss_series 33.66902542114258
epoch 2227, loss_perior 33.65227508544922, loss_series 33.65227508544922
epoch 2228, loss_perior 33.73003387451172, loss_series 33.73003387451172
epoch 2229, loss_perior 33.718482971191406, loss_series 33.718482971191406
epoch 2230, loss_perior 33.67272186279297, loss_series 33.67272186279297
epoch 2231, loss_perior 33.722042083740234, loss_series 33.722042083740234
epoch 2232, loss_perior 33.672183990478516, loss_series 33.672183990478516
epoch 2233, loss_perior 33.683231353759766, loss_series 33.683231353759766
epoch 2234, loss_perior 33.71937942504883, loss_series 33.71937942504883
epoch 2235, loss_perior 33.71089172363281, loss_series 33.71089172363281
epoch 2236, loss_perior 33.72636413574219, loss_series 33.72636413574219
epoch 2237, loss_perior 33.714210510253906, loss_series 33.714210510253906
epoch 2238, loss_perior 33.723175048828125, loss_series 33.723175048828125
epoch 2239, loss_perior 33.716636657714844, loss_series 33.716636657714844
epoch 2240, loss_perior 33.68938446044922, loss_series 33.68938446044922
epoch 2241, loss_perior 33.744483947753906, loss_series 33.744483947753906
epoch 2242, loss_perior 33.697654724121094, loss_series 33.697654724121094
epoch 2243, loss_perior 33.696353912353516, loss_series 33.696353912353516
epoch 2244, loss_perior 33.704559326171875, loss_series 33.704559326171875
epoch 2245, loss_perior 33.70671081542969, loss_series 33.70671081542969
epoch 2246, loss_perior 33.729000091552734, loss_series 33.729000091552734
epoch 2247, loss_perior 33.71828842163086, loss_series 33.71828842163086
epoch 2248, loss_perior 33.695648193359375, loss_series 33.695648193359375
epoch 2249, loss_perior 33.74589538574219, loss_series 33.74589538574219
epoch 2250, loss_perior 33.70015335083008, loss_series 33.70015335083008
epoch 2251, loss_perior 33.71641540527344, loss_series 33.71641540527344
epoch 2252, loss_perior 33.6470947265625, loss_series 33.6470947265625
epoch 2253, loss_perior 33.65968322753906, loss_series 33.65968322753906
epoch 2254, loss_perior 33.77314758300781, loss_series 33.77314758300781
epoch 2255, loss_perior 33.725399017333984, loss_series 33.725399017333984
epoch 2256, loss_perior 33.69434356689453, loss_series 33.69434356689453
epoch 2257, loss_perior 33.7368278503418, loss_series 33.7368278503418
epoch 2258, loss_perior 33.6960563659668, loss_series 33.6960563659668
epoch 2259, loss_perior 33.702919006347656, loss_series 33.702919006347656
epoch 2260, loss_perior 33.67464828491211, loss_series 33.67464828491211
epoch 2261, loss_perior 33.65900802612305, loss_series 33.65900802612305
epoch 2262, loss_perior 33.63683319091797, loss_series 33.63683319091797
epoch 2263, loss_perior 33.67150115966797, loss_series 33.67150115966797
epoch 2264, loss_perior 33.652809143066406, loss_series 33.652809143066406
epoch 2265, loss_perior 33.705482482910156, loss_series 33.705482482910156
epoch 2266, loss_perior 33.67531967163086, loss_series 33.67531967163086
epoch 2267, loss_perior 33.68794250488281, loss_series 33.68794250488281
epoch 2268, loss_perior 33.66753005981445, loss_series 33.66753005981445
epoch 2269, loss_perior 33.699954986572266, loss_series 33.699954986572266
epoch 2270, loss_perior 33.63733673095703, loss_series 33.63733673095703
epoch 2271, loss_perior 33.701072692871094, loss_series 33.701072692871094
epoch 2272, loss_perior 33.68250274658203, loss_series 33.68250274658203
epoch 2273, loss_perior 33.68789291381836, loss_series 33.68789291381836
epoch 2274, loss_perior 33.696319580078125, loss_series 33.696319580078125
epoch 2275, loss_perior 33.6867561340332, loss_series 33.6867561340332
epoch 2276, loss_perior 33.692787170410156, loss_series 33.692787170410156
epoch 2277, loss_perior 33.725521087646484, loss_series 33.725521087646484
epoch 2278, loss_perior 33.70136260986328, loss_series 33.70136260986328
epoch 2279, loss_perior 33.724998474121094, loss_series 33.724998474121094
epoch 2280, loss_perior 33.75336837768555, loss_series 33.75336837768555
epoch 2281, loss_perior 33.7093620300293, loss_series 33.7093620300293
	speed: 0.4015s/iter; left time: 749.5677s
epoch 2282, loss_perior 33.70656967163086, loss_series 33.70656967163086
epoch 2283, loss_perior 33.703575134277344, loss_series 33.703575134277344
epoch 2284, loss_perior 33.713356018066406, loss_series 33.713356018066406
epoch 2285, loss_perior 33.66101837158203, loss_series 33.66101837158203
epoch 2286, loss_perior 33.66604995727539, loss_series 33.66604995727539
epoch 2287, loss_perior 33.71613311767578, loss_series 33.71613311767578
epoch 2288, loss_perior 33.70045471191406, loss_series 33.70045471191406
epoch 2289, loss_perior 33.72007751464844, loss_series 33.72007751464844
epoch 2290, loss_perior 33.68002700805664, loss_series 33.68002700805664
epoch 2291, loss_perior 33.67657470703125, loss_series 33.67657470703125
epoch 2292, loss_perior 33.690582275390625, loss_series 33.690582275390625
epoch 2293, loss_perior 33.6951789855957, loss_series 33.6951789855957
epoch 2294, loss_perior 33.68513107299805, loss_series 33.68513107299805
epoch 2295, loss_perior 33.69561004638672, loss_series 33.69561004638672
epoch 2296, loss_perior 33.681739807128906, loss_series 33.681739807128906
epoch 2297, loss_perior 33.69358825683594, loss_series 33.69358825683594
epoch 2298, loss_perior 33.68535614013672, loss_series 33.68535614013672
epoch 2299, loss_perior 33.69746398925781, loss_series 33.69746398925781
epoch 2300, loss_perior 33.651702880859375, loss_series 33.651702880859375
epoch 2301, loss_perior 33.72984313964844, loss_series 33.72984313964844
epoch 2302, loss_perior 33.694854736328125, loss_series 33.694854736328125
epoch 2303, loss_perior 33.71809387207031, loss_series 33.71809387207031
epoch 2304, loss_perior 33.70675277709961, loss_series 33.70675277709961
epoch 2305, loss_perior 33.704200744628906, loss_series 33.704200744628906
epoch 2306, loss_perior 33.70301818847656, loss_series 33.70301818847656
epoch 2307, loss_perior 33.69268035888672, loss_series 33.69268035888672
epoch 2308, loss_perior 33.74195861816406, loss_series 33.74195861816406
epoch 2309, loss_perior 33.681724548339844, loss_series 33.681724548339844
epoch 2310, loss_perior 33.717628479003906, loss_series 33.717628479003906
epoch 2311, loss_perior 33.722991943359375, loss_series 33.722991943359375
epoch 2312, loss_perior 33.71502685546875, loss_series 33.71502685546875
epoch 2313, loss_perior 33.66371154785156, loss_series 33.66371154785156
epoch 2314, loss_perior 33.707366943359375, loss_series 33.707366943359375
epoch 2315, loss_perior 33.71226119995117, loss_series 33.71226119995117
epoch 2316, loss_perior 33.707645416259766, loss_series 33.707645416259766
epoch 2317, loss_perior 33.70655822753906, loss_series 33.70655822753906
epoch 2318, loss_perior 33.698211669921875, loss_series 33.698211669921875
epoch 2319, loss_perior 33.70317077636719, loss_series 33.70317077636719
epoch 2320, loss_perior 33.70299530029297, loss_series 33.70299530029297
epoch 2321, loss_perior 33.68562698364258, loss_series 33.68562698364258
epoch 2322, loss_perior 33.76706314086914, loss_series 33.76706314086914
epoch 2323, loss_perior 33.67236328125, loss_series 33.67236328125
epoch 2324, loss_perior 33.64491653442383, loss_series 33.64491653442383
epoch 2325, loss_perior 33.68220520019531, loss_series 33.68220520019531
epoch 2326, loss_perior 33.70429229736328, loss_series 33.70429229736328
epoch 2327, loss_perior 33.705780029296875, loss_series 33.705780029296875
epoch 2328, loss_perior 33.691375732421875, loss_series 33.691375732421875
epoch 2329, loss_perior 33.6501579284668, loss_series 33.6501579284668
epoch 2330, loss_perior 33.7135124206543, loss_series 33.7135124206543
epoch 2331, loss_perior 33.662837982177734, loss_series 33.662837982177734
epoch 2332, loss_perior 33.71450424194336, loss_series 33.71450424194336
epoch 2333, loss_perior 33.68295669555664, loss_series 33.68295669555664
epoch 2334, loss_perior 33.71690368652344, loss_series 33.71690368652344
epoch 2335, loss_perior 33.67314910888672, loss_series 33.67314910888672
epoch 2336, loss_perior 33.67768096923828, loss_series 33.67768096923828
epoch 2337, loss_perior 33.68840789794922, loss_series 33.68840789794922
epoch 2338, loss_perior 33.71091079711914, loss_series 33.71091079711914
epoch 2339, loss_perior 33.66169738769531, loss_series 33.66169738769531
epoch 2340, loss_perior 33.6745719909668, loss_series 33.6745719909668
epoch 2341, loss_perior 33.675193786621094, loss_series 33.675193786621094
epoch 2342, loss_perior 33.71469497680664, loss_series 33.71469497680664
epoch 2343, loss_perior 33.674530029296875, loss_series 33.674530029296875
epoch 2344, loss_perior 33.6807975769043, loss_series 33.6807975769043
epoch 2345, loss_perior 33.69713592529297, loss_series 33.69713592529297
epoch 2346, loss_perior 33.67113494873047, loss_series 33.67113494873047
epoch 2347, loss_perior 33.67493438720703, loss_series 33.67493438720703
epoch 2348, loss_perior 33.7216911315918, loss_series 33.7216911315918
epoch 2349, loss_perior 33.733543395996094, loss_series 33.733543395996094
epoch 2350, loss_perior 33.71895980834961, loss_series 33.71895980834961
epoch 2351, loss_perior 33.70371627807617, loss_series 33.70371627807617
epoch 2352, loss_perior 33.663265228271484, loss_series 33.663265228271484
epoch 2353, loss_perior 33.69899368286133, loss_series 33.69899368286133
epoch 2354, loss_perior 33.7017936706543, loss_series 33.7017936706543
epoch 2355, loss_perior 33.69645309448242, loss_series 33.69645309448242
epoch 2356, loss_perior 33.69515609741211, loss_series 33.69515609741211
epoch 2357, loss_perior 33.706878662109375, loss_series 33.706878662109375
epoch 2358, loss_perior 33.68046188354492, loss_series 33.68046188354492
epoch 2359, loss_perior 33.69050216674805, loss_series 33.69050216674805
epoch 2360, loss_perior 33.698638916015625, loss_series 33.698638916015625
epoch 2361, loss_perior 33.69634246826172, loss_series 33.69634246826172
epoch 2362, loss_perior 33.67716979980469, loss_series 33.67716979980469
epoch 2363, loss_perior 33.67436981201172, loss_series 33.67436981201172
epoch 2364, loss_perior 33.65728759765625, loss_series 33.65728759765625
epoch 2365, loss_perior 33.740623474121094, loss_series 33.740623474121094
epoch 2366, loss_perior 33.66019821166992, loss_series 33.66019821166992
epoch 2367, loss_perior 33.70626449584961, loss_series 33.70626449584961
epoch 2368, loss_perior 33.68730926513672, loss_series 33.68730926513672
epoch 2369, loss_perior 33.69352340698242, loss_series 33.69352340698242
epoch 2370, loss_perior 33.720943450927734, loss_series 33.720943450927734
epoch 2371, loss_perior 33.6907958984375, loss_series 33.6907958984375
epoch 2372, loss_perior 33.675071716308594, loss_series 33.675071716308594
epoch 2373, loss_perior 33.68663024902344, loss_series 33.68663024902344
epoch 2374, loss_perior 33.695064544677734, loss_series 33.695064544677734
epoch 2375, loss_perior 33.693580627441406, loss_series 33.693580627441406
epoch 2376, loss_perior 33.644386291503906, loss_series 33.644386291503906
epoch 2377, loss_perior 33.65234375, loss_series 33.65234375
epoch 2378, loss_perior 33.729652404785156, loss_series 33.729652404785156
epoch 2379, loss_perior 33.65082931518555, loss_series 33.65082931518555
epoch 2380, loss_perior 33.66539764404297, loss_series 33.66539764404297
epoch 2381, loss_perior 33.725807189941406, loss_series 33.725807189941406
	speed: 0.4018s/iter; left time: 709.9403s
epoch 2382, loss_perior 33.7332878112793, loss_series 33.7332878112793
epoch 2383, loss_perior 33.717796325683594, loss_series 33.717796325683594
epoch 2384, loss_perior 33.66688537597656, loss_series 33.66688537597656
epoch 2385, loss_perior 33.67547607421875, loss_series 33.67547607421875
epoch 2386, loss_perior 33.650001525878906, loss_series 33.650001525878906
epoch 2387, loss_perior 33.70477294921875, loss_series 33.70477294921875
epoch 2388, loss_perior 33.71083068847656, loss_series 33.71083068847656
epoch 2389, loss_perior 33.647735595703125, loss_series 33.647735595703125
epoch 2390, loss_perior 33.654354095458984, loss_series 33.654354095458984
epoch 2391, loss_perior 33.6966438293457, loss_series 33.6966438293457
epoch 2392, loss_perior 33.681724548339844, loss_series 33.681724548339844
epoch 2393, loss_perior 33.689117431640625, loss_series 33.689117431640625
epoch 2394, loss_perior 33.69542694091797, loss_series 33.69542694091797
epoch 2395, loss_perior 33.68690490722656, loss_series 33.68690490722656
epoch 2396, loss_perior 33.70393371582031, loss_series 33.70393371582031
epoch 2397, loss_perior 33.677574157714844, loss_series 33.677574157714844
epoch 2398, loss_perior 33.735435485839844, loss_series 33.735435485839844
epoch 2399, loss_perior 33.674102783203125, loss_series 33.674102783203125
epoch 2400, loss_perior 33.70906066894531, loss_series 33.70906066894531
epoch 2401, loss_perior 33.67931365966797, loss_series 33.67931365966797
epoch 2402, loss_perior 33.713539123535156, loss_series 33.713539123535156
epoch 2403, loss_perior 33.708900451660156, loss_series 33.708900451660156
epoch 2404, loss_perior 33.72956848144531, loss_series 33.72956848144531
epoch 2405, loss_perior 33.728759765625, loss_series 33.728759765625
epoch 2406, loss_perior 33.7169303894043, loss_series 33.7169303894043
epoch 2407, loss_perior 33.71669006347656, loss_series 33.71669006347656
epoch 2408, loss_perior 33.770198822021484, loss_series 33.770198822021484
epoch 2409, loss_perior 33.74925994873047, loss_series 33.74925994873047
epoch 2410, loss_perior 33.779457092285156, loss_series 33.779457092285156
epoch 2411, loss_perior 33.72404479980469, loss_series 33.72404479980469
epoch 2412, loss_perior 33.74479675292969, loss_series 33.74479675292969
epoch 2413, loss_perior 33.6767578125, loss_series 33.6767578125
epoch 2414, loss_perior 33.66364288330078, loss_series 33.66364288330078
epoch 2415, loss_perior 33.70699691772461, loss_series 33.70699691772461
epoch 2416, loss_perior 33.72479248046875, loss_series 33.72479248046875
epoch 2417, loss_perior 33.7257080078125, loss_series 33.7257080078125
epoch 2418, loss_perior 33.685157775878906, loss_series 33.685157775878906
epoch 2419, loss_perior 33.699790954589844, loss_series 33.699790954589844
epoch 2420, loss_perior 33.727745056152344, loss_series 33.727745056152344
epoch 2421, loss_perior 33.69916915893555, loss_series 33.69916915893555
epoch 2422, loss_perior 33.674163818359375, loss_series 33.674163818359375
epoch 2423, loss_perior 33.69929885864258, loss_series 33.69929885864258
epoch 2424, loss_perior 33.719085693359375, loss_series 33.719085693359375
epoch 2425, loss_perior 33.656494140625, loss_series 33.656494140625
epoch 2426, loss_perior 33.680824279785156, loss_series 33.680824279785156
epoch 2427, loss_perior 33.69625473022461, loss_series 33.69625473022461
epoch 2428, loss_perior 33.67646026611328, loss_series 33.67646026611328
epoch 2429, loss_perior 33.695274353027344, loss_series 33.695274353027344
epoch 2430, loss_perior 33.725807189941406, loss_series 33.725807189941406
epoch 2431, loss_perior 33.696800231933594, loss_series 33.696800231933594
epoch 2432, loss_perior 33.712242126464844, loss_series 33.712242126464844
epoch 2433, loss_perior 33.69940185546875, loss_series 33.69940185546875
epoch 2434, loss_perior 33.67283630371094, loss_series 33.67283630371094
epoch 2435, loss_perior 33.72095489501953, loss_series 33.72095489501953
epoch 2436, loss_perior 33.708744049072266, loss_series 33.708744049072266
epoch 2437, loss_perior 33.734893798828125, loss_series 33.734893798828125
epoch 2438, loss_perior 33.709938049316406, loss_series 33.709938049316406
epoch 2439, loss_perior 33.669227600097656, loss_series 33.669227600097656
epoch 2440, loss_perior 33.72467803955078, loss_series 33.72467803955078
epoch 2441, loss_perior 33.67744064331055, loss_series 33.67744064331055
epoch 2442, loss_perior 33.718902587890625, loss_series 33.718902587890625
epoch 2443, loss_perior 33.68125534057617, loss_series 33.68125534057617
epoch 2444, loss_perior 33.71318817138672, loss_series 33.71318817138672
epoch 2445, loss_perior 33.67230224609375, loss_series 33.67230224609375
epoch 2446, loss_perior 33.695152282714844, loss_series 33.695152282714844
epoch 2447, loss_perior 33.69508361816406, loss_series 33.69508361816406
epoch 2448, loss_perior 33.715003967285156, loss_series 33.715003967285156
epoch 2449, loss_perior 33.7288818359375, loss_series 33.7288818359375
epoch 2450, loss_perior 33.70562744140625, loss_series 33.70562744140625
epoch 2451, loss_perior 33.69441604614258, loss_series 33.69441604614258
epoch 2452, loss_perior 33.76370620727539, loss_series 33.76370620727539
epoch 2453, loss_perior 33.68077850341797, loss_series 33.68077850341797
epoch 2454, loss_perior 33.68882369995117, loss_series 33.68882369995117
epoch 2455, loss_perior 33.65687561035156, loss_series 33.65687561035156
epoch 2456, loss_perior 33.700592041015625, loss_series 33.700592041015625
epoch 2457, loss_perior 33.672943115234375, loss_series 33.672943115234375
epoch 2458, loss_perior 33.718177795410156, loss_series 33.718177795410156
epoch 2459, loss_perior 33.65938949584961, loss_series 33.65938949584961
epoch 2460, loss_perior 33.68741989135742, loss_series 33.68741989135742
epoch 2461, loss_perior 33.68071746826172, loss_series 33.68071746826172
epoch 2462, loss_perior 33.683650970458984, loss_series 33.683650970458984
epoch 2463, loss_perior 33.663787841796875, loss_series 33.663787841796875
epoch 2464, loss_perior 33.7003173828125, loss_series 33.7003173828125
epoch 2465, loss_perior 33.70208740234375, loss_series 33.70208740234375
epoch 2466, loss_perior 33.671478271484375, loss_series 33.671478271484375
epoch 2467, loss_perior 33.70521545410156, loss_series 33.70521545410156
epoch 2468, loss_perior 33.681739807128906, loss_series 33.681739807128906
epoch 2469, loss_perior 33.70954895019531, loss_series 33.70954895019531
epoch 2470, loss_perior 33.63900375366211, loss_series 33.63900375366211
epoch 2471, loss_perior 33.71036911010742, loss_series 33.71036911010742
epoch 2472, loss_perior 33.701148986816406, loss_series 33.701148986816406
epoch 2473, loss_perior 33.70206069946289, loss_series 33.70206069946289
epoch 2474, loss_perior 33.71169662475586, loss_series 33.71169662475586
epoch 2475, loss_perior 33.706687927246094, loss_series 33.706687927246094
epoch 2476, loss_perior 33.71567916870117, loss_series 33.71567916870117
epoch 2477, loss_perior 33.66960144042969, loss_series 33.66960144042969
epoch 2478, loss_perior 33.69640350341797, loss_series 33.69640350341797
epoch 2479, loss_perior 33.71528244018555, loss_series 33.71528244018555
epoch 2480, loss_perior 33.67198181152344, loss_series 33.67198181152344
epoch 2481, loss_perior 33.697914123535156, loss_series 33.697914123535156
	speed: 0.4017s/iter; left time: 669.5640s
epoch 2482, loss_perior 33.70136642456055, loss_series 33.70136642456055
epoch 2483, loss_perior 33.63821029663086, loss_series 33.63821029663086
epoch 2484, loss_perior 33.681121826171875, loss_series 33.681121826171875
epoch 2485, loss_perior 33.681800842285156, loss_series 33.681800842285156
epoch 2486, loss_perior 33.69046401977539, loss_series 33.69046401977539
epoch 2487, loss_perior 33.7244987487793, loss_series 33.7244987487793
epoch 2488, loss_perior 33.68803024291992, loss_series 33.68803024291992
epoch 2489, loss_perior 33.715641021728516, loss_series 33.715641021728516
epoch 2490, loss_perior 33.732139587402344, loss_series 33.732139587402344
epoch 2491, loss_perior 33.679283142089844, loss_series 33.679283142089844
epoch 2492, loss_perior 33.707862854003906, loss_series 33.707862854003906
epoch 2493, loss_perior 33.69221496582031, loss_series 33.69221496582031
epoch 2494, loss_perior 33.720611572265625, loss_series 33.720611572265625
epoch 2495, loss_perior 33.684791564941406, loss_series 33.684791564941406
epoch 2496, loss_perior 33.71904754638672, loss_series 33.71904754638672
epoch 2497, loss_perior 33.66682434082031, loss_series 33.66682434082031
epoch 2498, loss_perior 33.700252532958984, loss_series 33.700252532958984
epoch 2499, loss_perior 33.6396484375, loss_series 33.6396484375
epoch 2500, loss_perior 33.6898307800293, loss_series 33.6898307800293
epoch 2501, loss_perior 33.73014450073242, loss_series 33.73014450073242
epoch 2502, loss_perior 33.66461181640625, loss_series 33.66461181640625
epoch 2503, loss_perior 33.708740234375, loss_series 33.708740234375
epoch 2504, loss_perior 33.710960388183594, loss_series 33.710960388183594
epoch 2505, loss_perior 33.73487091064453, loss_series 33.73487091064453
epoch 2506, loss_perior 33.701454162597656, loss_series 33.701454162597656
epoch 2507, loss_perior 33.73310470581055, loss_series 33.73310470581055
epoch 2508, loss_perior 33.700077056884766, loss_series 33.700077056884766
epoch 2509, loss_perior 33.71827697753906, loss_series 33.71827697753906
epoch 2510, loss_perior 33.67787170410156, loss_series 33.67787170410156
epoch 2511, loss_perior 33.698265075683594, loss_series 33.698265075683594
epoch 2512, loss_perior 33.65940856933594, loss_series 33.65940856933594
epoch 2513, loss_perior 33.706451416015625, loss_series 33.706451416015625
epoch 2514, loss_perior 33.74505615234375, loss_series 33.74505615234375
epoch 2515, loss_perior 33.7096061706543, loss_series 33.7096061706543
epoch 2516, loss_perior 33.71406555175781, loss_series 33.71406555175781
epoch 2517, loss_perior 33.696285247802734, loss_series 33.696285247802734
epoch 2518, loss_perior 33.706199645996094, loss_series 33.706199645996094
epoch 2519, loss_perior 33.68141174316406, loss_series 33.68141174316406
epoch 2520, loss_perior 33.65195846557617, loss_series 33.65195846557617
epoch 2521, loss_perior 33.740234375, loss_series 33.740234375
epoch 2522, loss_perior 33.70793914794922, loss_series 33.70793914794922
epoch 2523, loss_perior 33.682373046875, loss_series 33.682373046875
epoch 2524, loss_perior 33.69324493408203, loss_series 33.69324493408203
epoch 2525, loss_perior 33.676185607910156, loss_series 33.676185607910156
epoch 2526, loss_perior 33.720970153808594, loss_series 33.720970153808594
epoch 2527, loss_perior 33.675636291503906, loss_series 33.675636291503906
epoch 2528, loss_perior 33.712120056152344, loss_series 33.712120056152344
epoch 2529, loss_perior 33.6707878112793, loss_series 33.6707878112793
epoch 2530, loss_perior 33.69879150390625, loss_series 33.69879150390625
epoch 2531, loss_perior 33.690467834472656, loss_series 33.690467834472656
epoch 2532, loss_perior 33.70140838623047, loss_series 33.70140838623047
epoch 2533, loss_perior 33.65869903564453, loss_series 33.65869903564453
epoch 2534, loss_perior 33.68557357788086, loss_series 33.68557357788086
epoch 2535, loss_perior 33.68701171875, loss_series 33.68701171875
epoch 2536, loss_perior 33.705665588378906, loss_series 33.705665588378906
epoch 2537, loss_perior 33.68923568725586, loss_series 33.68923568725586
epoch 2538, loss_perior 33.663536071777344, loss_series 33.663536071777344
epoch 2539, loss_perior 33.67210388183594, loss_series 33.67210388183594
epoch 2540, loss_perior 33.71472930908203, loss_series 33.71472930908203
epoch 2541, loss_perior 33.67120361328125, loss_series 33.67120361328125
epoch 2542, loss_perior 33.7353515625, loss_series 33.7353515625
epoch 2543, loss_perior 33.70255661010742, loss_series 33.70255661010742
epoch 2544, loss_perior 33.6525993347168, loss_series 33.6525993347168
epoch 2545, loss_perior 33.696502685546875, loss_series 33.696502685546875
epoch 2546, loss_perior 33.67730712890625, loss_series 33.67730712890625
epoch 2547, loss_perior 33.68119812011719, loss_series 33.68119812011719
epoch 2548, loss_perior 33.71145248413086, loss_series 33.71145248413086
epoch 2549, loss_perior 33.69770431518555, loss_series 33.69770431518555
epoch 2550, loss_perior 33.71515655517578, loss_series 33.71515655517578
epoch 2551, loss_perior 33.70083236694336, loss_series 33.70083236694336
epoch 2552, loss_perior 33.69010925292969, loss_series 33.69010925292969
epoch 2553, loss_perior 33.68629455566406, loss_series 33.68629455566406
epoch 2554, loss_perior 33.673362731933594, loss_series 33.673362731933594
epoch 2555, loss_perior 33.64024353027344, loss_series 33.64024353027344
epoch 2556, loss_perior 33.724369049072266, loss_series 33.724369049072266
epoch 2557, loss_perior 33.67516326904297, loss_series 33.67516326904297
epoch 2558, loss_perior 33.69712829589844, loss_series 33.69712829589844
epoch 2559, loss_perior 33.69622802734375, loss_series 33.69622802734375
epoch 2560, loss_perior 33.67539596557617, loss_series 33.67539596557617
epoch 2561, loss_perior 33.65164566040039, loss_series 33.65164566040039
epoch 2562, loss_perior 33.628761291503906, loss_series 33.628761291503906
epoch 2563, loss_perior 33.723026275634766, loss_series 33.723026275634766
epoch 2564, loss_perior 33.66166687011719, loss_series 33.66166687011719
epoch 2565, loss_perior 33.713844299316406, loss_series 33.713844299316406
epoch 2566, loss_perior 33.632728576660156, loss_series 33.632728576660156
epoch 2567, loss_perior 33.66128158569336, loss_series 33.66128158569336
epoch 2568, loss_perior 33.69310760498047, loss_series 33.69310760498047
epoch 2569, loss_perior 33.718231201171875, loss_series 33.718231201171875
epoch 2570, loss_perior 33.733638763427734, loss_series 33.733638763427734
epoch 2571, loss_perior 33.64520263671875, loss_series 33.64520263671875
epoch 2572, loss_perior 33.74544906616211, loss_series 33.74544906616211
epoch 2573, loss_perior 33.692649841308594, loss_series 33.692649841308594
epoch 2574, loss_perior 33.709503173828125, loss_series 33.709503173828125
epoch 2575, loss_perior 33.68132781982422, loss_series 33.68132781982422
epoch 2576, loss_perior 33.72957229614258, loss_series 33.72957229614258
epoch 2577, loss_perior 33.656917572021484, loss_series 33.656917572021484
epoch 2578, loss_perior 33.64124298095703, loss_series 33.64124298095703
epoch 2579, loss_perior 33.69895935058594, loss_series 33.69895935058594
epoch 2580, loss_perior 33.67289352416992, loss_series 33.67289352416992
epoch 2581, loss_perior 33.69664001464844, loss_series 33.69664001464844
	speed: 0.4016s/iter; left time: 629.2604s
epoch 2582, loss_perior 33.68935775756836, loss_series 33.68935775756836
epoch 2583, loss_perior 33.6691780090332, loss_series 33.6691780090332
epoch 2584, loss_perior 33.664833068847656, loss_series 33.664833068847656
epoch 2585, loss_perior 33.616172790527344, loss_series 33.616172790527344
epoch 2586, loss_perior 33.74012756347656, loss_series 33.74012756347656
epoch 2587, loss_perior 33.67488479614258, loss_series 33.67488479614258
epoch 2588, loss_perior 33.69163131713867, loss_series 33.69163131713867
epoch 2589, loss_perior 33.68857192993164, loss_series 33.68857192993164
epoch 2590, loss_perior 33.703128814697266, loss_series 33.703128814697266
epoch 2591, loss_perior 33.697410583496094, loss_series 33.697410583496094
epoch 2592, loss_perior 33.68107604980469, loss_series 33.68107604980469
epoch 2593, loss_perior 33.65898132324219, loss_series 33.65898132324219
epoch 2594, loss_perior 33.68013000488281, loss_series 33.68013000488281
epoch 2595, loss_perior 33.692474365234375, loss_series 33.692474365234375
epoch 2596, loss_perior 33.66905975341797, loss_series 33.66905975341797
epoch 2597, loss_perior 33.65237045288086, loss_series 33.65237045288086
epoch 2598, loss_perior 33.70374298095703, loss_series 33.70374298095703
epoch 2599, loss_perior 33.658782958984375, loss_series 33.658782958984375
epoch 2600, loss_perior 33.70423889160156, loss_series 33.70423889160156
epoch 2601, loss_perior 33.688575744628906, loss_series 33.688575744628906
epoch 2602, loss_perior 33.71424865722656, loss_series 33.71424865722656
epoch 2603, loss_perior 33.67019271850586, loss_series 33.67019271850586
epoch 2604, loss_perior 33.65292739868164, loss_series 33.65292739868164
epoch 2605, loss_perior 33.65312576293945, loss_series 33.65312576293945
epoch 2606, loss_perior 33.687408447265625, loss_series 33.687408447265625
epoch 2607, loss_perior 33.66518783569336, loss_series 33.66518783569336
epoch 2608, loss_perior 33.691097259521484, loss_series 33.691097259521484
epoch 2609, loss_perior 33.66572570800781, loss_series 33.66572570800781
epoch 2610, loss_perior 33.697059631347656, loss_series 33.697059631347656
epoch 2611, loss_perior 33.64784240722656, loss_series 33.64784240722656
epoch 2612, loss_perior 33.74021530151367, loss_series 33.74021530151367
epoch 2613, loss_perior 33.72917175292969, loss_series 33.72917175292969
epoch 2614, loss_perior 33.70865249633789, loss_series 33.70865249633789
epoch 2615, loss_perior 33.66259002685547, loss_series 33.66259002685547
epoch 2616, loss_perior 33.673763275146484, loss_series 33.673763275146484
epoch 2617, loss_perior 33.6844367980957, loss_series 33.6844367980957
epoch 2618, loss_perior 33.71044158935547, loss_series 33.71044158935547
epoch 2619, loss_perior 33.71260452270508, loss_series 33.71260452270508
epoch 2620, loss_perior 33.654029846191406, loss_series 33.654029846191406
epoch 2621, loss_perior 33.70415496826172, loss_series 33.70415496826172
epoch 2622, loss_perior 33.70252227783203, loss_series 33.70252227783203
epoch 2623, loss_perior 33.676231384277344, loss_series 33.676231384277344
epoch 2624, loss_perior 33.689449310302734, loss_series 33.689449310302734
epoch 2625, loss_perior 33.676551818847656, loss_series 33.676551818847656
epoch 2626, loss_perior 33.67409133911133, loss_series 33.67409133911133
epoch 2627, loss_perior 33.739830017089844, loss_series 33.739830017089844
epoch 2628, loss_perior 33.71025848388672, loss_series 33.71025848388672
epoch 2629, loss_perior 33.66634750366211, loss_series 33.66634750366211
epoch 2630, loss_perior 33.705238342285156, loss_series 33.705238342285156
epoch 2631, loss_perior 33.698970794677734, loss_series 33.698970794677734
epoch 2632, loss_perior 33.66887664794922, loss_series 33.66887664794922
epoch 2633, loss_perior 33.666419982910156, loss_series 33.666419982910156
epoch 2634, loss_perior 33.731590270996094, loss_series 33.731590270996094
epoch 2635, loss_perior 33.733177185058594, loss_series 33.733177185058594
epoch 2636, loss_perior 33.74577331542969, loss_series 33.74577331542969
epoch 2637, loss_perior 33.69620132446289, loss_series 33.69620132446289
epoch 2638, loss_perior 33.73810958862305, loss_series 33.73810958862305
epoch 2639, loss_perior 33.657981872558594, loss_series 33.657981872558594
epoch 2640, loss_perior 33.73442840576172, loss_series 33.73442840576172
epoch 2641, loss_perior 33.66923522949219, loss_series 33.66923522949219
epoch 2642, loss_perior 33.647010803222656, loss_series 33.647010803222656
epoch 2643, loss_perior 33.70301055908203, loss_series 33.70301055908203
epoch 2644, loss_perior 33.69083786010742, loss_series 33.69083786010742
epoch 2645, loss_perior 33.67135238647461, loss_series 33.67135238647461
epoch 2646, loss_perior 33.7024040222168, loss_series 33.7024040222168
epoch 2647, loss_perior 33.68598937988281, loss_series 33.68598937988281
epoch 2648, loss_perior 33.62828826904297, loss_series 33.62828826904297
epoch 2649, loss_perior 33.692718505859375, loss_series 33.692718505859375
epoch 2650, loss_perior 33.70796585083008, loss_series 33.70796585083008
epoch 2651, loss_perior 33.672752380371094, loss_series 33.672752380371094
epoch 2652, loss_perior 33.721046447753906, loss_series 33.721046447753906
epoch 2653, loss_perior 33.71068572998047, loss_series 33.71068572998047
epoch 2654, loss_perior 33.697486877441406, loss_series 33.697486877441406
epoch 2655, loss_perior 33.683692932128906, loss_series 33.683692932128906
epoch 2656, loss_perior 33.6998291015625, loss_series 33.6998291015625
epoch 2657, loss_perior 33.66642761230469, loss_series 33.66642761230469
epoch 2658, loss_perior 33.67042922973633, loss_series 33.67042922973633
epoch 2659, loss_perior 33.74989318847656, loss_series 33.74989318847656
epoch 2660, loss_perior 33.678375244140625, loss_series 33.678375244140625
epoch 2661, loss_perior 33.67802810668945, loss_series 33.67802810668945
epoch 2662, loss_perior 33.69386291503906, loss_series 33.69386291503906
epoch 2663, loss_perior 33.657596588134766, loss_series 33.657596588134766
epoch 2664, loss_perior 33.688438415527344, loss_series 33.688438415527344
epoch 2665, loss_perior 33.705142974853516, loss_series 33.705142974853516
epoch 2666, loss_perior 33.69639587402344, loss_series 33.69639587402344
epoch 2667, loss_perior 33.63829803466797, loss_series 33.63829803466797
epoch 2668, loss_perior 33.678627014160156, loss_series 33.678627014160156
epoch 2669, loss_perior 33.71580505371094, loss_series 33.71580505371094
epoch 2670, loss_perior 33.6629753112793, loss_series 33.6629753112793
epoch 2671, loss_perior 33.695960998535156, loss_series 33.695960998535156
epoch 2672, loss_perior 33.66793441772461, loss_series 33.66793441772461
epoch 2673, loss_perior 33.67670440673828, loss_series 33.67670440673828
epoch 2674, loss_perior 33.65827560424805, loss_series 33.65827560424805
epoch 2675, loss_perior 33.75408935546875, loss_series 33.75408935546875
epoch 2676, loss_perior 33.664886474609375, loss_series 33.664886474609375
epoch 2677, loss_perior 33.74116897583008, loss_series 33.74116897583008
epoch 2678, loss_perior 33.68388366699219, loss_series 33.68388366699219
epoch 2679, loss_perior 33.69868469238281, loss_series 33.69868469238281
epoch 2680, loss_perior 33.69950485229492, loss_series 33.69950485229492
epoch 2681, loss_perior 33.65425109863281, loss_series 33.65425109863281
	speed: 0.4014s/iter; left time: 588.9031s
epoch 2682, loss_perior 33.66604232788086, loss_series 33.66604232788086
epoch 2683, loss_perior 33.72715759277344, loss_series 33.72715759277344
epoch 2684, loss_perior 33.716190338134766, loss_series 33.716190338134766
epoch 2685, loss_perior 33.69467544555664, loss_series 33.69467544555664
epoch 2686, loss_perior 33.71070861816406, loss_series 33.71070861816406
epoch 2687, loss_perior 33.73097229003906, loss_series 33.73097229003906
epoch 2688, loss_perior 33.65044403076172, loss_series 33.65044403076172
epoch 2689, loss_perior 33.652339935302734, loss_series 33.652339935302734
epoch 2690, loss_perior 33.658287048339844, loss_series 33.658287048339844
epoch 2691, loss_perior 33.70175552368164, loss_series 33.70175552368164
epoch 2692, loss_perior 33.71305847167969, loss_series 33.71305847167969
epoch 2693, loss_perior 33.74762725830078, loss_series 33.74762725830078
epoch 2694, loss_perior 33.695343017578125, loss_series 33.695343017578125
epoch 2695, loss_perior 33.680877685546875, loss_series 33.680877685546875
epoch 2696, loss_perior 33.694580078125, loss_series 33.694580078125
epoch 2697, loss_perior 33.68366241455078, loss_series 33.68366241455078
epoch 2698, loss_perior 33.670318603515625, loss_series 33.670318603515625
epoch 2699, loss_perior 33.69252014160156, loss_series 33.69252014160156
epoch 2700, loss_perior 33.697113037109375, loss_series 33.697113037109375
epoch 2701, loss_perior 33.67475891113281, loss_series 33.67475891113281
epoch 2702, loss_perior 33.665977478027344, loss_series 33.665977478027344
epoch 2703, loss_perior 33.71234130859375, loss_series 33.71234130859375
epoch 2704, loss_perior 33.688018798828125, loss_series 33.688018798828125
epoch 2705, loss_perior 33.70811462402344, loss_series 33.70811462402344
epoch 2706, loss_perior 33.67826843261719, loss_series 33.67826843261719
epoch 2707, loss_perior 33.69827651977539, loss_series 33.69827651977539
epoch 2708, loss_perior 33.74712371826172, loss_series 33.74712371826172
epoch 2709, loss_perior 33.7319221496582, loss_series 33.7319221496582
epoch 2710, loss_perior 33.67959213256836, loss_series 33.67959213256836
epoch 2711, loss_perior 33.70016098022461, loss_series 33.70016098022461
epoch 2712, loss_perior 33.69630432128906, loss_series 33.69630432128906
epoch 2713, loss_perior 33.67169189453125, loss_series 33.67169189453125
epoch 2714, loss_perior 33.66535949707031, loss_series 33.66535949707031
epoch 2715, loss_perior 33.64029312133789, loss_series 33.64029312133789
epoch 2716, loss_perior 33.697715759277344, loss_series 33.697715759277344
epoch 2717, loss_perior 33.7447395324707, loss_series 33.7447395324707
epoch 2718, loss_perior 33.683929443359375, loss_series 33.683929443359375
epoch 2719, loss_perior 33.702754974365234, loss_series 33.702754974365234
epoch 2720, loss_perior 33.66893005371094, loss_series 33.66893005371094
epoch 2721, loss_perior 33.724937438964844, loss_series 33.724937438964844
epoch 2722, loss_perior 33.66665267944336, loss_series 33.66665267944336
epoch 2723, loss_perior 33.64793014526367, loss_series 33.64793014526367
epoch 2724, loss_perior 33.682708740234375, loss_series 33.682708740234375
epoch 2725, loss_perior 33.67487335205078, loss_series 33.67487335205078
epoch 2726, loss_perior 33.685325622558594, loss_series 33.685325622558594
epoch 2727, loss_perior 33.68254470825195, loss_series 33.68254470825195
epoch 2728, loss_perior 33.65922546386719, loss_series 33.65922546386719
epoch 2729, loss_perior 33.715633392333984, loss_series 33.715633392333984
epoch 2730, loss_perior 33.668216705322266, loss_series 33.668216705322266
epoch 2731, loss_perior 33.67661666870117, loss_series 33.67661666870117
epoch 2732, loss_perior 33.731224060058594, loss_series 33.731224060058594
epoch 2733, loss_perior 33.71257019042969, loss_series 33.71257019042969
epoch 2734, loss_perior 33.709625244140625, loss_series 33.709625244140625
epoch 2735, loss_perior 33.699485778808594, loss_series 33.699485778808594
epoch 2736, loss_perior 33.652706146240234, loss_series 33.652706146240234
epoch 2737, loss_perior 33.702362060546875, loss_series 33.702362060546875
epoch 2738, loss_perior 33.731239318847656, loss_series 33.731239318847656
epoch 2739, loss_perior 33.7310791015625, loss_series 33.7310791015625
epoch 2740, loss_perior 33.69746780395508, loss_series 33.69746780395508
epoch 2741, loss_perior 33.686927795410156, loss_series 33.686927795410156
epoch 2742, loss_perior 33.684654235839844, loss_series 33.684654235839844
epoch 2743, loss_perior 33.65142059326172, loss_series 33.65142059326172
epoch 2744, loss_perior 33.661983489990234, loss_series 33.661983489990234
epoch 2745, loss_perior 33.678123474121094, loss_series 33.678123474121094
epoch 2746, loss_perior 33.70525360107422, loss_series 33.70525360107422
epoch 2747, loss_perior 33.641639709472656, loss_series 33.641639709472656
epoch 2748, loss_perior 33.69615936279297, loss_series 33.69615936279297
epoch 2749, loss_perior 33.6943359375, loss_series 33.6943359375
epoch 2750, loss_perior 33.66172409057617, loss_series 33.66172409057617
epoch 2751, loss_perior 33.73265838623047, loss_series 33.73265838623047
epoch 2752, loss_perior 33.692237854003906, loss_series 33.692237854003906
epoch 2753, loss_perior 33.6950569152832, loss_series 33.6950569152832
epoch 2754, loss_perior 33.67352294921875, loss_series 33.67352294921875
epoch 2755, loss_perior 33.711029052734375, loss_series 33.711029052734375
epoch 2756, loss_perior 33.690670013427734, loss_series 33.690670013427734
epoch 2757, loss_perior 33.6887092590332, loss_series 33.6887092590332
epoch 2758, loss_perior 33.62000274658203, loss_series 33.62000274658203
epoch 2759, loss_perior 33.6827507019043, loss_series 33.6827507019043
epoch 2760, loss_perior 33.68959426879883, loss_series 33.68959426879883
epoch 2761, loss_perior 33.645423889160156, loss_series 33.645423889160156
epoch 2762, loss_perior 33.68826675415039, loss_series 33.68826675415039
epoch 2763, loss_perior 33.74433898925781, loss_series 33.74433898925781
epoch 2764, loss_perior 33.68480682373047, loss_series 33.68480682373047
epoch 2765, loss_perior 33.746891021728516, loss_series 33.746891021728516
Epoch: 2, Cost time: 776.355s 
epoch 2766, loss_perior 33.6640510559082, loss_series 33.6640510559082
epoch 2767, loss_perior 33.70410919189453, loss_series 33.70410919189453
epoch 2768, loss_perior 33.7130126953125, loss_series 33.7130126953125
epoch 2769, loss_perior 33.71519470214844, loss_series 33.71519470214844
epoch 2770, loss_perior 33.70777130126953, loss_series 33.70777130126953
epoch 2771, loss_perior 33.666351318359375, loss_series 33.666351318359375
epoch 2772, loss_perior 33.697715759277344, loss_series 33.697715759277344
epoch 2773, loss_perior 33.71934509277344, loss_series 33.71934509277344
epoch 2774, loss_perior 33.72231674194336, loss_series 33.72231674194336
epoch 2775, loss_perior 33.70809555053711, loss_series 33.70809555053711
epoch 2776, loss_perior 33.666603088378906, loss_series 33.666603088378906
epoch 2777, loss_perior 33.69805908203125, loss_series 33.69805908203125
epoch 2778, loss_perior 33.66730499267578, loss_series 33.66730499267578
epoch 2779, loss_perior 33.685333251953125, loss_series 33.685333251953125
epoch 2780, loss_perior 33.71297836303711, loss_series 33.71297836303711
epoch 2781, loss_perior 33.685062408447266, loss_series 33.685062408447266
epoch 2782, loss_perior 33.63218307495117, loss_series 33.63218307495117
epoch 2783, loss_perior 33.625144958496094, loss_series 33.625144958496094
epoch 2784, loss_perior 33.67170715332031, loss_series 33.67170715332031
epoch 2785, loss_perior 33.630279541015625, loss_series 33.630279541015625
epoch 2786, loss_perior 33.66547393798828, loss_series 33.66547393798828
epoch 2787, loss_perior 33.65931701660156, loss_series 33.65931701660156
epoch 2788, loss_perior 33.696041107177734, loss_series 33.696041107177734
epoch 2789, loss_perior 33.660762786865234, loss_series 33.660762786865234
epoch 2790, loss_perior 33.64487075805664, loss_series 33.64487075805664
epoch 2791, loss_perior 33.64234924316406, loss_series 33.64234924316406
epoch 2792, loss_perior 33.646175384521484, loss_series 33.646175384521484
epoch 2793, loss_perior 33.68836975097656, loss_series 33.68836975097656
epoch 2794, loss_perior 33.67052459716797, loss_series 33.67052459716797
epoch 2795, loss_perior 33.674171447753906, loss_series 33.674171447753906
epoch 2796, loss_perior 33.65369415283203, loss_series 33.65369415283203
epoch 2797, loss_perior 33.66808319091797, loss_series 33.66808319091797
epoch 2798, loss_perior 33.679054260253906, loss_series 33.679054260253906
epoch 2799, loss_perior 33.68026351928711, loss_series 33.68026351928711
epoch 2800, loss_perior 33.70241165161133, loss_series 33.70241165161133
epoch 2801, loss_perior 33.66761779785156, loss_series 33.66761779785156
epoch 2802, loss_perior 33.686248779296875, loss_series 33.686248779296875
epoch 2803, loss_perior 33.683921813964844, loss_series 33.683921813964844
epoch 2804, loss_perior 33.671783447265625, loss_series 33.671783447265625
epoch 2805, loss_perior 33.683773040771484, loss_series 33.683773040771484
epoch 2806, loss_perior 33.704689025878906, loss_series 33.704689025878906



2024-12-15 12:48:08
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4133s/iter; left time: 1673.9045s
	speed: 0.3932s/iter; left time: 1553.0469s
	speed: 0.3942s/iter; left time: 1517.7316s
	speed: 0.3950s/iter; left time: 1481.1794s
	speed: 0.3954s/iter; left time: 1443.0334s
	speed: 0.3955s/iter; left time: 1403.9413s
	speed: 0.3957s/iter; left time: 1365.2537s
	speed: 0.3958s/iter; left time: 1326.0961s
	speed: 0.3956s/iter; left time: 1285.8175s
	speed: 0.3957s/iter; left time: 1246.4586s
	speed: 0.3955s/iter; left time: 1206.1870s
	speed: 0.3954s/iter; left time: 1166.3258s
	speed: 0.3953s/iter; left time: 1126.4646s
Epoch: 1, Cost time: 768.720s 
	speed: 2.9314s/iter; left time: 7818.0234s
	speed: 0.3955s/iter; left time: 1015.2341s
	speed: 0.3954s/iter; left time: 975.4499s
	speed: 0.3957s/iter; left time: 936.6587s
	speed: 0.3955s/iter; left time: 896.6456s
	speed: 0.3956s/iter; left time: 857.2657s
	speed: 0.3953s/iter; left time: 817.0175s
	speed: 0.3953s/iter; left time: 777.5164s
	speed: 0.3953s/iter; left time: 737.9901s
	speed: 0.3952s/iter; left time: 698.3370s
	speed: 0.3951s/iter; left time: 658.6745s
	speed: 0.3953s/iter; left time: 619.4186s
	speed: 0.3950s/iter; left time: 579.4965s
Epoch: 2, Cost time: 767.327s 
	speed: 2.9292s/iter; left time: 3761.0601s
	speed: 0.3946s/iter; left time: 467.2145s
	speed: 0.3948s/iter; left time: 428.0065s
	speed: 0.3950s/iter; left time: 388.6745s
	speed: 0.3953s/iter; left time: 349.4242s
	speed: 0.3949s/iter; left time: 309.5957s
	speed: 0.3947s/iter; left time: 270.0059s
	speed: 0.3951s/iter; left time: 230.7357s
	speed: 0.3947s/iter; left time: 191.0346s
	speed: 0.3950s/iter; left time: 151.6706s
	speed: 0.3947s/iter; left time: 112.0960s
	speed: 0.3949s/iter; left time: 72.6700s
	speed: 0.3947s/iter; left time: 33.1569s
Epoch: 3, Cost time: 766.335s 
Threshold : 0.024933516491204538
pa_accuracy           : 0.9877
pa_precision          : 0.8582
pa_recall             : 0.8461
pa_f_score            : 0.8521
MCC_score             : nan
Affiliation precision : 0.5170
Affiliation recall    : 0.8634
R_AUC_ROC             : 0.6946
R_AUC_PR              : 0.6489
VUS_ROC               : 0.6731
VUS_PR                : 0.6283
Accuracy : 0.9877, Precision : 0.8582, Recall : 0.8461, F-score : 0.8521 



2024-12-17 07:37:02
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-17 07:37:31
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4138s/iter; left time: 1675.9888s
	speed: 0.3933s/iter; left time: 1553.4450s
	speed: 0.3947s/iter; left time: 1519.5520s
	speed: 0.3953s/iter; left time: 1482.2719s
	speed: 0.3956s/iter; left time: 1443.9947s
	speed: 0.3957s/iter; left time: 1404.7375s
	speed: 0.3958s/iter; left time: 1365.5134s
	speed: 0.3959s/iter; left time: 1326.2740s
	speed: 0.3962s/iter; left time: 1287.7518s
	speed: 0.3961s/iter; left time: 1247.6516s
	speed: 0.3963s/iter; left time: 1208.6351s
	speed: 0.3961s/iter; left time: 1168.4908s
	speed: 0.3962s/iter; left time: 1129.2379s
Epoch: 1, Cost time: 769.332s 
	speed: 2.9327s/iter; left time: 7821.6159s
	speed: 0.3964s/iter; left time: 1017.5051s
	speed: 0.3967s/iter; left time: 978.6976s
	speed: 0.3966s/iter; left time: 938.8005s
	speed: 0.3964s/iter; left time: 898.7129s
	speed: 0.3963s/iter; left time: 858.8349s
	speed: 0.3962s/iter; left time: 818.9647s
	speed: 0.3964s/iter; left time: 779.6428s
	speed: 0.3963s/iter; left time: 739.9652s
	speed: 0.3964s/iter; left time: 700.4411s
	speed: 0.3962s/iter; left time: 660.4530s
	speed: 0.3963s/iter; left time: 620.9607s
	speed: 0.3961s/iter; left time: 581.0503s
Epoch: 2, Cost time: 768.802s 
	speed: 2.9322s/iter; left time: 3764.9359s
	speed: 0.3958s/iter; left time: 468.6299s
	speed: 0.3957s/iter; left time: 428.9124s
	speed: 0.3956s/iter; left time: 389.2998s
	speed: 0.3957s/iter; left time: 349.8427s
	speed: 0.3959s/iter; left time: 310.3791s
	speed: 0.3957s/iter; left time: 270.6264s
	speed: 0.3956s/iter; left time: 231.0419s
	speed: 0.3956s/iter; left time: 191.4758s
	speed: 0.3954s/iter; left time: 151.8497s
	speed: 0.3959s/iter; left time: 112.4231s
	speed: 0.3956s/iter; left time: 72.7826s
	speed: 0.3956s/iter; left time: 33.2288s
Epoch: 3, Cost time: 767.760s 
Threshold : 0.9981813724040993
pa_accuracy           : 0.9881
pa_precision          : 0.8609
pa_recall             : 0.8533
pa_f_score            : 0.8571
MCC_score             : nan
Affiliation precision : 0.5114
Affiliation recall    : 0.8882
R_AUC_ROC             : 0.6985
R_AUC_PR              : 0.6530
VUS_ROC               : 0.6838
VUS_PR                : 0.6389
Accuracy : 0.9881, Precision : 0.8609, Recall : 0.8533, F-score : 0.8571 



2024-12-21 05:57:29
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4133s/iter; left time: 1674.0199s
	speed: 0.3934s/iter; left time: 1554.0392s
	speed: 0.3953s/iter; left time: 1521.8410s
	speed: 0.3957s/iter; left time: 1483.7548s
	speed: 0.3966s/iter; left time: 1447.4199s
	speed: 0.3965s/iter; left time: 1407.5237s
	speed: 0.3965s/iter; left time: 1368.0652s
	speed: 0.3968s/iter; left time: 1329.2491s
	speed: 0.3966s/iter; left time: 1288.8564s
	speed: 0.3965s/iter; left time: 1248.9118s
	speed: 0.3968s/iter; left time: 1210.1222s
	speed: 0.3964s/iter; left time: 1169.2955s
	speed: 0.3966s/iter; left time: 1130.2891s
Epoch: 1, Cost time: 770.037s 
	speed: 2.9342s/iter; left time: 7825.5498s
	speed: 0.3972s/iter; left time: 1019.7303s
	speed: 0.3971s/iter; left time: 979.5735s
	speed: 0.3974s/iter; left time: 940.5739s
	speed: 0.3973s/iter; left time: 900.6860s
	speed: 0.3970s/iter; left time: 860.3285s
	speed: 0.3970s/iter; left time: 820.7000s
	speed: 0.3972s/iter; left time: 781.2295s
	speed: 0.3971s/iter; left time: 741.4367s
	speed: 0.3972s/iter; left time: 701.8954s
	speed: 0.3972s/iter; left time: 662.0608s
	speed: 0.3971s/iter; left time: 622.2988s
	speed: 0.3969s/iter; left time: 582.2828s
Epoch: 2, Cost time: 769.959s 
	speed: 2.9346s/iter; left time: 3768.0867s
	speed: 0.3970s/iter; left time: 470.0792s
	speed: 0.3972s/iter; left time: 430.5348s
	speed: 0.3972s/iter; left time: 390.8040s
	speed: 0.3972s/iter; left time: 351.1387s
	speed: 0.3973s/iter; left time: 311.4695s
	speed: 0.3970s/iter; left time: 271.5487s
	speed: 0.3971s/iter; left time: 231.9199s
	speed: 0.3972s/iter; left time: 192.2235s
	speed: 0.3972s/iter; left time: 152.5150s
	speed: 0.3971s/iter; left time: 112.7698s
	speed: 0.3972s/iter; left time: 73.0885s
	speed: 0.3972s/iter; left time: 33.3655s
Epoch: 3, Cost time: 769.960s 
Threshold : 1.0
pa_accuracy           : 0.9580
pa_precision          : 0.0000
pa_recall             : 0.0000
pa_f_score            : 0.0000
MCC_score             : nan
Affiliation precision : nan
Affiliation recall    : 0.0000
R_AUC_ROC             : 0.5105
R_AUC_PR              : 0.5407
VUS_ROC               : 0.5104
VUS_PR                : 0.5404
Accuracy : 0.9580, Precision : 0.0000, Recall : 0.0000, F-score : 0.0000 



2024-12-21 09:42:08
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4132s/iter; left time: 1673.3340s
	speed: 0.3932s/iter; left time: 1553.2198s
	speed: 0.3946s/iter; left time: 1519.3219s
	speed: 0.3952s/iter; left time: 1482.1797s
	speed: 0.3953s/iter; left time: 1442.6835s
	speed: 0.3957s/iter; left time: 1404.6343s
	speed: 0.3957s/iter; left time: 1365.2177s
	speed: 0.3961s/iter; left time: 1326.9697s
	speed: 0.3958s/iter; left time: 1286.4566s
	speed: 0.3960s/iter; left time: 1247.2481s
	speed: 0.3958s/iter; left time: 1207.3252s
	speed: 0.3959s/iter; left time: 1167.8202s
	speed: 0.3963s/iter; left time: 1129.3795s
Epoch: 1, Cost time: 769.272s 
	speed: 2.9346s/iter; left time: 7826.4592s
	speed: 0.3965s/iter; left time: 1017.8081s
	speed: 0.3965s/iter; left time: 978.2335s
	speed: 0.3966s/iter; left time: 938.7339s
	speed: 0.3967s/iter; left time: 899.3425s
	speed: 0.3967s/iter; left time: 859.5678s
	speed: 0.3968s/iter; left time: 820.1489s
	speed: 0.3966s/iter; left time: 780.0911s
	speed: 0.3969s/iter; left time: 740.9735s
	speed: 0.3964s/iter; left time: 700.4616s
	speed: 0.3967s/iter; left time: 661.2588s
	speed: 0.3968s/iter; left time: 621.8321s
	speed: 0.3968s/iter; left time: 582.0587s
Epoch: 2, Cost time: 769.499s 
	speed: 2.9358s/iter; left time: 3769.5962s
	speed: 0.3965s/iter; left time: 469.4976s
	speed: 0.3964s/iter; left time: 429.7361s
	speed: 0.3963s/iter; left time: 389.9201s
	speed: 0.3966s/iter; left time: 350.6378s
	speed: 0.3965s/iter; left time: 310.8498s
	speed: 0.3963s/iter; left time: 271.0852s
	speed: 0.3965s/iter; left time: 231.5560s
	speed: 0.3963s/iter; left time: 191.8306s
	speed: 0.3964s/iter; left time: 152.2282s
	speed: 0.3962s/iter; left time: 112.5266s
	speed: 0.3966s/iter; left time: 72.9717s
	speed: 0.3964s/iter; left time: 33.2938s
Epoch: 3, Cost time: 769.095s 
Threshold : 0.09859157879650648
pa_accuracy           : 0.9873
pa_precision          : 0.8557
pa_recall             : 0.8386
pa_f_score            : 0.8470
MCC_score             : 5.4313
Affiliation precision : 0.5255
Affiliation recall    : 0.8905
R_AUC_ROC             : 0.7033
R_AUC_PR              : 0.6555
VUS_ROC               : 0.6834
VUS_PR                : 0.6364
Accuracy : 0.9873, Precision : 0.8557, Recall : 0.8386, F-score : 0.8470 



2024-12-21 12:51:39
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4136s/iter; left time: 1675.0974s
	speed: 0.3931s/iter; left time: 1552.9221s
	speed: 0.3947s/iter; left time: 1519.4772s
	speed: 0.3956s/iter; left time: 1483.4241s
	speed: 0.3958s/iter; left time: 1444.5030s
	speed: 0.3962s/iter; left time: 1406.4030s
	speed: 0.3965s/iter; left time: 1367.9027s
	speed: 0.3964s/iter; left time: 1327.9584s
	speed: 0.3961s/iter; left time: 1287.3649s
	speed: 0.3964s/iter; left time: 1248.6808s
	speed: 0.3966s/iter; left time: 1209.6744s
	speed: 0.3965s/iter; left time: 1169.5530s
	speed: 0.3967s/iter; left time: 1130.6084s
Epoch: 1, Cost time: 769.784s 
	speed: 2.9349s/iter; left time: 7827.3107s
	speed: 0.3968s/iter; left time: 1018.5175s
	speed: 0.3970s/iter; left time: 979.3984s
	speed: 0.3967s/iter; left time: 939.0796s
	speed: 0.3967s/iter; left time: 899.2207s
	speed: 0.3967s/iter; left time: 859.7277s
	speed: 0.3968s/iter; left time: 820.2619s
	speed: 0.3968s/iter; left time: 780.4162s
	speed: 0.3972s/iter; left time: 741.5421s
	speed: 0.3972s/iter; left time: 701.8244s
	speed: 0.3969s/iter; left time: 661.6337s
	speed: 0.3969s/iter; left time: 621.9107s
	speed: 0.3968s/iter; left time: 582.1412s
Epoch: 2, Cost time: 769.696s 
	speed: 2.9352s/iter; left time: 3768.8595s
	speed: 0.3969s/iter; left time: 469.8850s
	speed: 0.3971s/iter; left time: 430.4411s
	speed: 0.3970s/iter; left time: 390.6119s
	speed: 0.3966s/iter; left time: 350.5915s
	speed: 0.3966s/iter; left time: 310.9308s
	speed: 0.3971s/iter; left time: 271.5949s
	speed: 0.3967s/iter; left time: 231.6901s
	speed: 0.3969s/iter; left time: 192.1044s
	speed: 0.3970s/iter; left time: 152.4453s
	speed: 0.3967s/iter; left time: 112.6659s
	speed: 0.3969s/iter; left time: 73.0323s
	speed: 0.3966s/iter; left time: 33.3110s
Epoch: 3, Cost time: 769.640s 
Threshold : 0.999984860420227
pa_accuracy           : 0.9868
pa_precision          : 0.8563
pa_recall             : 0.8243
pa_f_score            : 0.8400
MCC_score             : 9.9149
Affiliation precision : 0.5152
Affiliation recall    : 0.9005
R_AUC_ROC             : 0.7044
R_AUC_PR              : 0.6566
VUS_ROC               : 0.6851
VUS_PR                : 0.6381
Accuracy : 0.9868, Precision : 0.8563, Recall : 0.8243, F-score : 0.8400 



2024-12-22 14:47:09
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4134s/iter; left time: 1674.1403s
	speed: 0.3933s/iter; left time: 1553.3669s
	speed: 0.3945s/iter; left time: 1518.7932s
	speed: 0.3955s/iter; left time: 1483.0847s
	speed: 0.3956s/iter; left time: 1443.7955s
	speed: 0.3962s/iter; left time: 1406.3607s
	speed: 0.3963s/iter; left time: 1367.0698s
	speed: 0.3965s/iter; left time: 1328.2720s
	speed: 0.3962s/iter; left time: 1287.7626s
	speed: 0.3964s/iter; left time: 1248.7049s
	speed: 0.3967s/iter; left time: 1210.0835s
	speed: 0.3964s/iter; left time: 1169.3505s
	speed: 0.3964s/iter; left time: 1129.7700s
Epoch: 1, Cost time: 769.770s 
	speed: 2.9355s/iter; left time: 7829.0994s



2024-12-23 03:35:18
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4134s/iter; left time: 1674.0805s
	speed: 0.3939s/iter; left time: 1555.7895s
	speed: 0.3955s/iter; left time: 1522.8460s
	speed: 0.3962s/iter; left time: 1485.8693s
	speed: 0.3968s/iter; left time: 1448.3527s
	speed: 0.3971s/iter; left time: 1409.5701s
	speed: 0.3970s/iter; left time: 1369.5332s
	speed: 0.3972s/iter; left time: 1330.6286s
	speed: 0.3972s/iter; left time: 1290.9791s
	speed: 0.3974s/iter; left time: 1251.8696s
	speed: 0.3973s/iter; left time: 1211.7347s
	speed: 0.3974s/iter; left time: 1172.3015s
	speed: 0.3972s/iter; left time: 1131.9039s
Epoch: 1, Cost time: 771.289s 
	speed: 2.9412s/iter; left time: 7844.2477s
	speed: 0.3977s/iter; left time: 1020.9079s
	speed: 0.3978s/iter; left time: 981.4100s
	speed: 0.3980s/iter; left time: 942.0851s
	speed: 0.3980s/iter; left time: 902.3578s
	speed: 0.3977s/iter; left time: 861.7194s
	speed: 0.3978s/iter; left time: 822.2338s
	speed: 0.3980s/iter; left time: 782.8809s
	speed: 0.3979s/iter; left time: 742.9327s
	speed: 0.3978s/iter; left time: 702.8664s
	speed: 0.3978s/iter; left time: 663.0601s
	speed: 0.3977s/iter; left time: 623.1244s
	speed: 0.3979s/iter; left time: 583.7718s
Epoch: 2, Cost time: 771.346s 
	speed: 2.9398s/iter; left time: 3774.6772s
	speed: 0.3977s/iter; left time: 470.8653s
	speed: 0.3976s/iter; left time: 431.0069s
	speed: 0.3974s/iter; left time: 391.0277s
	speed: 0.3974s/iter; left time: 351.2888s
	speed: 0.3978s/iter; left time: 311.8712s
	speed: 0.3978s/iter; left time: 272.1055s
	speed: 0.3977s/iter; left time: 232.2803s
	speed: 0.3978s/iter; left time: 192.5272s
	speed: 0.3978s/iter; left time: 152.7663s
	speed: 0.3974s/iter; left time: 112.8563s
	speed: 0.3976s/iter; left time: 73.1655s
	speed: 0.3975s/iter; left time: 33.3880s



2024-12-25 09:33:19
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4133s/iter; left time: 1674.0138s
	speed: 0.3933s/iter; left time: 1553.6807s
	speed: 0.3950s/iter; left time: 1520.9204s
	speed: 0.3967s/iter; left time: 1487.7118s



2024-12-25 09:49:45
================ Hyperparameters ===============
anormly_ratio: 0.6
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4136s/iter; left time: 1675.0964s
	speed: 0.3929s/iter; left time: 1552.0683s
	speed: 0.3948s/iter; left time: 1520.0487s
	speed: 0.3956s/iter; left time: 1483.3370s
	speed: 0.3957s/iter; left time: 1444.4442s
	speed: 0.3961s/iter; left time: 1406.2762s
	speed: 0.3961s/iter; left time: 1366.6704s
	speed: 0.3962s/iter; left time: 1327.2958s
	speed: 0.3962s/iter; left time: 1287.6084s
	speed: 0.3962s/iter; left time: 1248.0735s
	speed: 0.3961s/iter; left time: 1207.9896s
	speed: 0.3965s/iter; left time: 1169.6065s
	speed: 0.3964s/iter; left time: 1129.8280s
Epoch: 1, Cost time: 769.607s 
	speed: 2.9343s/iter; left time: 7825.7495s
	speed: 0.3970s/iter; left time: 1019.1832s
	speed: 0.3971s/iter; left time: 979.7059s
	speed: 0.3970s/iter; left time: 939.8164s
	speed: 0.3966s/iter; left time: 899.1680s
	speed: 0.3969s/iter; left time: 860.1308s
	speed: 0.3969s/iter; left time: 820.3626s
	speed: 0.3970s/iter; left time: 780.8988s
	speed: 0.3970s/iter; left time: 741.2322s
	speed: 0.3969s/iter; left time: 701.4038s
	speed: 0.3972s/iter; left time: 662.0719s
	speed: 0.3969s/iter; left time: 622.0071s
	speed: 0.3970s/iter; left time: 582.4250s
Epoch: 2, Cost time: 769.791s 
	speed: 2.9349s/iter; left time: 3768.4355s
	speed: 0.3966s/iter; left time: 469.5686s
	speed: 0.3966s/iter; left time: 429.9049s
	speed: 0.3964s/iter; left time: 390.0196s
	speed: 0.3967s/iter; left time: 350.6974s
	speed: 0.3967s/iter; left time: 311.0461s
	speed: 0.3967s/iter; left time: 271.3496s
	speed: 0.3967s/iter; left time: 231.6938s
	speed: 0.3967s/iter; left time: 191.9865s
	speed: 0.3966s/iter; left time: 152.3085s
	speed: 0.3967s/iter; left time: 112.6522s
	speed: 0.3966s/iter; left time: 72.9762s
	speed: 0.3966s/iter; left time: 33.3151s
Epoch: 3, Cost time: 769.212s 
====================  Test in progress  ===================
/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/


2024-12-25 10:38:38
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 1
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4137s/iter; left time: 531.1977s
	speed: 0.3927s/iter; left time: 465.0152s
	speed: 0.3940s/iter; left time: 427.0751s
	speed: 0.3950s/iter; left time: 388.7179s
	speed: 0.3954s/iter; left time: 349.5702s
	speed: 0.3956s/iter; left time: 310.1539s
	speed: 0.3956s/iter; left time: 270.6220s
	speed: 0.3957s/iter; left time: 231.1150s
	speed: 0.3958s/iter; left time: 191.5860s
	speed: 0.3960s/iter; left time: 152.0510s
	speed: 0.3959s/iter; left time: 112.4352s
	speed: 0.3960s/iter; left time: 72.8702s
	speed: 0.3959s/iter; left time: 33.2547s
Epoch: 1, Cost time: 768.818s 
====================  Test in progress  ===================
====================  Threshhold  ===================

Threshold : 0.3475081319808968
==================== EVALUATION Metrics ===================

pa_accuracy           : 0.9877
==================== EVALUATION Metrics ===================

pa_precision          : 0.8966
==================== EVALUATION Metrics ===================

pa_recall             : 0.7995
==================== EVALUATION Metrics ===================

pa_f_score            : 0.8453
==================== EVALUATION Metrics ===================

MCC_score             : nan
==================== EVALUATION Metrics ===================

Affiliation precision : 0.5160
==================== EVALUATION Metrics ===================

Affiliation recall    : 0.8417
==================== EVALUATION Metrics ===================

R_AUC_ROC             : 0.6621
==================== EVALUATION Metrics ===================

R_AUC_PR              : 0.6353
==================== EVALUATION Metrics ===================

VUS_ROC               : 0.6485
==================== EVALUATION Metrics ===================

VUS_PR                : 0.6222
============================= =============================
====================  MODEL DETECTION  ===================
Anomaly detected starting at index:
 75285, 79225, 88045, 127673, 133644, 146504, 152943, 167873, 170833, 171123, 187242, 192948, 193592, 198855, 202369, 204109, 238952, 240412, 241863, 264082, 264652, 267215, 272884, 273604, 283877, 285384, 292229, 292514, 310020, 311505, 312336, 313579, 315037, 326103, 330243, 336943, 341253, 342703, 375792, 380193, 381568, 394146, 407145, 414305, 418610, 438865, 444329, 446402, 467581, 467791, 480378, 494668, 514871, 515985, 517093, 518389, 519808, 526576, 543041, 546146, 567331, 569190, 574102, 592469, 603691, 615117, 616557, 617171, 617996, 623480, 627642, 659171, 666627, 667782, 670270, 678871, 685119
Total number of indices: 698880
====================  FINAL METRICS  ===================
Accuracy : 0.9961, Precision : 0.9156, Recall : 1.0000, F-score : 0.9559 



2024-12-25 11:48:52
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4134s/iter; left time: 1102.5478s
	speed: 0.3937s/iter; left time: 1010.5311s
	speed: 0.3949s/iter; left time: 974.2515s
	speed: 0.3959s/iter; left time: 937.1473s
	speed: 0.3960s/iter; left time: 897.6692s
	speed: 0.3961s/iter; left time: 858.3712s
	speed: 0.3964s/iter; left time: 819.3699s
	speed: 0.3964s/iter; left time: 779.7182s
	speed: 0.3963s/iter; left time: 739.8482s
	speed: 0.3968s/iter; left time: 701.2138s
	speed: 0.3968s/iter; left time: 661.5048s
	speed: 0.3976s/iter; left time: 622.9895s
	speed: 0.3976s/iter; left time: 583.2770s
Epoch: 1, Cost time: 770.674s 
	speed: 2.9407s/iter; left time: 3775.8178s
	speed: 0.3980s/iter; left time: 471.1925s
	speed: 0.3983s/iter; left time: 431.7549s
	speed: 0.3987s/iter; left time: 392.2717s
	speed: 0.3987s/iter; left time: 352.4896s
	speed: 0.3988s/iter; left time: 312.6287s
	speed: 0.3986s/iter; left time: 272.6115s
	speed: 0.3981s/iter; left time: 232.4757s
	speed: 0.3980s/iter; left time: 192.6474s
	speed: 0.3980s/iter; left time: 152.8494s
	speed: 0.3973s/iter; left time: 112.8331s
	speed: 0.3974s/iter; left time: 73.1300s
	speed: 0.3976s/iter; left time: 33.3950s
Epoch: 2, Cost time: 771.422s 
====================  Test in progress  ===================
====================  Threshhold  ===================

Threshold : 1.0
==================== EVALUATION Metrics ===================

pa_accuracy           : 0.9580
pa_precision          : 0.0000
pa_recall             : 0.0000
pa_f_score            : 0.0000
MCC_score             : nan
Affiliation precision : nan
Affiliation recall    : 0.0000
R_AUC_ROC             : 0.5105
R_AUC_PR              : 0.5407
VUS_ROC               : 0.5104
VUS_PR                : 0.5404
==========================================================
====================  MODEL DETECTION  ===================
No anomalies detected in the dataset.



2024-12-25 12:43:41
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4135s/iter; left time: 1102.7158s
	speed: 0.3934s/iter; left time: 1009.9175s
	speed: 0.3949s/iter; left time: 974.2146s
	speed: 0.3956s/iter; left time: 936.2979s
	speed: 0.3959s/iter; left time: 897.5985s
	speed: 0.3960s/iter; left time: 858.0485s
	speed: 0.3962s/iter; left time: 818.8441s
	speed: 0.3961s/iter; left time: 779.1299s
	speed: 0.3964s/iter; left time: 740.0460s
	speed: 0.3962s/iter; left time: 700.0727s
	speed: 0.3962s/iter; left time: 660.4256s
	speed: 0.3963s/iter; left time: 620.9738s
	speed: 0.3964s/iter; left time: 581.5230s
Epoch: 1, Cost time: 769.694s 
	speed: 2.9347s/iter; left time: 3768.1521s
	speed: 0.3968s/iter; left time: 469.7703s
	speed: 0.3965s/iter; left time: 429.7902s
	speed: 0.3966s/iter; left time: 390.2302s
	speed: 0.3963s/iter; left time: 350.3670s
	speed: 0.3961s/iter; left time: 310.5599s
	speed: 0.3965s/iter; left time: 271.2277s
	speed: 0.3961s/iter; left time: 231.3509s
	speed: 0.3962s/iter; left time: 191.7489s
	speed: 0.3962s/iter; left time: 152.1487s
	speed: 0.3961s/iter; left time: 112.4847s
	speed: 0.3962s/iter; left time: 72.8994s
	speed: 0.3959s/iter; left time: 33.2529s
Epoch: 2, Cost time: 768.689s 
====================  Test in progress  ===================
====================  Threshhold  ===================

Threshold : 0.999974250793457
==================== EVALUATION Metrics ===================

pa_accuracy           : 0.9845
pa_precision          : 0.8866
pa_recall             : 0.7217
pa_f_score            : 0.7957
MCC_score             : 8.6444
Affiliation precision : 0.5121
Affiliation recall    : 0.8313
R_AUC_ROC             : 0.6480
R_AUC_PR              : 0.6174
VUS_ROC               : 0.6349
VUS_PR                : 0.6047
==========================================================
====================  MODEL DETECTION  ===================
Anomaly detected starting at index:
 10620, 61292, 75285, 79225, 88045, 121078, 146504, 147136, 170833, 192948, 198855, 202369, 238952, 240412, 241863, 264082, 264652, 267215, 273604, 283877, 292514, 309553, 311151, 311505, 313579, 315037, 326103, 330243, 358332, 361616, 368042, 380193, 394146, 414305, 438068, 438865, 444329, 446402, 466020, 467581, 467791, 494668, 514871, 515985, 517093, 518389, 519808, 532130, 533376, 549765, 559102, 567331, 571215, 574102, 592469, 603691, 616557, 632943, 661631, 666627, 667782, 670270
Total number of indices: 698880
====================  FINAL METRICS  ===================
Accuracy : 0.9961, Precision : 0.9155, Recall : 1.0000, F-score : 0.9559 



2024-12-25 15:02:56
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 2
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4139s/iter; left time: 1103.9731s
	speed: 0.3931s/iter; left time: 1009.1151s
	speed: 0.3947s/iter; left time: 973.7081s
	speed: 0.3954s/iter; left time: 935.8189s
	speed: 0.3960s/iter; left time: 897.6748s
	speed: 0.3959s/iter; left time: 857.9772s
	speed: 0.3965s/iter; left time: 819.5289s
	speed: 0.3963s/iter; left time: 779.5406s
	speed: 0.3962s/iter; left time: 739.7126s
	speed: 0.3965s/iter; left time: 700.6556s
	speed: 0.3964s/iter; left time: 660.7444s
	speed: 0.3966s/iter; left time: 621.4072s
	speed: 0.3963s/iter; left time: 581.3097s
Epoch: 1, Cost time: 770.055s 
	speed: 2.9377s/iter; left time: 3772.0549s
	speed: 0.3969s/iter; left time: 469.9505s
	speed: 0.3968s/iter; left time: 430.1474s
	speed: 0.3970s/iter; left time: 390.6007s
	speed: 0.3968s/iter; left time: 350.7788s
	speed: 0.3972s/iter; left time: 311.4068s
	speed: 0.3968s/iter; left time: 271.4343s
	speed: 0.3969s/iter; left time: 231.7660s
	speed: 0.3969s/iter; left time: 192.1095s
	speed: 0.3967s/iter; left time: 152.3449s
	speed: 0.3968s/iter; left time: 112.6843s
	speed: 0.3968s/iter; left time: 73.0041s
	speed: 0.3967s/iter; left time: 33.3211s
Epoch: 2, Cost time: 769.692s 
====================  Test in progress  ===================
====================  Threshhold  ===================

Threshold : 1.0
==================== EVALUATION Metrics ===================

pa_accuracy           : 0.9580
pa_precision          : 0.0000
pa_recall             : 0.0000
pa_f_score            : 0.0000
MCC_score             : nan
Affiliation precision : nan
Affiliation recall    : 0.0000
R_AUC_ROC             : 0.5105
R_AUC_PR              : 0.5407
VUS_ROC               : 0.5104
VUS_PR                : 0.5404
==========================================================
====================  MODEL DETECTION  ===================
No anomalies detected in the dataset.



2024-12-26 13:51:00
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4133s/iter; left time: 1673.8628s



2024-12-26 13:56:16
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.4131s/iter; left time: 1673.1715s



2024-12-27 03:48:46
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-27 03:53:28
================ Hyperparameters ===============
anormly_ratio: 0.4
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================
====================  Threshhold  ===================

Threshold : 1.0
==================== EVALUATION Metrics ===================

pa_accuracy           : 0.9580
pa_precision          : 0.0000
pa_recall             : 0.0000
pa_f_score            : 0.0000
MCC_score             : nan
Affiliation precision : nan
Affiliation recall    : 0.0000
R_AUC_ROC             : 0.5105
R_AUC_PR              : 0.5407
VUS_ROC               : 0.5104
VUS_PR                : 0.5404
==========================================================
====================  MODEL DETECTION  ===================
No anomalies detected in the dataset.



2024-12-27 04:05:24
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 04:06:39
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 512
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 04:09:54
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 256
====================  Train  ===================



2024-12-27 04:14:50
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 04:26:24
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:28:50
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])



2024-12-27 04:29:22
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:36:56
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 04:37:53
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:40:51
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:51:43
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:53:31
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 04:56:16
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105]), torch.Size([256, 1, 105, 105])]
Labels shape: torch.Size([256, 105])
====================  Test in progress  ===================



2024-12-27 04:58:02
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
	speed: 0.1041s/iter; left time: 2096.9742s
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
	speed: 0.0831s/iter; left time: 1664.0713s
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])
Series shape: [torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105]), torch.Size([105, 1, 105, 105])]
Labels shape: torch.Size([105, 105])



2024-12-27 04:59:16
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.1041s/iter; left time: 2096.3553s
	speed: 0.0834s/iter; left time: 1670.7984s
	speed: 0.0836s/iter; left time: 1666.3065s
	speed: 0.0836s/iter; left time: 1659.1216s
	speed: 0.0839s/iter; left time: 1656.0598s
	speed: 0.0842s/iter; left time: 1654.0651s
	speed: 0.0842s/iter; left time: 1643.9679s
	speed: 0.0841s/iter; left time: 1635.1115s
	speed: 0.0843s/iter; left time: 1629.1059s
	speed: 0.0843s/iter; left time: 1621.0847s
	speed: 0.0843s/iter; left time: 1612.6005s
	speed: 0.0843s/iter; left time: 1605.5889s
	speed: 0.0845s/iter; left time: 1599.4404s
	speed: 0.0845s/iter; left time: 1592.3733s
	speed: 0.0846s/iter; left time: 1585.1285s
	speed: 0.0843s/iter; left time: 1570.6059s
	speed: 0.0846s/iter; left time: 1568.8475s
	speed: 0.0845s/iter; left time: 1558.0871s
	speed: 0.0844s/iter; left time: 1548.1540s
	speed: 0.0845s/iter; left time: 1541.2185s
	speed: 0.0848s/iter; left time: 1537.0788s
	speed: 0.0846s/iter; left time: 1526.5259s
	speed: 0.0847s/iter; left time: 1519.1313s
	speed: 0.0846s/iter; left time: 1509.5009s
	speed: 0.0844s/iter; left time: 1496.7242s
	speed: 0.0843s/iter; left time: 1486.9664s
	speed: 0.0843s/iter; left time: 1478.2296s
	speed: 0.0844s/iter; left time: 1471.7215s
	speed: 0.0848s/iter; left time: 1470.9473s
	speed: 0.0849s/iter; left time: 1462.6366s
	speed: 0.0848s/iter; left time: 1452.5553s
	speed: 0.0847s/iter; left time: 1442.6467s
	speed: 0.0846s/iter; left time: 1431.9925s
	speed: 0.0847s/iter; left time: 1425.8100s
	speed: 0.0847s/iter; left time: 1417.6117s
	speed: 0.0846s/iter; left time: 1408.1848s
	speed: 0.0846s/iter; left time: 1398.9429s
	speed: 0.0847s/iter; left time: 1391.7900s
	speed: 0.0847s/iter; left time: 1384.3334s
	speed: 0.0845s/iter; left time: 1371.7352s
	speed: 0.0847s/iter; left time: 1366.4088s
	speed: 0.0845s/iter; left time: 1354.7874s
	speed: 0.0848s/iter; left time: 1350.7993s
	speed: 0.0847s/iter; left time: 1340.6316s
	speed: 0.0847s/iter; left time: 1332.4467s
	speed: 0.0846s/iter; left time: 1322.4962s
	speed: 0.0849s/iter; left time: 1318.3432s
	speed: 0.0847s/iter; left time: 1307.3563s
	speed: 0.0847s/iter; left time: 1299.7210s
	speed: 0.0845s/iter; left time: 1287.3503s
	speed: 0.0848s/iter; left time: 1283.3632s
	speed: 0.0846s/iter; left time: 1272.7120s
	speed: 0.0848s/iter; left time: 1267.2833s
	speed: 0.0847s/iter; left time: 1256.3626s
	speed: 0.0849s/iter; left time: 1251.0815s
	speed: 0.0846s/iter; left time: 1238.6381s
	speed: 0.0847s/iter; left time: 1231.8986s
	speed: 0.0846s/iter; left time: 1221.0819s
	speed: 0.0848s/iter; left time: 1215.3688s
	speed: 0.0846s/iter; left time: 1203.7021s
	speed: 0.0848s/iter; left time: 1198.7960s
	speed: 0.0848s/iter; left time: 1189.6780s
	speed: 0.0849s/iter; left time: 1183.3704s
	speed: 0.0846s/iter; left time: 1170.6758s
	speed: 0.0848s/iter; left time: 1164.3270s
	speed: 0.0846s/iter; left time: 1153.6958s
	speed: 0.0846s/iter; left time: 1145.6896s
Epoch [1/3], Loss: 34.1224, Accuracy: 1102500.00%



2024-12-27 05:12:04
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 256
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 05:19:19
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 255
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 05:20:58
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 255
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 255
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 05:28:11
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.1038s/iter; left time: 2090.8813s
	speed: 0.0833s/iter; left time: 1668.1772s
	speed: 0.0834s/iter; left time: 1661.9785s
	speed: 0.0837s/iter; left time: 1659.7616s
	speed: 0.0837s/iter; left time: 1651.3573s
	speed: 0.0838s/iter; left time: 1646.1623s
	speed: 0.0842s/iter; left time: 1645.1157s
	speed: 0.0838s/iter; left time: 1628.4548s
	speed: 0.0839s/iter; left time: 1623.1036s
	speed: 0.0841s/iter; left time: 1618.2406s
	speed: 0.0842s/iter; left time: 1610.2978s
	speed: 0.0842s/iter; left time: 1601.9782s
	speed: 0.0842s/iter; left time: 1593.9641s
	speed: 0.0841s/iter; left time: 1584.3298s
	speed: 0.0843s/iter; left time: 1579.3381s
	speed: 0.0842s/iter; left time: 1569.3908s
	speed: 0.0843s/iter; left time: 1563.4213s
	speed: 0.0842s/iter; left time: 1552.5838s
	speed: 0.0842s/iter; left time: 1543.7722s
	speed: 0.0842s/iter; left time: 1534.8337s
	speed: 0.0843s/iter; left time: 1528.4538s
	speed: 0.0842s/iter; left time: 1519.3920s
	speed: 0.0843s/iter; left time: 1511.1604s
	speed: 0.0843s/iter; left time: 1503.9366s
	speed: 0.0843s/iter; left time: 1494.9338s
	speed: 0.0843s/iter; left time: 1486.4420s
	speed: 0.0843s/iter; left time: 1477.7268s
	speed: 0.0843s/iter; left time: 1469.8291s
	speed: 0.0842s/iter; left time: 1459.9293s
	speed: 0.0843s/iter; left time: 1452.5061s
	speed: 0.0844s/iter; left time: 1446.2663s
	speed: 0.0843s/iter; left time: 1436.3921s
	speed: 0.0844s/iter; left time: 1429.1515s



2024-12-27 05:34:15
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================



2024-12-27 05:36:37
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
====================  Test in progress  ===================



2024-12-27 05:45:17
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 105
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.1033s/iter; left time: 2080.0272s



2024-12-27 05:46:32
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
	speed: 0.2279s/iter; left time: 1868.4731s
	speed: 0.1973s/iter; left time: 1598.2489s
	speed: 0.1980s/iter; left time: 1584.1271s
	speed: 0.1985s/iter; left time: 1568.0180s
	speed: 0.1988s/iter; left time: 1550.4826s
	speed: 0.1991s/iter; left time: 1533.0322s
	speed: 0.1994s/iter; left time: 1515.5237s
	speed: 0.1994s/iter; left time: 1495.5923s
	speed: 0.1995s/iter; left time: 1475.8316s
	speed: 0.1998s/iter; left time: 1458.6138s
	speed: 0.2001s/iter; left time: 1440.2795s
	speed: 0.1999s/iter; left time: 1419.1470s
	speed: 0.2000s/iter; left time: 1399.8225s
	speed: 0.2000s/iter; left time: 1379.8345s
	speed: 0.2000s/iter; left time: 1359.7418s
	speed: 0.1998s/iter; left time: 1338.7119s
	speed: 0.2003s/iter; left time: 1321.9089s
	speed: 0.2002s/iter; left time: 1301.1392s
	speed: 0.1998s/iter; left time: 1278.7211s
	speed: 0.2002s/iter; left time: 1261.0612s
	speed: 0.1999s/iter; left time: 1239.3089s
	speed: 0.1998s/iter; left time: 1218.6935s
	speed: 0.1999s/iter; left time: 1199.0984s
	speed: 0.1998s/iter; left time: 1178.4031s
	speed: 0.1997s/iter; left time: 1157.9481s
	speed: 0.1997s/iter; left time: 1138.0625s
	speed: 0.2000s/iter; left time: 1119.8968s
Epoch: 1, Cost time: 779.151s 
	speed: 2.5789s/iter; left time: 14011.1089s
	speed: 0.1995s/iter; left time: 1063.7055s
	speed: 0.1994s/iter; left time: 1043.2402s
	speed: 0.1993s/iter; left time: 1023.0198s
	speed: 0.1995s/iter; left time: 1003.9334s
	speed: 0.1995s/iter; left time: 983.9440s
	speed: 0.1995s/iter; left time: 964.0336s
	speed: 0.1994s/iter; left time: 943.8985s
	speed: 0.1995s/iter; left time: 924.4360s
	speed: 0.1993s/iter; left time: 903.5010s
	speed: 0.1993s/iter; left time: 883.3174s
	speed: 0.1995s/iter; left time: 864.2927s
	speed: 0.1993s/iter; left time: 843.7657s
	speed: 0.1992s/iter; left time: 823.3068s
	speed: 0.1993s/iter; left time: 803.8964s
	speed: 0.1991s/iter; left time: 783.1674s
	speed: 0.1991s/iter; left time: 762.9827s
	speed: 0.1990s/iter; left time: 742.8779s
	speed: 0.1990s/iter; left time: 723.0738s
	speed: 0.1991s/iter; left time: 703.5687s
	speed: 0.1990s/iter; left time: 683.0296s
	speed: 0.1989s/iter; left time: 663.0100s
	speed: 0.1991s/iter; left time: 643.6164s
	speed: 0.1989s/iter; left time: 623.2187s
	speed: 0.1991s/iter; left time: 603.9091s
	speed: 0.1990s/iter; left time: 583.5293s
	speed: 0.1990s/iter; left time: 563.7114s
Epoch: 2, Cost time: 776.238s 
	speed: 2.5832s/iter; left time: 6889.3124s



2024-12-27 06:13:29
================ Hyperparameters ===============
anormly_ratio: 0.5
batch_size: 256
d_model: 256
data_path: SMD
dataset: SMD
device_ids: [0, 1, 2, 3]
devices: 0,1,2,3
e_layers: 3
gpu: 0
index: 137
input_c: 38
k: 3
loss_fuc: MSE
lr: 0.0001
mode: train
model_save_path: checkpoints
n_heads: 1
num_epochs: 3
output_c: 38
patch_size: [5, 7]
rec_timeseries: True
use_gpu: True
use_multi_gpu: True
win_size: 105
====================  Train  ===================
Epoch [1/3], Loss: 0.0034
Epoch [1/3], Loss: 0.0069
Epoch [1/3], Loss: 0.0103
Epoch [1/3], Loss: 0.0138
Epoch [1/3], Loss: 0.0173
Epoch [1/3], Loss: 0.0209
Epoch [1/3], Loss: 0.0244
Epoch [1/3], Loss: 0.0280
Epoch [1/3], Loss: 0.0316
Epoch [1/3], Loss: 0.0352
Epoch [1/3], Loss: 0.0389
Epoch [1/3], Loss: 0.0426
Epoch [1/3], Loss: 0.0463
Epoch [1/3], Loss: 0.0500
Epoch [1/3], Loss: 0.0538
Epoch [1/3], Loss: 0.0576
Epoch [1/3], Loss: 0.0614
Epoch [1/3], Loss: 0.0653
Epoch [1/3], Loss: 0.0692
Epoch [1/3], Loss: 0.0731
Epoch [1/3], Loss: 0.0771
Epoch [1/3], Loss: 0.0810
Epoch [1/3], Loss: 0.0850
Epoch [1/3], Loss: 0.0891
Epoch [1/3], Loss: 0.0931
Epoch [1/3], Loss: 0.0972
Epoch [1/3], Loss: 0.1014
Epoch [1/3], Loss: 0.1055
Epoch [1/3], Loss: 0.1097
Epoch [1/3], Loss: 0.1140
Epoch [1/3], Loss: 0.1182
Epoch [1/3], Loss: 0.1226
Epoch [1/3], Loss: 0.1269
Epoch [1/3], Loss: 0.1313
Epoch [1/3], Loss: 0.1357
Epoch [1/3], Loss: 0.1402
Epoch [1/3], Loss: 0.1447
Epoch [1/3], Loss: 0.1492
Epoch [1/3], Loss: 0.1538
Epoch [1/3], Loss: 0.1584
Epoch [1/3], Loss: 0.1631
Epoch [1/3], Loss: 0.1679
Epoch [1/3], Loss: 0.1727
Epoch [1/3], Loss: 0.1776
Epoch [1/3], Loss: 0.1825
Epoch [1/3], Loss: 0.1874
Epoch [1/3], Loss: 0.1924
Epoch [1/3], Loss: 0.1975
Epoch [1/3], Loss: 0.2027
Epoch [1/3], Loss: 0.2078
Epoch [1/3], Loss: 0.2131
Epoch [1/3], Loss: 0.2184
Epoch [1/3], Loss: 0.2237
Epoch [1/3], Loss: 0.2292
Epoch [1/3], Loss: 0.2347
Epoch [1/3], Loss: 0.2402
Epoch [1/3], Loss: 0.2458
Epoch [1/3], Loss: 0.2515
Epoch [1/3], Loss: 0.2572
Epoch [1/3], Loss: 0.2630
Epoch [1/3], Loss: 0.2689
Epoch [1/3], Loss: 0.2747
Epoch [1/3], Loss: 0.2807
Epoch [1/3], Loss: 0.2867
Epoch [1/3], Loss: 0.2928
Epoch [1/3], Loss: 0.2989
Epoch [1/3], Loss: 0.3051
Epoch [1/3], Loss: 0.3113
Epoch [1/3], Loss: 0.3175
Epoch [1/3], Loss: 0.3238
Epoch [1/3], Loss: 0.3302
Epoch [1/3], Loss: 0.3366
Epoch [1/3], Loss: 0.3431
Epoch [1/3], Loss: 0.3496
Epoch [1/3], Loss: 0.3561
Epoch [1/3], Loss: 0.3627
Epoch [1/3], Loss: 0.3693
Epoch [1/3], Loss: 0.3760
Epoch [1/3], Loss: 0.3828
Epoch [1/3], Loss: 0.3896
Epoch [1/3], Loss: 0.3964
Epoch [1/3], Loss: 0.4033
Epoch [1/3], Loss: 0.4102
Epoch [1/3], Loss: 0.4173
Epoch [1/3], Loss: 0.4243
Epoch [1/3], Loss: 0.4314
Epoch [1/3], Loss: 0.4385
Epoch [1/3], Loss: 0.4458
Epoch [1/3], Loss: 0.4530
Epoch [1/3], Loss: 0.4604
Epoch [1/3], Loss: 0.4678
Epoch [1/3], Loss: 0.4752
Epoch [1/3], Loss: 0.4826
Epoch [1/3], Loss: 0.4901
Epoch [1/3], Loss: 0.4977
Epoch [1/3], Loss: 0.5053
Epoch [1/3], Loss: 0.5130
Epoch [1/3], Loss: 0.5207
Epoch [1/3], Loss: 0.5285
Epoch [1/3], Loss: 0.5364
	speed: 0.2292s/iter; left time: 1878.9423s
Epoch [1/3], Loss: 0.5443
Epoch [1/3], Loss: 0.5523
Epoch [1/3], Loss: 0.5603
Epoch [1/3], Loss: 0.5684
Epoch [1/3], Loss: 0.5765
Epoch [1/3], Loss: 0.5847
Epoch [1/3], Loss: 0.5929
Epoch [1/3], Loss: 0.6012
Epoch [1/3], Loss: 0.6095
Epoch [1/3], Loss: 0.6178
Epoch [1/3], Loss: 0.6263
Epoch [1/3], Loss: 0.6347
Epoch [1/3], Loss: 0.6432
Epoch [1/3], Loss: 0.6517
Epoch [1/3], Loss: 0.6603
Epoch [1/3], Loss: 0.6689
Epoch [1/3], Loss: 0.6775
Epoch [1/3], Loss: 0.6861
Epoch [1/3], Loss: 0.6948
Epoch [1/3], Loss: 0.7036
Epoch [1/3], Loss: 0.7123
Epoch [1/3], Loss: 0.7211
Epoch [1/3], Loss: 0.7299
Epoch [1/3], Loss: 0.7388
Epoch [1/3], Loss: 0.7477
Epoch [1/3], Loss: 0.7566
Epoch [1/3], Loss: 0.7655
Epoch [1/3], Loss: 0.7744
Epoch [1/3], Loss: 0.7834
Epoch [1/3], Loss: 0.7924
Epoch [1/3], Loss: 0.8015
Epoch [1/3], Loss: 0.8106
Epoch [1/3], Loss: 0.8197
Epoch [1/3], Loss: 0.8289
Epoch [1/3], Loss: 0.8381
Epoch [1/3], Loss: 0.8473
Epoch [1/3], Loss: 0.8565
Epoch [1/3], Loss: 0.8658
Epoch [1/3], Loss: 0.8751
Epoch [1/3], Loss: 0.8844
Epoch [1/3], Loss: 0.8938
Epoch [1/3], Loss: 0.9032
Epoch [1/3], Loss: 0.9127
Epoch [1/3], Loss: 0.9221
Epoch [1/3], Loss: 0.9317
Epoch [1/3], Loss: 0.9412
Epoch [1/3], Loss: 0.9508
Epoch [1/3], Loss: 0.9604
Epoch [1/3], Loss: 0.9700
Epoch [1/3], Loss: 0.9797
Epoch [1/3], Loss: 0.9894
Epoch [1/3], Loss: 0.9992
Epoch [1/3], Loss: 1.0089
Epoch [1/3], Loss: 1.0187
Epoch [1/3], Loss: 1.0286
Epoch [1/3], Loss: 1.0384
Epoch [1/3], Loss: 1.0484
Epoch [1/3], Loss: 1.0583
Epoch [1/3], Loss: 1.0683
Epoch [1/3], Loss: 1.0784
Epoch [1/3], Loss: 1.0885
Epoch [1/3], Loss: 1.0986
Epoch [1/3], Loss: 1.1087
Epoch [1/3], Loss: 1.1189
Epoch [1/3], Loss: 1.1292
Epoch [1/3], Loss: 1.1395
Epoch [1/3], Loss: 1.1498
Epoch [1/3], Loss: 1.1601
Epoch [1/3], Loss: 1.1705
Epoch [1/3], Loss: 1.1810
Epoch [1/3], Loss: 1.1914
Epoch [1/3], Loss: 1.2019
Epoch [1/3], Loss: 1.2125
Epoch [1/3], Loss: 1.2230
Epoch [1/3], Loss: 1.2337
Epoch [1/3], Loss: 1.2443
Epoch [1/3], Loss: 1.2551
Epoch [1/3], Loss: 1.2659
Epoch [1/3], Loss: 1.2767
Epoch [1/3], Loss: 1.2875
Epoch [1/3], Loss: 1.2984
Epoch [1/3], Loss: 1.3093
Epoch [1/3], Loss: 1.3202
Epoch [1/3], Loss: 1.3312
Epoch [1/3], Loss: 1.3422
Epoch [1/3], Loss: 1.3532
Epoch [1/3], Loss: 1.3643
Epoch [1/3], Loss: 1.3754
Epoch [1/3], Loss: 1.3865
Epoch [1/3], Loss: 1.3976
Epoch [1/3], Loss: 1.4087
Epoch [1/3], Loss: 1.4199
Epoch [1/3], Loss: 1.4311
Epoch [1/3], Loss: 1.4424
Epoch [1/3], Loss: 1.4536
Epoch [1/3], Loss: 1.4649
Epoch [1/3], Loss: 1.4762
Epoch [1/3], Loss: 1.4876
Epoch [1/3], Loss: 1.4989
Epoch [1/3], Loss: 1.5102
	speed: 0.1986s/iter; left time: 1608.6321s
Epoch [1/3], Loss: 1.5216
Epoch [1/3], Loss: 1.5330
Epoch [1/3], Loss: 1.5444
Epoch [1/3], Loss: 1.5558
Epoch [1/3], Loss: 1.5672
Epoch [1/3], Loss: 1.5787
Epoch [1/3], Loss: 1.5901
Epoch [1/3], Loss: 1.6016
Epoch [1/3], Loss: 1.6131
Epoch [1/3], Loss: 1.6246
Epoch [1/3], Loss: 1.6361
Epoch [1/3], Loss: 1.6475
Epoch [1/3], Loss: 1.6591
Epoch [1/3], Loss: 1.6706
Epoch [1/3], Loss: 1.6821
Epoch [1/3], Loss: 1.6936
Epoch [1/3], Loss: 1.7052
Epoch [1/3], Loss: 1.7168
Epoch [1/3], Loss: 1.7284
Epoch [1/3], Loss: 1.7400
Epoch [1/3], Loss: 1.7516
Epoch [1/3], Loss: 1.7632
Epoch [1/3], Loss: 1.7748
Epoch [1/3], Loss: 1.7865
Epoch [1/3], Loss: 1.7981
Epoch [1/3], Loss: 1.8097
Epoch [1/3], Loss: 1.8214
Epoch [1/3], Loss: 1.8330
Epoch [1/3], Loss: 1.8447
Epoch [1/3], Loss: 1.8564
Epoch [1/3], Loss: 1.8680
Epoch [1/3], Loss: 1.8797
Epoch [1/3], Loss: 1.8914
Epoch [1/3], Loss: 1.9030
Epoch [1/3], Loss: 1.9147
Epoch [1/3], Loss: 1.9264
Epoch [1/3], Loss: 1.9381
Epoch [1/3], Loss: 1.9498
Epoch [1/3], Loss: 1.9615
Epoch [1/3], Loss: 1.9732
Epoch [1/3], Loss: 1.9850
Epoch [1/3], Loss: 1.9967
Epoch [1/3], Loss: 2.0085
Epoch [1/3], Loss: 2.0202
Epoch [1/3], Loss: 2.0320
Epoch [1/3], Loss: 2.0438
Epoch [1/3], Loss: 2.0556
Epoch [1/3], Loss: 2.0673
Epoch [1/3], Loss: 2.0791
Epoch [1/3], Loss: 2.0909
Epoch [1/3], Loss: 2.1026
Epoch [1/3], Loss: 2.1144
Epoch [1/3], Loss: 2.1262
Epoch [1/3], Loss: 2.1380
Epoch [1/3], Loss: 2.1498
Epoch [1/3], Loss: 2.1616
Epoch [1/3], Loss: 2.1734
Epoch [1/3], Loss: 2.1852
Epoch [1/3], Loss: 2.1970
Epoch [1/3], Loss: 2.2089
Epoch [1/3], Loss: 2.2207
Epoch [1/3], Loss: 2.2325
Epoch [1/3], Loss: 2.2444
Epoch [1/3], Loss: 2.2562
Epoch [1/3], Loss: 2.2681
Epoch [1/3], Loss: 2.2799
Epoch [1/3], Loss: 2.2918
Epoch [1/3], Loss: 2.3036
Epoch [1/3], Loss: 2.3155
Epoch [1/3], Loss: 2.3273
Epoch [1/3], Loss: 2.3392
Epoch [1/3], Loss: 2.3511
Epoch [1/3], Loss: 2.3630
Epoch [1/3], Loss: 2.3749
Epoch [1/3], Loss: 2.3868
Epoch [1/3], Loss: 2.3987
Epoch [1/3], Loss: 2.4105
Epoch [1/3], Loss: 2.4224
Epoch [1/3], Loss: 2.4343
Epoch [1/3], Loss: 2.4462
Epoch [1/3], Loss: 2.4581
Epoch [1/3], Loss: 2.4700
Epoch [1/3], Loss: 2.4819
Epoch [1/3], Loss: 2.4938
Epoch [1/3], Loss: 2.5057
Epoch [1/3], Loss: 2.5176
Epoch [1/3], Loss: 2.5296
Epoch [1/3], Loss: 2.5416
Epoch [1/3], Loss: 2.5535
Epoch [1/3], Loss: 2.5654
Epoch [1/3], Loss: 2.5774
Epoch [1/3], Loss: 2.5893
Epoch [1/3], Loss: 2.6013
Epoch [1/3], Loss: 2.6132
Epoch [1/3], Loss: 2.6252
Epoch [1/3], Loss: 2.6372
Epoch [1/3], Loss: 2.6492
Epoch [1/3], Loss: 2.6611
Epoch [1/3], Loss: 2.6731
Epoch [1/3], Loss: 2.6851
	speed: 0.1990s/iter; left time: 1591.5971s
Epoch [1/3], Loss: 2.6971
Epoch [1/3], Loss: 2.7090
Epoch [1/3], Loss: 2.7210
Epoch [1/3], Loss: 2.7329
Epoch [1/3], Loss: 2.7449
Epoch [1/3], Loss: 2.7569
Epoch [1/3], Loss: 2.7689
Epoch [1/3], Loss: 2.7810
Epoch [1/3], Loss: 2.7930
Epoch [1/3], Loss: 2.8050
Epoch [1/3], Loss: 2.8170
Epoch [1/3], Loss: 2.8290
Epoch [1/3], Loss: 2.8410
Epoch [1/3], Loss: 2.8529
Epoch [1/3], Loss: 2.8649
Epoch [1/3], Loss: 2.8770
Epoch [1/3], Loss: 2.8889
Epoch [1/3], Loss: 2.9010
Epoch [1/3], Loss: 2.9130
Epoch [1/3], Loss: 2.9250
Epoch [1/3], Loss: 2.9370
Epoch [1/3], Loss: 2.9491
Epoch [1/3], Loss: 2.9611
Epoch [1/3], Loss: 2.9732
Epoch [1/3], Loss: 2.9852
Epoch [1/3], Loss: 2.9972
Epoch [1/3], Loss: 3.0091
Epoch [1/3], Loss: 3.0212
Epoch [1/3], Loss: 3.0332
Epoch [1/3], Loss: 3.0452
Epoch [1/3], Loss: 3.0573
Epoch [1/3], Loss: 3.0693
Epoch [1/3], Loss: 3.0814
Epoch [1/3], Loss: 3.0934
Epoch [1/3], Loss: 3.1054
Epoch [1/3], Loss: 3.1174
Epoch [1/3], Loss: 3.1294
Epoch [1/3], Loss: 3.1415
Epoch [1/3], Loss: 3.1535
Epoch [1/3], Loss: 3.1656
Epoch [1/3], Loss: 3.1776
Epoch [1/3], Loss: 3.1896
Epoch [1/3], Loss: 3.2017
Epoch [1/3], Loss: 3.2138
Epoch [1/3], Loss: 3.2258
Epoch [1/3], Loss: 3.2378
Epoch [1/3], Loss: 3.2499
Epoch [1/3], Loss: 3.2619
Epoch [1/3], Loss: 3.2740
Epoch [1/3], Loss: 3.2860
Epoch [1/3], Loss: 3.2981
Epoch [1/3], Loss: 3.3102
Epoch [1/3], Loss: 3.3222
Epoch [1/3], Loss: 3.3343
Epoch [1/3], Loss: 3.3463
Epoch [1/3], Loss: 3.3584
Epoch [1/3], Loss: 3.3705
Epoch [1/3], Loss: 3.3826
Epoch [1/3], Loss: 3.3947
Epoch [1/3], Loss: 3.4068
Epoch [1/3], Loss: 3.4188
Epoch [1/3], Loss: 3.4309
Epoch [1/3], Loss: 3.4430
Epoch [1/3], Loss: 3.4551
Epoch [1/3], Loss: 3.4671
Epoch [1/3], Loss: 3.4792
Epoch [1/3], Loss: 3.4913
Epoch [1/3], Loss: 3.5034
Epoch [1/3], Loss: 3.5155
Epoch [1/3], Loss: 3.5276
Epoch [1/3], Loss: 3.5396
Epoch [1/3], Loss: 3.5518
Epoch [1/3], Loss: 3.5638
Epoch [1/3], Loss: 3.5760
Epoch [1/3], Loss: 3.5881
Epoch [1/3], Loss: 3.6002
Epoch [1/3], Loss: 3.6123
Epoch [1/3], Loss: 3.6244
Epoch [1/3], Loss: 3.6365
Epoch [1/3], Loss: 3.6486
Epoch [1/3], Loss: 3.6607
Epoch [1/3], Loss: 3.6728
Epoch [1/3], Loss: 3.6849
Epoch [1/3], Loss: 3.6970
Epoch [1/3], Loss: 3.7090
Epoch [1/3], Loss: 3.7211
Epoch [1/3], Loss: 3.7332
Epoch [1/3], Loss: 3.7453
Epoch [1/3], Loss: 3.7575
Epoch [1/3], Loss: 3.7696
Epoch [1/3], Loss: 3.7817
Epoch [1/3], Loss: 3.7938
Epoch [1/3], Loss: 3.8059
Epoch [1/3], Loss: 3.8180
Epoch [1/3], Loss: 3.8301
Epoch [1/3], Loss: 3.8423
Epoch [1/3], Loss: 3.8544
Epoch [1/3], Loss: 3.8664
Epoch [1/3], Loss: 3.8785
Epoch [1/3], Loss: 3.8906
	speed: 0.1996s/iter; left time: 1576.3820s
Epoch [1/3], Loss: 3.9028
Epoch [1/3], Loss: 3.9149
Epoch [1/3], Loss: 3.9270
Epoch [1/3], Loss: 3.9391
Epoch [1/3], Loss: 3.9512
Epoch [1/3], Loss: 3.9634
Epoch [1/3], Loss: 3.9755
Epoch [1/3], Loss: 3.9877
Epoch [1/3], Loss: 3.9998
Epoch [1/3], Loss: 4.0119
Epoch [1/3], Loss: 4.0241
Epoch [1/3], Loss: 4.0362
Epoch [1/3], Loss: 4.0483
Epoch [1/3], Loss: 4.0605
Epoch [1/3], Loss: 4.0726
Epoch [1/3], Loss: 4.0847
Epoch [1/3], Loss: 4.0969
Epoch [1/3], Loss: 4.1090
Epoch [1/3], Loss: 4.1211
Epoch [1/3], Loss: 4.1333
Epoch [1/3], Loss: 4.1454
Epoch [1/3], Loss: 4.1575
Epoch [1/3], Loss: 4.1696
Epoch [1/3], Loss: 4.1818
Epoch [1/3], Loss: 4.1939
Epoch [1/3], Loss: 4.2060
Epoch [1/3], Loss: 4.2182
Epoch [1/3], Loss: 4.2303
Epoch [1/3], Loss: 4.2424
Epoch [1/3], Loss: 4.2546
Epoch [1/3], Loss: 4.2667
Epoch [1/3], Loss: 4.2789
Epoch [1/3], Loss: 4.2910
Epoch [1/3], Loss: 4.3031
Epoch [1/3], Loss: 4.3153
Epoch [1/3], Loss: 4.3274
Epoch [1/3], Loss: 4.3395
Epoch [1/3], Loss: 4.3517
Epoch [1/3], Loss: 4.3638
Epoch [1/3], Loss: 4.3760
Epoch [1/3], Loss: 4.3881
Epoch [1/3], Loss: 4.4003
Epoch [1/3], Loss: 4.4124
Epoch [1/3], Loss: 4.4246
Epoch [1/3], Loss: 4.4367
Epoch [1/3], Loss: 4.4489
Epoch [1/3], Loss: 4.4610
Epoch [1/3], Loss: 4.4732
Epoch [1/3], Loss: 4.4853
Epoch [1/3], Loss: 4.4974
Epoch [1/3], Loss: 4.5096
Epoch [1/3], Loss: 4.5217
Epoch [1/3], Loss: 4.5339
Epoch [1/3], Loss: 4.5460
Epoch [1/3], Loss: 4.5582
Epoch [1/3], Loss: 4.5703
Epoch [1/3], Loss: 4.5825
Epoch [1/3], Loss: 4.5946
Epoch [1/3], Loss: 4.6068
Epoch [1/3], Loss: 4.6190
Epoch [1/3], Loss: 4.6311
Epoch [1/3], Loss: 4.6433
Epoch [1/3], Loss: 4.6554
Epoch [1/3], Loss: 4.6676
Epoch [1/3], Loss: 4.6798
Epoch [1/3], Loss: 4.6919
Epoch [1/3], Loss: 4.7041
Epoch [1/3], Loss: 4.7162
Epoch [1/3], Loss: 4.7283
Epoch [1/3], Loss: 4.7404
Epoch [1/3], Loss: 4.7526
Epoch [1/3], Loss: 4.7648
Epoch [1/3], Loss: 4.7770
Epoch [1/3], Loss: 4.7891
Epoch [1/3], Loss: 4.8013
Epoch [1/3], Loss: 4.8134
Epoch [1/3], Loss: 4.8256
Epoch [1/3], Loss: 4.8378
Epoch [1/3], Loss: 4.8499
Epoch [1/3], Loss: 4.8621
Epoch [1/3], Loss: 4.8742
Epoch [1/3], Loss: 4.8864
Epoch [1/3], Loss: 4.8986
Epoch [1/3], Loss: 4.9107
Epoch [1/3], Loss: 4.9229
Epoch [1/3], Loss: 4.9350
Epoch [1/3], Loss: 4.9472
Epoch [1/3], Loss: 4.9593
Epoch [1/3], Loss: 4.9715
Epoch [1/3], Loss: 4.9836
Epoch [1/3], Loss: 4.9958
Epoch [1/3], Loss: 5.0079
Epoch [1/3], Loss: 5.0201
Epoch [1/3], Loss: 5.0322
Epoch [1/3], Loss: 5.0444
Epoch [1/3], Loss: 5.0566
Epoch [1/3], Loss: 5.0687
Epoch [1/3], Loss: 5.0809
Epoch [1/3], Loss: 5.0931
Epoch [1/3], Loss: 5.1052
	speed: 0.1996s/iter; left time: 1556.4814s
Epoch [1/3], Loss: 5.1174
Epoch [1/3], Loss: 5.1296
Epoch [1/3], Loss: 5.1417
Epoch [1/3], Loss: 5.1539
Epoch [1/3], Loss: 5.1661
Epoch [1/3], Loss: 5.1783
Epoch [1/3], Loss: 5.1904
Epoch [1/3], Loss: 5.2026
Epoch [1/3], Loss: 5.2148
Epoch [1/3], Loss: 5.2269
Epoch [1/3], Loss: 5.2391
Epoch [1/3], Loss: 5.2512
Epoch [1/3], Loss: 5.2634
Epoch [1/3], Loss: 5.2756
Epoch [1/3], Loss: 5.2877
Epoch [1/3], Loss: 5.2999
Epoch [1/3], Loss: 5.3121
Epoch [1/3], Loss: 5.3243
Epoch [1/3], Loss: 5.3365
Epoch [1/3], Loss: 5.3486
Epoch [1/3], Loss: 5.3608
Epoch [1/3], Loss: 5.3730
Epoch [1/3], Loss: 5.3852
Epoch [1/3], Loss: 5.3974
Epoch [1/3], Loss: 5.4096
Epoch [1/3], Loss: 5.4217
Epoch [1/3], Loss: 5.4339
Epoch [1/3], Loss: 5.4461
Epoch [1/3], Loss: 5.4582
Epoch [1/3], Loss: 5.4704
Epoch [1/3], Loss: 5.4826
Epoch [1/3], Loss: 5.4947
Epoch [1/3], Loss: 5.5069
Epoch [1/3], Loss: 5.5191
Epoch [1/3], Loss: 5.5313
Epoch [1/3], Loss: 5.5434
Epoch [1/3], Loss: 5.5556
Epoch [1/3], Loss: 5.5678
Epoch [1/3], Loss: 5.5800
Epoch [1/3], Loss: 5.5921
Epoch [1/3], Loss: 5.6043
Epoch [1/3], Loss: 5.6165
Epoch [1/3], Loss: 5.6286
Epoch [1/3], Loss: 5.6408
Epoch [1/3], Loss: 5.6531
Epoch [1/3], Loss: 5.6652
Epoch [1/3], Loss: 5.6774
Epoch [1/3], Loss: 5.6896
Epoch [1/3], Loss: 5.7018
Epoch [1/3], Loss: 5.7140
Epoch [1/3], Loss: 5.7262
Epoch [1/3], Loss: 5.7383
Epoch [1/3], Loss: 5.7505
Epoch [1/3], Loss: 5.7627
Epoch [1/3], Loss: 5.7749
Epoch [1/3], Loss: 5.7870
Epoch [1/3], Loss: 5.7992
Epoch [1/3], Loss: 5.8114
Epoch [1/3], Loss: 5.8236
Epoch [1/3], Loss: 5.8358
Epoch [1/3], Loss: 5.8479
Epoch [1/3], Loss: 5.8601
Epoch [1/3], Loss: 5.8722
Epoch [1/3], Loss: 5.8844
Epoch [1/3], Loss: 5.8966
Epoch [1/3], Loss: 5.9088
Epoch [1/3], Loss: 5.9210
Epoch [1/3], Loss: 5.9332
Epoch [1/3], Loss: 5.9454
Epoch [1/3], Loss: 5.9575
Epoch [1/3], Loss: 5.9697
Epoch [1/3], Loss: 5.9819
Epoch [1/3], Loss: 5.9941
Epoch [1/3], Loss: 6.0063
Epoch [1/3], Loss: 6.0185
Epoch [1/3], Loss: 6.0306
Epoch [1/3], Loss: 6.0428
Epoch [1/3], Loss: 6.0550
Epoch [1/3], Loss: 6.0672
Epoch [1/3], Loss: 6.0793
Epoch [1/3], Loss: 6.0915
Epoch [1/3], Loss: 6.1037
Epoch [1/3], Loss: 6.1159
Epoch [1/3], Loss: 6.1281
Epoch [1/3], Loss: 6.1402
Epoch [1/3], Loss: 6.1524
Epoch [1/3], Loss: 6.1646
Epoch [1/3], Loss: 6.1768
Epoch [1/3], Loss: 6.1890
Epoch [1/3], Loss: 6.2011
Epoch [1/3], Loss: 6.2133
Epoch [1/3], Loss: 6.2255
Epoch [1/3], Loss: 6.2376
Epoch [1/3], Loss: 6.2498
Epoch [1/3], Loss: 6.2620
Epoch [1/3], Loss: 6.2742
Epoch [1/3], Loss: 6.2864
Epoch [1/3], Loss: 6.2986
Epoch [1/3], Loss: 6.3108
Epoch [1/3], Loss: 6.3230
	speed: 0.2000s/iter; left time: 1539.4541s
Epoch [1/3], Loss: 6.3351
Epoch [1/3], Loss: 6.3473
Epoch [1/3], Loss: 6.3595
Epoch [1/3], Loss: 6.3717
Epoch [1/3], Loss: 6.3840
Epoch [1/3], Loss: 6.3961
Epoch [1/3], Loss: 6.4083
Epoch [1/3], Loss: 6.4205
Epoch [1/3], Loss: 6.4327
Epoch [1/3], Loss: 6.4449
Epoch [1/3], Loss: 6.4571
Epoch [1/3], Loss: 6.4693
Epoch [1/3], Loss: 6.4815
Epoch [1/3], Loss: 6.4937
Epoch [1/3], Loss: 6.5059
Epoch [1/3], Loss: 6.5181
Epoch [1/3], Loss: 6.5303
Epoch [1/3], Loss: 6.5424
Epoch [1/3], Loss: 6.5546
Epoch [1/3], Loss: 6.5668
Epoch [1/3], Loss: 6.5790
Epoch [1/3], Loss: 6.5912
Epoch [1/3], Loss: 6.6034
Epoch [1/3], Loss: 6.6156
Epoch [1/3], Loss: 6.6277
Epoch [1/3], Loss: 6.6400
Epoch [1/3], Loss: 6.6521
Epoch [1/3], Loss: 6.6643
Epoch [1/3], Loss: 6.6765
Epoch [1/3], Loss: 6.6887
Epoch [1/3], Loss: 6.7009
Epoch [1/3], Loss: 6.7131
Epoch [1/3], Loss: 6.7253
Epoch [1/3], Loss: 6.7375
Epoch [1/3], Loss: 6.7497
Epoch [1/3], Loss: 6.7619
Epoch [1/3], Loss: 6.7741
Epoch [1/3], Loss: 6.7863
Epoch [1/3], Loss: 6.7985
Epoch [1/3], Loss: 6.8107
Epoch [1/3], Loss: 6.8229
Epoch [1/3], Loss: 6.8351
Epoch [1/3], Loss: 6.8473
Epoch [1/3], Loss: 6.8595
Epoch [1/3], Loss: 6.8717
Epoch [1/3], Loss: 6.8838
Epoch [1/3], Loss: 6.8960
Epoch [1/3], Loss: 6.9082
Epoch [1/3], Loss: 6.9204
Epoch [1/3], Loss: 6.9326
Epoch [1/3], Loss: 6.9448
Epoch [1/3], Loss: 6.9570
Epoch [1/3], Loss: 6.9692
Epoch [1/3], Loss: 6.9814
Epoch [1/3], Loss: 6.9936
Epoch [1/3], Loss: 7.0058
Epoch [1/3], Loss: 7.0180
Epoch [1/3], Loss: 7.0302
Epoch [1/3], Loss: 7.0424
Epoch [1/3], Loss: 7.0546
Epoch [1/3], Loss: 7.0668
Epoch [1/3], Loss: 7.0790
Epoch [1/3], Loss: 7.0912
Epoch [1/3], Loss: 7.1034
Epoch [1/3], Loss: 7.1156
Epoch [1/3], Loss: 7.1277
Epoch [1/3], Loss: 7.1400
Epoch [1/3], Loss: 7.1522
Epoch [1/3], Loss: 7.1644
Epoch [1/3], Loss: 7.1765
Epoch [1/3], Loss: 7.1887
Epoch [1/3], Loss: 7.2010
Epoch [1/3], Loss: 7.2132
Epoch [1/3], Loss: 7.2254
Epoch [1/3], Loss: 7.2376
Epoch [1/3], Loss: 7.2498
Epoch [1/3], Loss: 7.2620
Epoch [1/3], Loss: 7.2742
Epoch [1/3], Loss: 7.2864
Epoch [1/3], Loss: 7.2986
Epoch [1/3], Loss: 7.3108
Epoch [1/3], Loss: 7.3230
Epoch [1/3], Loss: 7.3352
Epoch [1/3], Loss: 7.3474
Epoch [1/3], Loss: 7.3596
Epoch [1/3], Loss: 7.3718
Epoch [1/3], Loss: 7.3840
Epoch [1/3], Loss: 7.3962
Epoch [1/3], Loss: 7.4084
Epoch [1/3], Loss: 7.4206
Epoch [1/3], Loss: 7.4328
Epoch [1/3], Loss: 7.4450
Epoch [1/3], Loss: 7.4572
Epoch [1/3], Loss: 7.4694
Epoch [1/3], Loss: 7.4816
Epoch [1/3], Loss: 7.4938
Epoch [1/3], Loss: 7.5060
Epoch [1/3], Loss: 7.5182
Epoch [1/3], Loss: 7.5304
Epoch [1/3], Loss: 7.5426
	speed: 0.2003s/iter; left time: 1522.2484s
Epoch [1/3], Loss: 7.5548
Epoch [1/3], Loss: 7.5670
Epoch [1/3], Loss: 7.5792
Epoch [1/3], Loss: 7.5914
Epoch [1/3], Loss: 7.6036
Epoch [1/3], Loss: 7.6158
Epoch [1/3], Loss: 7.6279
Epoch [1/3], Loss: 7.6401
Epoch [1/3], Loss: 7.6523
Epoch [1/3], Loss: 7.6645
Epoch [1/3], Loss: 7.6767
Epoch [1/3], Loss: 7.6890
Epoch [1/3], Loss: 7.7012
Epoch [1/3], Loss: 7.7134
Epoch [1/3], Loss: 7.7256
Epoch [1/3], Loss: 7.7378
Epoch [1/3], Loss: 7.7500
Epoch [1/3], Loss: 7.7622
Epoch [1/3], Loss: 7.7744
Epoch [1/3], Loss: 7.7866
Epoch [1/3], Loss: 7.7988
Epoch [1/3], Loss: 7.8110
Epoch [1/3], Loss: 7.8232
Epoch [1/3], Loss: 7.8354
Epoch [1/3], Loss: 7.8476
Epoch [1/3], Loss: 7.8598
Epoch [1/3], Loss: 7.8720
Epoch [1/3], Loss: 7.8842
Epoch [1/3], Loss: 7.8964
Epoch [1/3], Loss: 7.9086
Epoch [1/3], Loss: 7.9208
Epoch [1/3], Loss: 7.9330
Epoch [1/3], Loss: 7.9451
Epoch [1/3], Loss: 7.9573
Epoch [1/3], Loss: 7.9695
Epoch [1/3], Loss: 7.9817
Epoch [1/3], Loss: 7.9939
Epoch [1/3], Loss: 8.0061
Epoch [1/3], Loss: 8.0183
Epoch [1/3], Loss: 8.0305
Epoch [1/3], Loss: 8.0427
Epoch [1/3], Loss: 8.0549
Epoch [1/3], Loss: 8.0672
Epoch [1/3], Loss: 8.0793
Epoch [1/3], Loss: 8.0916
Epoch [1/3], Loss: 8.1037
Epoch [1/3], Loss: 8.1159
Epoch [1/3], Loss: 8.1282
Epoch [1/3], Loss: 8.1404
Epoch [1/3], Loss: 8.1525
